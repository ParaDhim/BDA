{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionUnavailable",
     "evalue": "Connection has been closed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/py2neo/client/__init__.py:806\u001b[0m, in \u001b[0;36mConnectionPool.acquire\u001b[0;34m(self, force_reset, can_overfill)\u001b[0m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;66;03m# Plan A: select a free connection from the pool\u001b[39;00m\n\u001b[0;32m--> 806\u001b[0m     cx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_free_list\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpopleft\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    807\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m:\n",
      "\u001b[0;31mIndexError\u001b[0m: pop from an empty deque",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/py2neo/client/bolt.py:810\u001b[0m, in \u001b[0;36mBolt1._audit\u001b[0;34m(self, task)\u001b[0m\n\u001b[1;32m    809\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 810\u001b[0m     \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maudit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    811\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Neo4jError:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/py2neo/client/bolt.py:1303\u001b[0m, in \u001b[0;36mBoltResponse.audit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_ignored()\n\u001b[0;32m-> 1303\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_failure\n",
      "\u001b[0;31mClientError\u001b[0m: [Security.Unauthorized] The client is unauthorized due to authentication failure.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionUnavailable\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m         data\u001b[38;5;241m.\u001b[39mappend(json\u001b[38;5;241m.\u001b[39mloads(line))\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Create the citation graph in Neo4j\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m graph \u001b[38;5;241m=\u001b[39m \u001b[43mGraph\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbolt://localhost:7687\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mneo4j\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpassword\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[1;32m     17\u001b[0m     citing_paper \u001b[38;5;241m=\u001b[39m Node(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPaper\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39mrow[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpaper\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/py2neo/database.py:288\u001b[0m, in \u001b[0;36mGraph.__init__\u001b[0;34m(self, profile, name, **settings)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, profile\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msettings):\n\u001b[0;32m--> 288\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice \u001b[38;5;241m=\u001b[39m \u001b[43mGraphService\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msettings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m=\u001b[39m name\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschema \u001b[38;5;241m=\u001b[39m Schema(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/py2neo/database.py:119\u001b[0m, in \u001b[0;36mGraphService.__init__\u001b[0;34m(self, profile, **settings)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connector_settings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit_size\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m profile\u001b[38;5;241m.\u001b[39mrouting:\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# Ensures credentials are checked on construction\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     connector_settings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit_size\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 119\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connector \u001b[38;5;241m=\u001b[39m \u001b[43mConnector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconnector_settings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graphs \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/py2neo/client/__init__.py:960\u001b[0m, in \u001b[0;36mConnector.__init__\u001b[0;34m(self, profile, user_agent, init_size, max_size, max_age, routing_refresh_ttl)\u001b[0m\n\u001b[1;32m    958\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    959\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_router \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 960\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_add_pools\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initial_routers\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/py2neo/client/__init__.py:982\u001b[0m, in \u001b[0;36mConnector._add_pools\u001b[0;34m(self, *profiles)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    981\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdding connection pool for profile \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, profile)\n\u001b[0;32m--> 982\u001b[0m pool \u001b[38;5;241m=\u001b[39m \u001b[43mConnectionPool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    984\u001b[0m \u001b[43m    \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_user_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    985\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    986\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_max_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    987\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_age\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_max_age\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    988\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_broken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_on_broken\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    989\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pools[profile] \u001b[38;5;241m=\u001b[39m pool\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/py2neo/client/__init__.py:649\u001b[0m, in \u001b[0;36mConnectionPool.open\u001b[0;34m(cls, profile, user_agent, init_size, max_size, max_age, on_broken)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" Create a new connection pool, with an option to seed one\u001b[39;00m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;124;03mor more initial connections.\u001b[39;00m\n\u001b[1;32m    629\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[38;5;124;03m    scheme\u001b[39;00m\n\u001b[1;32m    647\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    648\u001b[0m pool \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(profile, user_agent, max_size, max_age, on_broken)\n\u001b[0;32m--> 649\u001b[0m seeds \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minit_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault_init_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m seed \u001b[38;5;129;01min\u001b[39;00m seeds:\n\u001b[1;32m    651\u001b[0m     seed\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/py2neo/client/__init__.py:649\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" Create a new connection pool, with an option to seed one\u001b[39;00m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;124;03mor more initial connections.\u001b[39;00m\n\u001b[1;32m    629\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[38;5;124;03m    scheme\u001b[39;00m\n\u001b[1;32m    647\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    648\u001b[0m pool \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(profile, user_agent, max_size, max_age, on_broken)\n\u001b[0;32m--> 649\u001b[0m seeds \u001b[38;5;241m=\u001b[39m [\u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(init_size \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_init_size)]\n\u001b[1;32m    650\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m seed \u001b[38;5;129;01min\u001b[39;00m seeds:\n\u001b[1;32m    651\u001b[0m     seed\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/py2neo/client/__init__.py:813\u001b[0m, in \u001b[0;36mConnectionPool.acquire\u001b[0;34m(self, force_reset, can_overfill)\u001b[0m\n\u001b[1;32m    807\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m:\n\u001b[1;32m    808\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_capacity() \u001b[38;5;129;01mor\u001b[39;00m can_overfill:\n\u001b[1;32m    809\u001b[0m         \u001b[38;5;66;03m# Plan B: if the pool isn't full, open\u001b[39;00m\n\u001b[1;32m    810\u001b[0m         \u001b[38;5;66;03m# a new connection. This may raise a\u001b[39;00m\n\u001b[1;32m    811\u001b[0m         \u001b[38;5;66;03m# ConnectionUnavailable exception, which\u001b[39;00m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# should bubble up to the caller.\u001b[39;00m\n\u001b[0;32m--> 813\u001b[0m         cx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m cx\u001b[38;5;241m.\u001b[39msupports_multi():\n\u001b[1;32m    815\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_supports_multi \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/py2neo/client/__init__.py:764\u001b[0m, in \u001b[0;36mConnectionPool._connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_connect\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    762\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" Open and return a new connection.\u001b[39;00m\n\u001b[1;32m    763\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 764\u001b[0m     cx \u001b[38;5;241m=\u001b[39m \u001b[43mConnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    765\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mon_release\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelease\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    766\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mon_broken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__on_broken\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    767\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_server_agent \u001b[38;5;241m=\u001b[39m cx\u001b[38;5;241m.\u001b[39mserver_agent\n\u001b[1;32m    768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cx\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/py2neo/client/__init__.py:174\u001b[0m, in \u001b[0;36mConnection.open\u001b[0;34m(cls, profile, user_agent, on_release, on_broken)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m profile\u001b[38;5;241m.\u001b[39mprotocol \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbolt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpy2neo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbolt\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Bolt\n\u001b[0;32m--> 174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBolt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mon_release\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_release\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon_broken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_broken\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m profile\u001b[38;5;241m.\u001b[39mprotocol \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpy2neo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhttp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTP\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/py2neo/client/bolt.py:361\u001b[0m, in \u001b[0;36mBolt.open\u001b[0;34m(cls, profile, user_agent, on_release, on_broken)\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to agree supported protocol version\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    360\u001b[0m     bolt \u001b[38;5;241m=\u001b[39m subclass(wire, profile, on_release\u001b[38;5;241m=\u001b[39mon_release)\n\u001b[0;32m--> 361\u001b[0m     \u001b[43mbolt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_hello\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbolt_user_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bolt\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, WireError) \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/py2neo/client/bolt.py:867\u001b[0m, in \u001b[0;36mBolt3._hello\u001b[0;34m(self, user_agent)\u001b[0m\n\u001b[1;32m    865\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend()\n\u001b[1;32m    866\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fetch()\n\u001b[0;32m--> 867\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_audit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mserver_agent \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserver\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    869\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnection_id \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconnection_id\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/py2neo/client/bolt.py:812\u001b[0m, in \u001b[0;36mBolt1._audit\u001b[0;34m(self, task)\u001b[0m\n\u001b[1;32m    810\u001b[0m     task\u001b[38;5;241m.\u001b[39maudit()\n\u001b[1;32m    811\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Neo4jError:\n\u001b[0;32m--> 812\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mforce\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    815\u001b[0m     \u001b[38;5;66;03m# On 1 Apr 2021, this was moved here from _wait. Because\u001b[39;00m\n\u001b[1;32m    816\u001b[0m     \u001b[38;5;66;03m# _wait is generally called before _audit, the post-failure\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;66;03m# Putting it here ensures that release is only ever done\u001b[39;00m\n\u001b[1;32m    822\u001b[0m     \u001b[38;5;66;03m# after such a reset.\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/py2neo/client/bolt.py:506\u001b[0m, in \u001b[0;36mBolt1.reset\u001b[0;34m(self, force)\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreset\u001b[39m(\u001b[38;5;28mself\u001b[39m, force\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 506\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_assert_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    507\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m force \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transaction:\n\u001b[1;32m    508\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mappend_message(\u001b[38;5;241m0x0F\u001b[39m, vital\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/py2neo/client/bolt.py:449\u001b[0m, in \u001b[0;36mBolt._assert_open\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_assert_open\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    448\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclosed:\n\u001b[0;32m--> 449\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ConnectionUnavailable(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection has been closed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    450\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbroken:\n\u001b[1;32m    451\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ConnectionUnavailable(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection is broken\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mConnectionUnavailable\u001b[0m: Connection has been closed"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import json\n",
    "from py2neo import Graph, Node, Relationship\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, lit\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# Load the data from the provided JSON file\n",
    "data = []\n",
    "with open('train.json', 'r') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "# Create the citation graph in Neo4j\n",
    "graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))\n",
    "for row in data:\n",
    "    citing_paper = Node(\"Paper\", id=row['paper'])\n",
    "    graph.merge(citing_paper, \"Paper\", \"id\")\n",
    "    for cited_paper in row['reference']:\n",
    "        cited_paper_node = Node(\"Paper\", id=cited_paper)\n",
    "        graph.merge(cited_paper_node, \"Paper\", \"id\")\n",
    "        relationship = Relationship(citing_paper, \"CITES\", cited_paper_node)\n",
    "        graph.merge(relationship)\n",
    "\n",
    "# Run SimRank algorithm using Apache Spark\n",
    "spark = SparkSession.builder.appName(\"SimRank\").getOrCreate()\n",
    "\n",
    "# Convert the Neo4j graph to a Spark DataFrame\n",
    "nodes = [record['id'] for record in graph.run(\"MATCH (p:Paper) RETURN p.id\")]\n",
    "node_df = spark.createDataFrame(nodes, \"string\").toDF(\"node\")\n",
    "edges = [(record['source']['id'], record['target']['id']) \n",
    "         for record in graph.run(\"MATCH (p1:Paper)-[r:CITES]->(p2:Paper) RETURN {source: p1, target: p2}\")]\n",
    "edge_df = spark.createDataFrame(edges, (\"source\", \"target\")).cache()\n",
    "\n",
    "# Define the SimRank function\n",
    "@udf(DoubleType())\n",
    "def simrank(source, target, c=0.9, max_iter=100, tolerance=1e-4):\n",
    "    if source == target:\n",
    "        return 1.0\n",
    "    \n",
    "    in_neighbors_u = [record['source']['id'] \n",
    "                      for record in graph.run(f\"MATCH (p1:Paper {{id: '{source}'}})<-[r:CITES]-(p2:Paper) RETURN p2.id\")]\n",
    "    in_neighbors_v = [record['source']['id'] \n",
    "                      for record in graph.run(f\"MATCH (p1:Paper {{id: '{target}'}})<-[r:CITES]-(p2:Paper) RETURN p2.id\")]\n",
    "    scale = c / (len(in_neighbors_u) * len(in_neighbors_v)) if in_neighbors_u and in_neighbors_v else 0\n",
    "    sim = 0.0\n",
    "    for u in in_neighbors_u:\n",
    "        for v in in_neighbors_v:\n",
    "            sim += simrank(u, v, c, max_iter, tolerance)\n",
    "    return scale * sim\n",
    "\n",
    "# Run SimRank with different importance factors\n",
    "for c in [0.7, 0.8, 0.9]:\n",
    "    sim_df = edge_df.crossJoin(node_df.alias(\"target\")) \\\n",
    "        .withColumn(\"similarity\", simrank(\"source\", \"target\", lit(c))) \\\n",
    "        .select(\"source\", \"target\", \"similarity\")\n",
    "    \n",
    "    # Get the SimRank similarity for the query nodes\n",
    "    query_nodes = [\"2982615777\", \"1556418098\"]\n",
    "    for node in query_nodes:\n",
    "        print(f\"SimRank similarity for node {node} (c={c}):\")\n",
    "        sim_df.filter(sim_df.source == node).orderBy(\"similarity\", ascending=False).show()\n",
    "\n",
    "# Cleanup\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wd/p8rd_3q56c3gyt1n_mf2kh8h0000gn/T/ipykernel_97007/1920709667.py:27: DeprecationWarning: write_transaction has been renamed to execute_write\n",
      "  session.write_transaction(create_graph, paper_id, references)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/wd/p8rd_3q56c3gyt1n_mf2kh8h0000gn/T/ipykernel_97007/1920709667.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mpaper_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"paper\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mreferences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"reference\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_transaction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaper_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreferences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/neo4j/_meta.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    241\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m                 \u001b[0mwarning_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_level\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m             \u001b[0minner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_without_warning\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/neo4j/_sync/work/session.py\u001b[0m in \u001b[0;36mwrite_transaction\u001b[0;34m(self, transaction_function, *args, **kwargs)\u001b[0m\n\u001b[1;32m    844\u001b[0m             \u001b[0mMethod\u001b[0m \u001b[0mwas\u001b[0m \u001b[0mrenamed\u001b[0m \u001b[0mto\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mmeth\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute_write\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m         \"\"\"\n\u001b[0;32m--> 846\u001b[0;31m         return self._run_transaction(\n\u001b[0m\u001b[1;32m    847\u001b[0m             \u001b[0mWRITE_ACCESS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    848\u001b[0m             \u001b[0mTelemetryAPI\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTX_FUNC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/neo4j/_sync/work/session.py\u001b[0m in \u001b[0;36m_run_transaction\u001b[0;34m(self, access_mode, api, transaction_function, args, kwargs)\u001b[0m\n\u001b[1;32m    568\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m                 self._open_transaction(\n\u001b[0m\u001b[1;32m    571\u001b[0m                     \u001b[0mtx_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mManagedTransaction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m                     \u001b[0mapi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtelemetry_sent\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/neo4j/_sync/work/session.py\u001b[0m in \u001b[0;36m_open_transaction\u001b[0;34m(self, tx_cls, access_mode, api, metadata, timeout, api_success_cb)\u001b[0m\n\u001b[1;32m    438\u001b[0m         \u001b[0mapi_success_cb\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     ) -> None:\n\u001b[0;32m--> 440\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccess_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccess_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connection\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mapi\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/neo4j/_sync/work/session.py\u001b[0m in \u001b[0;36m_connect\u001b[0;34m(self, access_mode, **acquire_kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0maccess_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_access_mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             super()._connect(\n\u001b[0m\u001b[1;32m    137\u001b[0m                 \u001b[0maccess_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0macquire_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             )\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/neo4j/_sync/work/workspace.py\u001b[0m in \u001b[0;36m_connect\u001b[0;34m(self, access_mode, auth, **acquire_kwargs)\u001b[0m\n\u001b[1;32m    182\u001b[0m         }\n\u001b[1;32m    183\u001b[0m         \u001b[0macquire_kwargs_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macquire_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0macquire_kwargs_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connection_access_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccess_mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/neo4j/_sync/io/_pool.py\u001b[0m in \u001b[0;36macquire\u001b[0;34m(self, access_mode, timeout, database, bookmarks, auth, liveness_check_timeout)\u001b[0m\n\u001b[1;32m    595\u001b[0m         )\n\u001b[1;32m    596\u001b[0m         \u001b[0mdeadline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDeadline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_timeout_or_deadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 597\u001b[0;31m         return self._acquire(\n\u001b[0m\u001b[1;32m    598\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeadline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mliveness_check_timeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m         )\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/neo4j/_sync/io/_pool.py\u001b[0m in \u001b[0;36m_acquire\u001b[0;34m(self, address, auth, deadline, liveness_check_timeout)\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m             \u001b[0;31m# try to find a free connection in the pool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m             connection = self._acquire_from_pool_checked(\n\u001b[0m\u001b[1;32m    291\u001b[0m                 \u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhealth_check\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeadline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m             )\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/neo4j/_sync/io/_pool.py\u001b[0m in \u001b[0;36m_acquire_from_pool_checked\u001b[0;34m(self, address, health_check, deadline)\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# no free connection available\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhealth_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeadline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m                 \u001b[0;31m# `close` is a noop on already closed connections.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                 \u001b[0;31m# This is to make sure that the connection is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/neo4j/_sync/io/_pool.py\u001b[0m in \u001b[0;36mhealth_check\u001b[0;34m(connection_, deadline_)\u001b[0m\n\u001b[1;32m    265\u001b[0m             \u001b[0mliveness_check_timeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mliveness_check_timeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m         \u001b[0;32mdef\u001b[0m \u001b[0mhealth_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeadline_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m             if (\n\u001b[1;32m    269\u001b[0m                 \u001b[0mconnection_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from neo4j import GraphDatabase\n",
    "import json\n",
    "\n",
    "# Load JSON data\n",
    "with open('train.json', 'r') as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "\n",
    "# Initialize Neo4j driver\n",
    "driver = GraphDatabase.driver(\"bolt://localhost:7689\", auth=(\"neo4j\", \"paras2003\"))\n",
    "\n",
    "def create_graph(tx, paper_id, references):\n",
    "    # Create node for citing paper\n",
    "    tx.run(\"MERGE (p:Paper {id: $paper_id})\", paper_id=paper_id)\n",
    "    \n",
    "    # Create nodes for referenced papers and the citation relationships\n",
    "    for ref in references:\n",
    "        tx.run(\"\"\"\n",
    "        MERGE (cited:Paper {id: $ref})\n",
    "        MERGE (citing)-[:CITES]->(cited)\n",
    "        \"\"\", citing=paper_id, ref=ref)\n",
    "\n",
    "# Create the citation graph\n",
    "with driver.session() as session:\n",
    "    for entry in data:\n",
    "        paper_id = entry[\"paper\"]\n",
    "        references = entry[\"reference\"]\n",
    "        session.write_transaction(create_graph, paper_id, references)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 61] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/wd/p8rd_3q56c3gyt1n_mf2kh8h0000gn/T/ipykernel_97007/2218380179.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mstop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1797\u001b[0m         \u001b[0;31m# We should clean the default session up. See SPARK-23228.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1798\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1799\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclearDefaultSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1800\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclearActiveSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1801\u001b[0m         \u001b[0mSparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_instantiatedSession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1710\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mUserHelpAutoCompletion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1712\u001b[0;31m         answer = self._gateway_client.send_command(\n\u001b[0m\u001b[1;32m   1713\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREFLECTION_COMMAND_NAME\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1714\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREFL_GET_UNKNOWN_SUB_COMMAND_NAME\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1034\u001b[0m          \u001b[0;32mif\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m         \"\"\"\n\u001b[0;32m-> 1036\u001b[0;31m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1037\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/clientserver.py\u001b[0m in \u001b[0;36m_get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconnection\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m             \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_new_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/clientserver.py\u001b[0m in \u001b[0;36m_create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    289\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_parameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython_parameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m             self.gateway_property, self)\n\u001b[0;32m--> 291\u001b[0;31m         \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect_to_java_server\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_thread_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/clientserver.py\u001b[0m in \u001b[0;36mconnect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    436\u001b[0m                 self.socket = self.ssl_context.wrap_socket(\n\u001b[1;32m    437\u001b[0m                     self.socket, server_hostname=self.java_address)\n\u001b[0;32m--> 438\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_address\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_port\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakefile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_connected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 61] Connection refused"
     ]
    }
   ],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 61] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/wd/p8rd_3q56c3gyt1n_mf2kh8h0000gn/T/ipykernel_97007/2498344094.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Initialize Spark session\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SimRank\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mmaster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"local[*]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    491\u001b[0m                 \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_instantiatedSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msession\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m                     \u001b[0msparkConf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m                         \u001b[0msparkConf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/conf.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, loadDefaults, _jvm, _jconf)\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m                 \u001b[0;31m# JVM is created, so create self._jconf directly through JVM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jconf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloadDefaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1710\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mUserHelpAutoCompletion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1712\u001b[0;31m         answer = self._gateway_client.send_command(\n\u001b[0m\u001b[1;32m   1713\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREFLECTION_COMMAND_NAME\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1714\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREFL_GET_UNKNOWN_SUB_COMMAND_NAME\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1034\u001b[0m          \u001b[0;32mif\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m         \"\"\"\n\u001b[0;32m-> 1036\u001b[0;31m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1037\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/clientserver.py\u001b[0m in \u001b[0;36m_get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconnection\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m             \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_new_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/clientserver.py\u001b[0m in \u001b[0;36m_create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    289\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_parameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython_parameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m             self.gateway_property, self)\n\u001b[0;32m--> 291\u001b[0;31m         \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect_to_java_server\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_thread_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/clientserver.py\u001b[0m in \u001b[0;36mconnect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    436\u001b[0m                 self.socket = self.ssl_context.wrap_socket(\n\u001b[1;32m    437\u001b[0m                     self.socket, server_hostname=self.java_address)\n\u001b[0;32m--> 438\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_address\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_port\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakefile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_connected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 61] Connection refused"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from graphframes import GraphFrame\n",
    "\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SimRank\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.jars.packages\", \"graphframes:graphframes:0.8.1-spark3.0-s_2.12\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "\n",
    "# Load data from Neo4j\n",
    "nodes_df = spark.read.format(\"neo4j\").option(\"url\", \"bolt://localhost:7689\").option(\"query\", \"MATCH (p:Paper) RETURN id(p) as id\").load()\n",
    "edges_df = spark.read.format(\"neo4j\").option(\"url\", \"bolt://localhost:7689\").option(\"query\", \"MATCH (a:Paper)-[:CITES]->(b:Paper) RETURN id(a) as src, id(b) as dst\").load()\n",
    "\n",
    "# Construct GraphFrame\n",
    "graph = GraphFrame(nodes_df, edges_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "import numpy as np\n",
    "\n",
    "def simrank(graph, query_node, C=0.8, max_iter=100, tolerance=0.0001):\n",
    "    nodes = list(graph.vertices.select(\"id\").toPandas()[\"id\"])\n",
    "    sim = {node: {other: (1.0 if node == other else 0.0) for other in nodes} for node in nodes}\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        prev_sim = {u: sim[u].copy() for u in sim}\n",
    "        for u, v in product(nodes, repeat=2):\n",
    "            if u != v:\n",
    "                in_neighbors_u = graph.edges.filter(f\"dst = {u}\").select(\"src\").toPandas()[\"src\"].tolist()\n",
    "                in_neighbors_v = graph.edges.filter(f\"dst = {v}\").select(\"src\").toPandas()[\"src\"].tolist()\n",
    "                \n",
    "                if in_neighbors_u and in_neighbors_v:\n",
    "                    scale = C / (len(in_neighbors_u) * len(in_neighbors_v))\n",
    "                    sim[u][v] = scale * sum(prev_sim[w][x] for w, x in product(in_neighbors_u, in_neighbors_v))\n",
    "\n",
    "        # Check for convergence\n",
    "        diff = sum(abs(sim[u][v] - prev_sim[u][v]) for u in nodes for v in nodes)\n",
    "        if diff < tolerance:\n",
    "            break\n",
    "    \n",
    "    return {v: sim[query_node][v] for v in nodes}\n",
    "\n",
    "# Calculate and display SimRank scores for each C value\n",
    "query_nodes = [2982615777, 1556418098]\n",
    "results = {}\n",
    "\n",
    "for C in [0.7, 0.8, 0.9]:\n",
    "    results[C] = {}\n",
    "    for query in query_nodes:\n",
    "        results[C][query] = simrank(graph, query_node=query, C=C)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting graph data from Neo4j...\n",
      "Found 614941 nodes and 11287 edges\n",
      "Creating adjacency matrix...\n",
      "\n",
      "=== SimRank Analysis with C = 0.7 ===\n"
     ]
    }
   ],
   "source": [
    "from neo4j import GraphDatabase\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "import pandas as pd\n",
    "\n",
    "class SimRankCalculator:\n",
    "    def __init__(self, neo4j_uri, neo4j_user, neo4j_password):\n",
    "        # Initialize Neo4j connection\n",
    "        self.driver = GraphDatabase.driver(neo4j_uri, auth=(neo4j_user, neo4j_password))\n",
    "\n",
    "    def get_graph_data_from_neo4j(self):\n",
    "        \"\"\"Extract graph data from Neo4j\"\"\"\n",
    "        with self.driver.session() as session:\n",
    "            # Get all nodes\n",
    "            nodes = session.run(\"MATCH (p:Paper) RETURN p.id AS id\")\n",
    "            nodes = [record[\"id\"] for record in nodes]\n",
    "            \n",
    "            # Get all edges\n",
    "            edges = session.run(\"\"\"\n",
    "                MATCH (p1:Paper)-[:CITES]->(p2:Paper)\n",
    "                RETURN p1.id AS source, p2.id AS target\n",
    "            \"\"\")\n",
    "            edges = [(record[\"source\"], record[\"target\"]) for record in edges]\n",
    "            \n",
    "            return nodes, edges\n",
    "\n",
    "    def create_adjacency_matrix(self, nodes, edges):\n",
    "        \"\"\"Create adjacency matrix from graph data\"\"\"\n",
    "        node_to_idx = {node: idx for idx, node in enumerate(nodes)}\n",
    "        n = len(nodes)\n",
    "        adj_matrix = np.zeros((n, n))\n",
    "        \n",
    "        for source, target in edges:\n",
    "            source_idx = node_to_idx[source]\n",
    "            target_idx = node_to_idx[target]\n",
    "            adj_matrix[source_idx][target_idx] = 1\n",
    "            \n",
    "        return adj_matrix, node_to_idx\n",
    "\n",
    "    def simrank_calculate(self, adj_matrix, C, max_iterations=100, tolerance=1e-4):\n",
    "        \"\"\"Calculate SimRank similarities\"\"\"\n",
    "        n = len(adj_matrix)\n",
    "        sim_old = np.identity(n)\n",
    "        \n",
    "        for iteration in range(max_iterations):\n",
    "            sim_new = np.zeros((n, n))\n",
    "            \n",
    "            # Compute similarities\n",
    "            for i, j in product(range(n), range(n)):\n",
    "                if i == j:\n",
    "                    sim_new[i][j] = 1.0\n",
    "                    continue\n",
    "                \n",
    "                # Get incoming neighbors\n",
    "                i_in = np.where(adj_matrix[:, i] > 0)[0]\n",
    "                j_in = np.where(adj_matrix[:, j] > 0)[0]\n",
    "                \n",
    "                if len(i_in) == 0 or len(j_in) == 0:\n",
    "                    continue\n",
    "                \n",
    "                # Calculate similarity\n",
    "                sum_sim = sum(sim_old[u][v] for u, v in product(i_in, j_in))\n",
    "                sim_new[i][j] = (C / (len(i_in) * len(j_in))) * sum_sim\n",
    "            \n",
    "            # Check convergence\n",
    "            if np.all(np.abs(sim_new - sim_old) < tolerance):\n",
    "                print(f\"Converged after {iteration + 1} iterations\")\n",
    "                return sim_new\n",
    "            \n",
    "            sim_old = sim_new.copy()\n",
    "        \n",
    "        print(\"Warning: Maximum iterations reached\")\n",
    "        return sim_old\n",
    "\n",
    "    def get_similar_nodes(self, query_node, sim_matrix, nodes, node_to_idx, top_k=10):\n",
    "        \"\"\"Get top-k similar nodes for a query node\"\"\"\n",
    "        if query_node not in node_to_idx:\n",
    "            return []\n",
    "        \n",
    "        query_idx = node_to_idx[query_node]\n",
    "        similarities = [(nodes[i], sim_matrix[query_idx][i]) \n",
    "                       for i in range(len(nodes))\n",
    "                       if i != query_idx]\n",
    "        \n",
    "        # Sort by similarity score\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        return similarities[:top_k]\n",
    "\n",
    "    def analyze_and_report(self, query_nodes, C_values):\n",
    "        \"\"\"Run complete analysis and generate report\"\"\"\n",
    "        try:\n",
    "            # Get graph data\n",
    "            print(\"Extracting graph data from Neo4j...\")\n",
    "            nodes, edges = self.get_graph_data_from_neo4j()\n",
    "            print(f\"Found {len(nodes)} nodes and {len(edges)} edges\")\n",
    "            \n",
    "            # Create adjacency matrix\n",
    "            print(\"Creating adjacency matrix...\")\n",
    "            adj_matrix, node_to_idx = self.create_adjacency_matrix(nodes, edges)\n",
    "            \n",
    "            # Run SimRank for different C values\n",
    "            for C in C_values:\n",
    "                print(f\"\\n=== SimRank Analysis with C = {C} ===\")\n",
    "                \n",
    "                # Calculate SimRank\n",
    "                sim_matrix = self.simrank_calculate(adj_matrix, C)\n",
    "                \n",
    "                # Get similar nodes for each query node\n",
    "                for query_node in query_nodes:\n",
    "                    print(f\"\\nQuery Node: {query_node}\")\n",
    "                    similar_nodes = self.get_similar_nodes(\n",
    "                        query_node, sim_matrix, nodes, node_to_idx\n",
    "                    )\n",
    "                    \n",
    "                    print(\"Top 10 Most Similar Papers:\")\n",
    "                    print(\"Paper ID\\t\\tSimilarity Score\")\n",
    "                    print(\"-\" * 40)\n",
    "                    for node_id, score in similar_nodes:\n",
    "                        print(f\"{node_id}\\t{score:.4f}\")\n",
    "                    \n",
    "                    # Calculate statistics\n",
    "                    scores = [score for _, score in similar_nodes]\n",
    "                    if scores:\n",
    "                        print(\"\\nSimilarity Statistics:\")\n",
    "                        print(f\"Average Similarity: {np.mean(scores):.4f}\")\n",
    "                        print(f\"Maximum Similarity: {np.max(scores):.4f}\")\n",
    "                        print(f\"Minimum Similarity: {np.min(scores):.4f}\")\n",
    "        \n",
    "        finally:\n",
    "            self.close()\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"Clean up resources\"\"\"\n",
    "        self.driver.close()\n",
    "\n",
    "def main():\n",
    "    # Initialize calculator\n",
    "    calculator = SimRankCalculator(\n",
    "        \"bolt://localhost:7689\",  # Note: Changed port to default Neo4j port\n",
    "        \"neo4j\",\n",
    "        \"paras2003\"\n",
    "    )\n",
    "    \n",
    "    # Define query nodes and C values\n",
    "    query_nodes = ['2982615777', '1556418098']\n",
    "    C_values = [0.7, 0.8, 0.9]\n",
    "    \n",
    "    # Run analysis\n",
    "    calculator.analyze_and_report(query_nodes, C_values)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, col\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "import pandas as pd\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "class CitationSimRank:\n",
    "    def __init__(self, neo4j_uri, neo4j_user, neo4j_password):\n",
    "        # Initialize Neo4j connection\n",
    "        self.driver = GraphDatabase.driver(neo4j_uri, auth=(neo4j_user, neo4j_password))\n",
    "        \n",
    "        # Initialize Spark session\n",
    "        self.spark = SparkSession.builder \\\n",
    "            .appName(\"CitationSimRank\") \\\n",
    "            .getOrCreate()\n",
    "\n",
    "    def create_graph_schema(self):\n",
    "        \"\"\"Create Neo4j constraints and indexes\"\"\"\n",
    "        with self.driver.session() as session:\n",
    "            # Create constraints for Paper nodes\n",
    "            session.run(\"CREATE CONSTRAINT paper_id IF NOT EXISTS FOR (p:Paper) REQUIRE p.id IS UNIQUE\")\n",
    "\n",
    "    def load_citation_data(self, data_df):\n",
    "        \"\"\"\n",
    "        Load citation data into Neo4j\n",
    "        data_df: DataFrame with columns 'paper' and 'reference' (list)\n",
    "        \"\"\"\n",
    "        with self.driver.session() as session:\n",
    "            # Create all paper nodes first\n",
    "            session.run(\"\"\"\n",
    "                UNWIND $papers AS paper\n",
    "                MERGE (p:Paper {id: paper})\n",
    "            \"\"\", papers=data_df['paper'].unique().tolist())\n",
    "            \n",
    "            # Create nodes for all referenced papers\n",
    "            referenced_papers = set()\n",
    "            for refs in data_df['reference']:\n",
    "                if refs:  # Check if reference list is not empty\n",
    "                    referenced_papers.update(refs)\n",
    "            \n",
    "            session.run(\"\"\"\n",
    "                UNWIND $papers AS paper\n",
    "                MERGE (p:Paper {id: paper})\n",
    "            \"\"\", papers=list(referenced_papers))\n",
    "            \n",
    "            # Create citation relationships\n",
    "            for _, row in data_df.iterrows():\n",
    "                if row['reference']:  # Only process non-empty reference lists\n",
    "                    session.run(\"\"\"\n",
    "                        MATCH (citing:Paper {id: $citing_id})\n",
    "                        UNWIND $cited_ids AS cited_id\n",
    "                        MATCH (cited:Paper {id: cited_id})\n",
    "                        MERGE (citing)-[:CITES]->(cited)\n",
    "                    \"\"\", citing_id=row['paper'], cited_ids=row['reference'])\n",
    "\n",
    "    def get_graph_data(self):\n",
    "        \"\"\"Extract graph data from Neo4j\"\"\"\n",
    "        with self.driver.session() as session:\n",
    "            # Get all nodes\n",
    "            nodes = session.run(\"MATCH (p:Paper) RETURN p.id AS id\")\n",
    "            nodes = [record[\"id\"] for record in nodes]\n",
    "            \n",
    "            # Get all edges (citations)\n",
    "            edges = session.run(\"\"\"\n",
    "                MATCH (p1:Paper)-[:CITES]->(p2:Paper)\n",
    "                RETURN p1.id AS source, p2.id AS target\n",
    "            \"\"\")\n",
    "            edges = [(record[\"source\"], record[\"target\"]) for record in edges]\n",
    "            \n",
    "            return nodes, edges\n",
    "\n",
    "    def create_adjacency_matrix(self, nodes, edges):\n",
    "        \"\"\"Create adjacency matrix using Spark\"\"\"\n",
    "        # Convert to Spark DataFrame\n",
    "        edges_df = self.spark.createDataFrame(edges, [\"source\", \"target\"])\n",
    "        \n",
    "        # Create node mapping\n",
    "        node_to_idx = {node: idx for idx, node in enumerate(nodes)}\n",
    "        n = len(nodes)\n",
    "        \n",
    "        # Broadcast node mapping\n",
    "        node_map_broadcast = self.spark.sparkContext.broadcast(node_to_idx)\n",
    "        \n",
    "        # Convert to matrix using Spark\n",
    "        adj_matrix = np.zeros((n, n))\n",
    "        \n",
    "        # Collect edges and update matrix\n",
    "        for row in edges_df.collect():\n",
    "            source_idx = node_map_broadcast.value[row.source]\n",
    "            target_idx = node_map_broadcast.value[row.target]\n",
    "            adj_matrix[source_idx][target_idx] = 1\n",
    "            \n",
    "        return adj_matrix, node_to_idx\n",
    "\n",
    "    def simrank_calculate(self, adj_matrix, C, max_iterations=100, tolerance=1e-4):\n",
    "        \"\"\"Calculate SimRank similarities using Spark\"\"\"\n",
    "        n = len(adj_matrix)\n",
    "        sim_old = np.identity(n)\n",
    "        \n",
    "        # Convert to Spark DataFrame for distributed computation\n",
    "        for iteration in range(max_iterations):\n",
    "            sim_new = np.zeros((n, n))\n",
    "            \n",
    "            # Compute similarities\n",
    "            for i, j in product(range(n), range(n)):\n",
    "                if i == j:\n",
    "                    sim_new[i][j] = 1.0\n",
    "                    continue\n",
    "                \n",
    "                # Get incoming neighbors\n",
    "                i_in = np.where(adj_matrix[:, i] > 0)[0]\n",
    "                j_in = np.where(adj_matrix[:, j] > 0)[0]\n",
    "                \n",
    "                if len(i_in) == 0 or len(j_in) == 0:\n",
    "                    continue\n",
    "                \n",
    "                # Calculate similarity\n",
    "                sum_sim = sum(sim_old[u][v] for u, v in product(i_in, j_in))\n",
    "                sim_new[i][j] = (C / (len(i_in) * len(j_in))) * sum_sim\n",
    "            \n",
    "            # Check convergence\n",
    "            if np.all(np.abs(sim_new - sim_old) < tolerance):\n",
    "                print(f\"Converged after {iteration + 1} iterations\")\n",
    "                return sim_new\n",
    "            \n",
    "            sim_old = sim_new.copy()\n",
    "        \n",
    "        print(\"Warning: Maximum iterations reached\")\n",
    "        return sim_old\n",
    "\n",
    "    def get_similar_papers(self, query_node, sim_matrix, nodes, node_to_idx, top_k=10):\n",
    "        \"\"\"Get top-k similar papers for a query paper\"\"\"\n",
    "        if query_node not in node_to_idx:\n",
    "            return []\n",
    "        \n",
    "        query_idx = node_to_idx[query_node]\n",
    "        similarities = [(nodes[i], sim_matrix[query_idx][i]) \n",
    "                       for i in range(len(nodes))\n",
    "                       if i != query_idx]\n",
    "        \n",
    "        # Sort by similarity score\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        return similarities[:top_k]\n",
    "\n",
    "    def analyze_citations(self, query_nodes, C_values, data_df):\n",
    "        \"\"\"Run complete citation analysis\"\"\"\n",
    "        try:\n",
    "            # Create graph schema\n",
    "            print(\"Creating Neo4j schema...\")\n",
    "            self.create_graph_schema()\n",
    "            \n",
    "            # Load data into Neo4j\n",
    "            print(\"Loading citation data into Neo4j...\")\n",
    "            self.load_citation_data(data_df)\n",
    "            \n",
    "            # Get graph data\n",
    "            print(\"Extracting graph data...\")\n",
    "            nodes, edges = self.get_graph_data()\n",
    "            print(f\"Found {len(nodes)} papers and {len(edges)} citations\")\n",
    "            \n",
    "            # Create adjacency matrix\n",
    "            print(\"Creating adjacency matrix...\")\n",
    "            adj_matrix, node_to_idx = self.create_adjacency_matrix(nodes, edges)\n",
    "            \n",
    "            # Run SimRank for different C values\n",
    "            results = {}\n",
    "            for C in C_values:\n",
    "                print(f\"\\n=== SimRank Analysis with C = {C} ===\")\n",
    "                \n",
    "                # Calculate SimRank\n",
    "                sim_matrix = self.simrank_calculate(adj_matrix, C)\n",
    "                \n",
    "                # Get similar papers for each query node\n",
    "                C_results = {}\n",
    "                for query_node in query_nodes:\n",
    "                    print(f\"\\nQuery Paper ID: {query_node}\")\n",
    "                    similar_papers = self.get_similar_papers(\n",
    "                        query_node, sim_matrix, nodes, node_to_idx\n",
    "                    )\n",
    "                    \n",
    "                    print(\"Top 10 Most Similar Papers:\")\n",
    "                    print(\"Paper ID\\t\\tSimilarity Score\")\n",
    "                    print(\"-\" * 40)\n",
    "                    for paper_id, score in similar_papers:\n",
    "                        print(f\"{paper_id}\\t{score:.4f}\")\n",
    "                    \n",
    "                    # Calculate statistics\n",
    "                    scores = [score for _, score in similar_papers]\n",
    "                    if scores:\n",
    "                        print(\"\\nSimilarity Statistics:\")\n",
    "                        print(f\"Average Similarity: {np.mean(scores):.4f}\")\n",
    "                        print(f\"Maximum Similarity: {np.max(scores):.4f}\")\n",
    "                        print(f\"Minimum Similarity: {np.min(scores):.4f}\")\n",
    "                    \n",
    "                    C_results[query_node] = similar_papers\n",
    "                \n",
    "                results[C] = C_results\n",
    "            \n",
    "            return results\n",
    "        \n",
    "        finally:\n",
    "            self.close()\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"Clean up resources\"\"\"\n",
    "        self.driver.close()\n",
    "        self.spark.stop()\n",
    "\n",
    "def main():\n",
    "    # Sample usage\n",
    "    neo4j_config = {\n",
    "        \"uri\": \"bolt://localhost:7687\",\n",
    "        \"user\": \"neo4j\",\n",
    "        \"password\": \"your_password\"\n",
    "    }\n",
    "    \n",
    "    # Initialize calculator\n",
    "    calculator = CitationSimRank(**neo4j_config)\n",
    "    \n",
    "    # Define query parameters\n",
    "    query_nodes = ['2982615777', '1556418098']\n",
    "    C_values = [0.7, 0.8, 0.9]\n",
    "    \n",
    "    # Load your citation data into a pandas DataFrame\n",
    "    # Example format:\n",
    "    # data_df = pd.DataFrame({\n",
    "    #     'paper': ['paper1', 'paper2'],\n",
    "    #     'reference': [['ref1', 'ref2'], ['ref3']]\n",
    "    # })\n",
    "    \n",
    "    # Run analysis\n",
    "    results = calculator.analyze_citations(query_nodes, C_values, data_df)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 61] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/wd/p8rd_3q56c3gyt1n_mf2kh8h0000gn/T/ipykernel_97007/2222197018.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;31m# Initialize calculator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m calculator = SparkSimRankCalculator(\n\u001b[0m\u001b[1;32m    129\u001b[0m     \u001b[0;34m\"bolt://localhost:7689\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;34m\"neo4j\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/wd/p8rd_3q56c3gyt1n_mf2kh8h0000gn/T/ipykernel_97007/2222197018.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, neo4j_uri, neo4j_user, neo4j_password)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# Initialize Spark session\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0;34m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CitationSimRank\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spark.driver.memory\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"4g\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    491\u001b[0m                 \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_instantiatedSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msession\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m                     \u001b[0msparkConf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m                         \u001b[0msparkConf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/conf.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, loadDefaults, _jvm, _jconf)\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m                 \u001b[0;31m# JVM is created, so create self._jconf directly through JVM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jconf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloadDefaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1710\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mUserHelpAutoCompletion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1712\u001b[0;31m         answer = self._gateway_client.send_command(\n\u001b[0m\u001b[1;32m   1713\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREFLECTION_COMMAND_NAME\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1714\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREFL_GET_UNKNOWN_SUB_COMMAND_NAME\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1034\u001b[0m          \u001b[0;32mif\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m         \"\"\"\n\u001b[0;32m-> 1036\u001b[0;31m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1037\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/clientserver.py\u001b[0m in \u001b[0;36m_get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconnection\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m             \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_new_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/clientserver.py\u001b[0m in \u001b[0;36m_create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    289\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_parameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython_parameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m             self.gateway_property, self)\n\u001b[0;32m--> 291\u001b[0;31m         \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect_to_java_server\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_thread_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/clientserver.py\u001b[0m in \u001b[0;36mconnect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    436\u001b[0m                 self.socket = self.ssl_context.wrap_socket(\n\u001b[1;32m    437\u001b[0m                     self.socket, server_hostname=self.java_address)\n\u001b[0;32m--> 438\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_address\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_port\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakefile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_connected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 61] Connection refused"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from neo4j import GraphDatabase\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "\n",
    "class SparkSimRankCalculator:\n",
    "    def __init__(self, neo4j_uri, neo4j_user, neo4j_password):\n",
    "        # Initialize Neo4j connection\n",
    "        self.driver = GraphDatabase.driver(neo4j_uri, auth=(neo4j_user, neo4j_password))\n",
    "        \n",
    "        # Initialize Spark session\n",
    "        self.spark = SparkSession.builder \\\n",
    "            .appName(\"CitationSimRank\") \\\n",
    "            .config(\"spark.driver.memory\", \"4g\") \\\n",
    "            .config(\"spark.executor.memory\", \"4g\") \\\n",
    "            .getOrCreate()\n",
    "\n",
    "    def get_graph_data_from_neo4j(self):\n",
    "        \"\"\"Extract graph data from Neo4j\"\"\"\n",
    "        with self.driver.session() as session:\n",
    "            # Get all nodes\n",
    "            nodes = session.run(\"MATCH (p:Paper) RETURN p.id AS id\")\n",
    "            nodes = [record[\"id\"] for record in nodes]\n",
    "            \n",
    "            # Get all edges\n",
    "            edges = session.run(\"\"\"\n",
    "                MATCH (p1:Paper)-[:CITES]->(p2:Paper)\n",
    "                RETURN p1.id AS source, p2.id AS target\n",
    "            \"\"\")\n",
    "            edges = [(record[\"source\"], record[\"target\"]) for record in edges]\n",
    "            \n",
    "            return nodes, edges\n",
    "\n",
    "    def create_spark_dataframes(self, nodes, edges):\n",
    "        \"\"\"Convert graph data to Spark DataFrames\"\"\"\n",
    "        # Create nodes DataFrame\n",
    "        nodes_df = self.spark.createDataFrame(\n",
    "            [(node,) for node in nodes],\n",
    "            [\"id\"]\n",
    "        )\n",
    "        \n",
    "        # Create edges DataFrame\n",
    "        edges_df = self.spark.createDataFrame(\n",
    "            edges,\n",
    "            [\"source\", \"target\"]\n",
    "        )\n",
    "        \n",
    "        return nodes_df, edges_df\n",
    "\n",
    "    def simrank_spark(self, nodes_df, edges_df, C, max_iterations=100, tolerance=1e-4):\n",
    "        \"\"\"\n",
    "        Implement SimRank using Spark\n",
    "        \"\"\"\n",
    "        # Convert to pandas for efficient matrix operations\n",
    "        nodes = [row.id for row in nodes_df.collect()]\n",
    "        edges = [(row.source, row.target) for row in edges_df.collect()]\n",
    "        \n",
    "        # Create adjacency matrix\n",
    "        node_to_idx = {node: idx for idx, node in enumerate(nodes)}\n",
    "        n = len(nodes)\n",
    "        adj_matrix = np.zeros((n, n))\n",
    "        \n",
    "        for source, target in edges:\n",
    "            source_idx = node_to_idx[source]\n",
    "            target_idx = node_to_idx[target]\n",
    "            adj_matrix[source_idx][target_idx] = 1\n",
    "        \n",
    "        # Initialize similarity matrix\n",
    "        sim_old = np.identity(n)\n",
    "        \n",
    "        # SimRank iterations\n",
    "        for iteration in range(max_iterations):\n",
    "            sim_new = np.zeros((n, n))\n",
    "            \n",
    "            # Compute similarities\n",
    "            for i, j in product(range(n), range(n)):\n",
    "                if i == j:\n",
    "                    sim_new[i][j] = 1.0\n",
    "                    continue\n",
    "                \n",
    "                # Get incoming neighbors\n",
    "                i_in = np.where(adj_matrix[:, i] > 0)[0]\n",
    "                j_in = np.where(adj_matrix[:, j] > 0)[0]\n",
    "                \n",
    "                if len(i_in) == 0 or len(j_in) == 0:\n",
    "                    continue\n",
    "                \n",
    "                # Calculate similarity\n",
    "                sum_sim = 0.0\n",
    "                for u, v in product(i_in, j_in):\n",
    "                    sum_sim += sim_old[u][v]\n",
    "                \n",
    "                sim_new[i][j] = (C / (len(i_in) * len(j_in))) * sum_sim\n",
    "            \n",
    "            # Check convergence\n",
    "            if np.all(np.abs(sim_new - sim_old) < tolerance):\n",
    "                print(f\"Converged after {iteration + 1} iterations\")\n",
    "                return sim_new, nodes, node_to_idx\n",
    "            \n",
    "            sim_old = sim_new.copy()\n",
    "        \n",
    "        print(\"Warning: Maximum iterations reached\")\n",
    "        return sim_old, nodes, node_to_idx\n",
    "\n",
    "    def get_similar_nodes(self, query_node, sim_matrix, nodes, node_to_idx, top_k=10):\n",
    "        \"\"\"Get top-k similar nodes for a query node\"\"\"\n",
    "        if query_node not in node_to_idx:\n",
    "            return []\n",
    "        \n",
    "        query_idx = node_to_idx[query_node]\n",
    "        similarities = [(nodes[i], sim_matrix[query_idx][i]) \n",
    "                       for i in range(len(nodes))\n",
    "                       if i != query_idx]\n",
    "        \n",
    "        # Sort by similarity score\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        return similarities[:top_k]\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"Clean up resources\"\"\"\n",
    "        self.driver.close()\n",
    "        self.spark.stop()\n",
    "\n",
    "\n",
    "# Initialize calculator\n",
    "calculator = SparkSimRankCalculator(\n",
    "    \"bolt://localhost:7689\",\n",
    "    \"neo4j\",\n",
    "    \"paras2003\"\n",
    ")\n",
    "\n",
    "try:\n",
    "    # Get graph data\n",
    "    print(\"Extracting graph data from Neo4j...\")\n",
    "    nodes, edges = calculator.get_graph_data_from_neo4j()\n",
    "    print(f\"Found {len(nodes)} nodes and {len(edges)} edges\")\n",
    "    \n",
    "    # Create Spark DataFrames\n",
    "    nodes_df, edges_df = calculator.create_spark_dataframes(nodes, edges)\n",
    "    \n",
    "    # Query nodes to analyze\n",
    "    query_nodes = ['2982615777', '1556418098']\n",
    "    C_values = [0.7, 0.8, 0.9]\n",
    "    \n",
    "    # Run SimRank for different C values\n",
    "    for C in C_values:\n",
    "        print(f\"\\n=== SimRank Analysis with C = {C} ===\")\n",
    "        \n",
    "        # Calculate SimRank\n",
    "        sim_matrix, nodes, node_to_idx = calculator.simrank_spark(\n",
    "            nodes_df, edges_df, C\n",
    "        )\n",
    "        \n",
    "        # Get similar nodes for each query node\n",
    "        for query_node in query_nodes:\n",
    "            print(f\"\\nQuery Node: {query_node}\")\n",
    "            similar_nodes = calculator.get_similar_nodes(\n",
    "                query_node, sim_matrix, nodes, node_to_idx\n",
    "            )\n",
    "            \n",
    "            print(\"Top 10 Most Similar Papers:\")\n",
    "            print(\"Paper ID\\t\\tSimilarity Score\")\n",
    "            print(\"-\" * 40)\n",
    "            for node_id, score in similar_nodes:\n",
    "                print(f\"{node_id}\\t{score:.4f}\")\n",
    "            \n",
    "            # Calculate statistics\n",
    "            scores = [score for _, score in similar_nodes]\n",
    "            if scores:\n",
    "                print(\"\\nSimilarity Statistics:\")\n",
    "                print(f\"Average Similarity: {np.mean(scores):.4f}\")\n",
    "                print(f\"Maximum Similarity: {np.max(scores):.4f}\")\n",
    "                print(f\"Minimum Similarity: {np.min(scores):.4f}\")\n",
    "\n",
    "finally:\n",
    "    # Clean up\n",
    "    calculator.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for C, query_results in results.items():\n",
    "    print(f\"\\nSimRank Results for C={C}\")\n",
    "    for query, similarities in query_results.items():\n",
    "        print(f\"\\nQuery Node ID: {query}\")\n",
    "        sorted_similarities = sorted(similarities.items(), key=lambda item: item[1], reverse=True)\n",
    "        for node, score in sorted_similarities[:10]:  # Top 10 similar nodes\n",
    "            print(f\"Node: {node}, Similarity: {score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Papers:   0%|          | 0/564340 [00:00<?, ?it/s]/var/folders/wd/p8rd_3q56c3gyt1n_mf2kh8h0000gn/T/ipykernel_97007/3991268908.py:28: DeprecationWarning: write_transaction has been renamed to execute_write\n",
      "  session.write_transaction(create_graph, paper_id, references)\n",
      "Processing Papers: 100%|| 564340/564340 [2:07:35<00:00, 73.72it/s]     \n"
     ]
    }
   ],
   "source": [
    "from neo4j import GraphDatabase\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load JSON data\n",
    "with open('train.json', 'r') as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "\n",
    "# Initialize Neo4j driver\n",
    "driver = GraphDatabase.driver(\"bolt://localhost:7689\", auth=(\"neo4j\", \"paras2003\"))\n",
    "\n",
    "def create_graph(tx, paper_id, references):\n",
    "    # Create node for citing paper\n",
    "    tx.run(\"MERGE (p:Paper {id: $paper_id})\", paper_id=paper_id)\n",
    "    \n",
    "    # Create nodes for referenced papers and the citation relationships\n",
    "    for ref in references:\n",
    "        tx.run(\"\"\"\n",
    "        MERGE (cited:Paper {id: $ref})\n",
    "        MERGE (citing)-[:CITES]->(cited)\n",
    "        \"\"\", citing=paper_id, ref=ref)\n",
    "\n",
    "# Create the citation graph with progress tracking\n",
    "with driver.session() as session:\n",
    "    for entry in tqdm(data, desc=\"Processing Papers\"):\n",
    "        paper_id = entry[\"paper\"]\n",
    "        references = entry[\"reference\"]\n",
    "        session.write_transaction(create_graph, paper_id, references)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/13 13:02:24 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/11/13 13:02:24 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "24/11/13 13:02:24 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/11/13 13:02:24 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "24/11/13 13:02:24 ERROR SparkContext: Failed to add /Users/parasdhiman/spark-jars/neo4j-connector-apache-spark_2.12-5.3.0.jar to Spark environment\n",
      "java.io.FileNotFoundException: Jar /Users/parasdhiman/spark-jars/neo4j-connector-apache-spark_2.12-5.3.0.jar not found\n",
      "\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)\n",
      "\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:499)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:480)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "24/11/13 13:02:24 ERROR SparkContext: Failed to add /Users/parasdhiman/spark-jars/neo4j-java-driver-4.4.9.jar to Spark environment\n",
      "java.io.FileNotFoundException: Jar /Users/parasdhiman/spark-jars/neo4j-java-driver-4.4.9.jar not found\n",
      "\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)\n",
      "\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:499)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:480)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "24/11/13 13:02:24 ERROR SparkContext: Failed to add /Users/parasdhiman/spark-jars/netty-all-4.1.97.Final.jar to Spark environment\n",
      "java.io.FileNotFoundException: Jar /Users/parasdhiman/spark-jars/netty-all-4.1.97.Final.jar not found\n",
      "\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)\n",
      "\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:499)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:480)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "24/11/13 13:02:24 ERROR SparkContext: Failed to add /Users/parasdhiman/spark-jars/reactive-streams-1.0.4.jar to Spark environment\n",
      "java.io.FileNotFoundException: Jar /Users/parasdhiman/spark-jars/reactive-streams-1.0.4.jar not found\n",
      "\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)\n",
      "\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:499)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:480)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "24/11/13 13:02:24 ERROR SparkContext: Failed to add /Users/parasdhiman/spark-jars/netty-handler-4.1.97.Final.jar to Spark environment\n",
      "java.io.FileNotFoundException: Jar /Users/parasdhiman/spark-jars/netty-handler-4.1.97.Final.jar not found\n",
      "\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)\n",
      "\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:499)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:480)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "24/11/13 13:02:24 ERROR SparkContext: Failed to add /Users/parasdhiman/spark-jars/netty-buffer-4.1.97.Final.jar to Spark environment\n",
      "java.io.FileNotFoundException: Jar /Users/parasdhiman/spark-jars/netty-buffer-4.1.97.Final.jar not found\n",
      "\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)\n",
      "\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:499)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:480)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "24/11/13 13:02:24 ERROR SparkContext: Failed to add /Users/parasdhiman/spark-jars/netty-common-4.1.97.Final.jar to Spark environment\n",
      "java.io.FileNotFoundException: Jar /Users/parasdhiman/spark-jars/netty-common-4.1.97.Final.jar not found\n",
      "\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)\n",
      "\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:499)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:480)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "24/11/13 13:02:24 ERROR SparkContext: Failed to add /Users/parasdhiman/spark-jars/netty-transport-4.1.97.Final.jar to Spark environment\n",
      "java.io.FileNotFoundException: Jar /Users/parasdhiman/spark-jars/netty-transport-4.1.97.Final.jar not found\n",
      "\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)\n",
      "\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:499)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:480)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "24/11/13 13:02:24 ERROR SparkContext: Failed to add /Users/parasdhiman/spark-jars/netty-resolver-4.1.97.Final.jar to Spark environment\n",
      "java.io.FileNotFoundException: Jar /Users/parasdhiman/spark-jars/netty-resolver-4.1.97.Final.jar not found\n",
      "\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)\n",
      "\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:499)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:480)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "24/11/13 13:02:24 ERROR SparkContext: Failed to add /Users/parasdhiman/spark-jars/netty-codec-4.1.97.Final.jar to Spark environment\n",
      "java.io.FileNotFoundException: Jar /Users/parasdhiman/spark-jars/netty-codec-4.1.97.Final.jar not found\n",
      "\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)\n",
      "\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:499)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:480)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error executing query: An error occurred while calling o504.load.\n",
      ": org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: org.neo4j.spark.DataSource. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: java.lang.ClassNotFoundException: org.neo4j.spark.DataSource.DefaultSource\n",
      "\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n",
      "\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:587)\n",
      "\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:520)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n",
      "\tat scala.util.Failure.orElse(Try.scala:224)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n",
      "\t... 15 more\n",
      "\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o504.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: org.neo4j.spark.DataSource. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: java.lang.ClassNotFoundException: org.neo4j.spark.DataSource.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:587)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:520)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 15 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 110\u001b[0m\n\u001b[1;32m    104\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124m    MATCH (p:Paper)-[:CITES]->(cited:Paper)\u001b[39m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;124m    RETURN p.id AS citing_paper, cited.id AS cited_paper\u001b[39m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;66;03m# Execute query\u001b[39;00m\n\u001b[0;32m--> 110\u001b[0m     result_df \u001b[38;5;241m=\u001b[39m \u001b[43mquery_neo4j\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspark\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m     result_df\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[37], line 85\u001b[0m, in \u001b[0;36mquery_neo4j\u001b[0;34m(spark, query, database)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;124;03mExecute a Cypher query against Neo4j using Spark\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;124;03m    Neo4j database name\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m     df \u001b[38;5;241m=\u001b[39m (\u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43morg.neo4j.spark.DataSource\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43murl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.neo4j.bolt.url\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauthentication.basic.username\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m               \u001b[49m\u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.neo4j.bolt.user\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauthentication.basic.password\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m               \u001b[49m\u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.neo4j.bolt.password\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquery\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdatabase\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m---> 85\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pyspark/sql/readwriter.py:314\u001b[0m, in \u001b[0;36mDataFrameReader.load\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jreader\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoSeq(path)))\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 314\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o504.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: org.neo4j.spark.DataSource. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: java.lang.ClassNotFoundException: org.neo4j.spark.DataSource.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:587)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:520)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 15 more\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def setup_spark_neo4j_connection(\n",
    "    neo4j_url=\"bolt://localhost:7687\",\n",
    "    neo4j_user=\"neo4j\",\n",
    "    neo4j_password=\"paras2003\",\n",
    "    spark_jars_dir=\"~/spark-jars\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Set up a Spark session with Neo4j connection configuration\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    neo4j_url : str\n",
    "        Neo4j bolt URL\n",
    "    neo4j_user : str\n",
    "        Neo4j username\n",
    "    neo4j_password : str\n",
    "        Neo4j password\n",
    "    spark_jars_dir : str\n",
    "        Directory containing the required JAR files\n",
    "    \"\"\"\n",
    "    \n",
    "    # Expand user directory path\n",
    "    jars_dir = os.path.expanduser(spark_jars_dir)\n",
    "    \n",
    "    # Create a comma-separated list of JAR files\n",
    "    jar_files = [\n",
    "        os.path.join(jars_dir, f) for f in [\n",
    "            \"neo4j-connector-apache-spark_2.12-5.3.0.jar\",\n",
    "            \"neo4j-java-driver-4.4.9.jar\",\n",
    "            \"netty-all-4.1.97.Final.jar\",\n",
    "            \"reactive-streams-1.0.4.jar\",\n",
    "            \"netty-handler-4.1.97.Final.jar\",\n",
    "            \"netty-buffer-4.1.97.Final.jar\",\n",
    "            \"netty-common-4.1.97.Final.jar\",\n",
    "            \"netty-transport-4.1.97.Final.jar\",\n",
    "            \"netty-resolver-4.1.97.Final.jar\",\n",
    "            \"netty-codec-4.1.97.Final.jar\"\n",
    "        ]\n",
    "    ]\n",
    "    \n",
    "    # Join jar paths with comma\n",
    "    jars = \",\".join(jar_files)\n",
    "    \n",
    "    # Stop any existing Spark session\n",
    "    SparkSession.builder.getOrCreate().stop()\n",
    "    \n",
    "    # Create new Spark session\n",
    "    spark = (SparkSession.builder\n",
    "        .appName(\"Neo4j Spark Connection\")\n",
    "        .config(\"spark.jars\", jars)  # Use local JARs instead of packages\n",
    "        .config(\"spark.neo4j.bolt.url\", neo4j_url)\n",
    "        .config(\"spark.neo4j.bolt.user\", neo4j_user)\n",
    "        .config(\"spark.neo4j.bolt.password\", neo4j_password)\n",
    "        .master(\"local[*]\")\n",
    "        .getOrCreate())\n",
    "    \n",
    "    return spark\n",
    "\n",
    "def query_neo4j(spark, query, database=\"neo4j\"):\n",
    "    \"\"\"\n",
    "    Execute a Cypher query against Neo4j using Spark\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    spark : SparkSession\n",
    "        Active Spark session\n",
    "    query : str\n",
    "        Cypher query to execute\n",
    "    database : str\n",
    "        Neo4j database name\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = (spark.read\n",
    "            .format(\"org.neo4j.spark.DataSource\")\n",
    "            .option(\"url\", spark.conf.get(\"spark.neo4j.bolt.url\"))\n",
    "            .option(\"authentication.basic.username\", \n",
    "                   spark.conf.get(\"spark.neo4j.bolt.user\"))\n",
    "            .option(\"authentication.basic.password\", \n",
    "                   spark.conf.get(\"spark.neo4j.bolt.password\"))\n",
    "            .option(\"query\", query)\n",
    "            .option(\"database\", database)\n",
    "            .load())\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error executing query: {str(e)}\")\n",
    "        raise\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    try:\n",
    "        # Initialize Spark session\n",
    "        spark = setup_spark_neo4j_connection(\n",
    "            neo4j_url=\"bolt://localhost:7687\",  # Update port if needed\n",
    "            neo4j_user=\"neo4j\",\n",
    "            neo4j_password=\"paras2003\"\n",
    "        )\n",
    "        \n",
    "        # Example query\n",
    "        query = \"\"\"\n",
    "        MATCH (p:Paper)-[:CITES]->(cited:Paper)\n",
    "        RETURN p.id AS citing_paper, cited.id AS cited_paper\n",
    "        \"\"\"\n",
    "        \n",
    "        # Execute query\n",
    "        result_df = query_neo4j(spark, query)\n",
    "        result_df.show()\n",
    "        \n",
    "    finally:\n",
    "        spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o111.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: org.neo4j.spark.DataSource. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: java.lang.ClassNotFoundException: org.neo4j.spark.DataSource.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:587)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:520)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 15 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 44\u001b[0m\n\u001b[1;32m     36\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;124mMATCH (p:Paper)-[:CITES]->(cited:Paper)\u001b[39m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;124mRETURN p.id AS citing_paper, cited.id AS cited_paper\u001b[39m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Load data from Neo4j into Spark DataFrame\u001b[39;00m\n\u001b[1;32m     42\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43morg.neo4j.spark.DataSource\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquery\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m---> 44\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Show the data in the DataFrame\u001b[39;00m\n\u001b[1;32m     47\u001b[0m df\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pyspark/sql/readwriter.py:314\u001b[0m, in \u001b[0;36mDataFrameReader.load\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jreader\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoSeq(path)))\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 314\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o111.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: org.neo4j.spark.DataSource. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: java.lang.ClassNotFoundException: org.neo4j.spark.DataSource.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:587)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:520)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 15 more\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Connection details\n",
    "url = \"bolt://localhost:7687\"\n",
    "username = \"neo4j\"\n",
    "password = \"paras2003\"\n",
    "dbname = \"neo4j\"\n",
    "\n",
    "# Create directory for JAR if needed\n",
    "home_dir = str(Path.home())\n",
    "spark_jars_dir = os.path.join(home_dir, \"spark_jars\")\n",
    "os.makedirs(spark_jars_dir, exist_ok=True)\n",
    "\n",
    "# Specify the JAR path\n",
    "jar_name = \"neo4j-connector-apache-spark_2.12-5.3.2_for_spark_3.jar\"\n",
    "jar_path = os.path.join(spark_jars_dir, jar_name)\n",
    "\n",
    "# Initialize Spark session with Neo4j connector\n",
    "spark = (SparkSession.builder\n",
    "    .appName(\"Neo4j Spark Connection\")\n",
    "    # Option 1: Using local JAR file (make sure the JAR is downloaded to the specified path)\n",
    "    .config(\"spark.jars\", jar_path)\n",
    "    # Option 2: Alternative method using packages (uncomment if preferred)\n",
    "    #.config(\"spark.jars.packages\", \"org.neo4j:neo4j-connector-apache-spark_2.12:5.3.2\")\n",
    "    # Neo4j connection configs\n",
    "    .config(\"neo4j.url\", url)\n",
    "    .config(\"neo4j.authentication.basic.username\", username)\n",
    "    .config(\"neo4j.authentication.basic.password\", password)\n",
    "    .config(\"neo4j.database\", dbname)\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# Define the Cypher query\n",
    "query = \"\"\"\n",
    "MATCH (p:Paper)-[:CITES]->(cited:Paper)\n",
    "RETURN p.id AS citing_paper, cited.id AS cited_paper\n",
    "\"\"\"\n",
    "\n",
    "# Load data from Neo4j into Spark DataFrame\n",
    "df = spark.read.format(\"org.neo4j.spark.DataSource\") \\\n",
    "    .option(\"query\", query) \\\n",
    "    .load()\n",
    "\n",
    "# Show the data in the DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Papers: 100%|| 564340/564340 [05:36<00:00, 1674.74it/s]\n"
     ]
    }
   ],
   "source": [
    "from neo4j import GraphDatabase\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load JSON data\n",
    "with open('train.json', 'r') as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "\n",
    "# Initialize Neo4j driver\n",
    "driver = GraphDatabase.driver(\"bolt://localhost:7689\", auth=(\"neo4j\", \"paras2003\"))\n",
    "\n",
    "# Function to create nodes and relationships\n",
    "def create_graph_batch(tx, batch):\n",
    "    for paper_id, references in batch:\n",
    "        tx.run(\"MERGE (p:Paper {id: $paper_id})\", paper_id=paper_id)\n",
    "        for ref in references:\n",
    "            tx.run(\"\"\"\n",
    "            MERGE (cited:Paper {id: $ref})\n",
    "            MERGE (p)-[:CITES]->(cited)\n",
    "            \"\"\", paper_id=paper_id, ref=ref)\n",
    "\n",
    "# Batch processing with progress tracking\n",
    "batch_size = 100  # Adjust as needed\n",
    "with driver.session() as session:\n",
    "    batch = []\n",
    "    for entry in tqdm(data, desc=\"Processing Papers\"):\n",
    "        paper_id = entry[\"paper\"]\n",
    "        references = entry[\"reference\"]\n",
    "        batch.append((paper_id, references))\n",
    "        \n",
    "        if len(batch) >= batch_size:\n",
    "            session.execute_write(create_graph_batch, batch)\n",
    "            batch = []\n",
    "    \n",
    "    # Process remaining data\n",
    "    if batch:\n",
    "        session.execute_write(create_graph_batch, batch)\n",
    "\n",
    "# Close the Neo4j driver\n",
    "driver.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Papers: 100%|| 564340/564340 [06:00<00:00, 1565.92it/s]\n",
      "24/11/13 10:17:00 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o287.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: neo4j. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat jdk.internal.reflect.GeneratedMethodAccessor19.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: java.lang.ClassNotFoundException: neo4j.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:587)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:520)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 14 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 69\u001b[0m\n\u001b[1;32m     56\u001b[0m spark \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     57\u001b[0m     SparkSession\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mneo4j.url\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbolt://localhost:7689\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mneo4j.authentication.basic.username\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mneo4j\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;241m.\u001b[39mgetOrCreate()\n\u001b[1;32m     62\u001b[0m )\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# Load data from Neo4j into DataFrames\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# 1. Nodes: Retrieve all Paper nodes\u001b[39;00m\n\u001b[1;32m     66\u001b[0m nodes_df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mneo4j\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43murl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbolt://localhost:7689\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquery\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMATCH (p:Paper) RETURN p.id as id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m---> 69\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# 2. Edges: Retrieve all 'CITES' relationships\u001b[39;00m\n\u001b[1;32m     72\u001b[0m edges_df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mneo4j\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbolt://localhost:7689\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMATCH (a:Paper)-[:CITES]->(b:Paper) RETURN a.id as src, b.id as dst\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;241m.\u001b[39mload()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pyspark/sql/readwriter.py:314\u001b[0m, in \u001b[0;36mDataFrameReader.load\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jreader\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoSeq(path)))\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 314\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o287.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: neo4j. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat jdk.internal.reflect.GeneratedMethodAccessor19.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: java.lang.ClassNotFoundException: neo4j.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:587)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:520)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 14 more\n"
     ]
    }
   ],
   "source": [
    "from neo4j import GraphDatabase\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load JSON data\n",
    "with open('train.json', 'r') as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "\n",
    "# Initialize Neo4j driver\n",
    "driver = GraphDatabase.driver(\"bolt://localhost:7689\", auth=(\"neo4j\", \"paras2003\"))\n",
    "\n",
    "# Function to create nodes and relationships in batches\n",
    "def create_graph_batch(tx, batch):\n",
    "    for paper_id, references in batch:\n",
    "        # Create the Paper node\n",
    "        tx.run(\"MERGE (p:Paper {id: $paper_id})\", paper_id=paper_id)\n",
    "        \n",
    "        # Create referenced papers and relationships\n",
    "        for ref in references:\n",
    "            tx.run(\"\"\"\n",
    "            MERGE (cited:Paper {id: $ref})\n",
    "            MERGE (p)-[:CITES]->(cited)\n",
    "            \"\"\", paper_id=paper_id, ref=ref)\n",
    "\n",
    "# Batch processing with progress tracking\n",
    "batch_size = 100  # Adjust as needed\n",
    "with driver.session() as session:\n",
    "    batch = []\n",
    "    for entry in tqdm(data, desc=\"Processing Papers\"):\n",
    "        paper_id = entry[\"paper\"]\n",
    "        references = entry[\"reference\"]\n",
    "        batch.append((paper_id, references))\n",
    "        \n",
    "        if len(batch) >= batch_size:\n",
    "            session.execute_write(create_graph_batch, batch)\n",
    "            batch = []\n",
    "    \n",
    "    # Process remaining data\n",
    "    if batch:\n",
    "        session.execute_write(create_graph_batch, batch)\n",
    "\n",
    "# Close the Neo4j driver\n",
    "driver.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Replace with the actual connection URI and credentials\n",
    "url = \"bolt://localhost:7689\"\n",
    "username = \"neo4j\"\n",
    "password = \"paras2003\"\n",
    "dbname = \"neo4j\"\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.config(\"neo4j.url\", url)\n",
    "    .config(\"neo4j.authentication.basic.username\", username)\n",
    "    .config(\"neo4j.authentication.basic.password\", password)\n",
    "    # .config(\"neo4j.database\", dbname)\n",
    "    .getOrCreate()\n",
    ")\n",
    "# from pyspark.sql import SparkSession\n",
    "# from graphframes import GraphFrame\n",
    "\n",
    "# # Initialize Spark session with Neo4j Spark connector\n",
    "# spark = (\n",
    "#     SparkSession.builder\n",
    "#     .appName(\"SimRank\")\n",
    "#     .config(\"spark.jars.packages\", \"org.neo4j:neo4j-spark-connector:5.3.2\")  # Ensure the Neo4j Spark connector is included\n",
    "#     .config(\"neo4j.url\", \"bolt://localhost:7689\")\n",
    "#     .config(\"neo4j.authentication.basic.username\", \"neo4j\")\n",
    "#     .config(\"neo4j.authentication.basic.password\", \"paras2003\")\n",
    "#     .config(\"neo4j.database\", \"SimRank\")  # Optional, specify database if needed\n",
    "#     .getOrCreate()\n",
    "# )\n",
    "\n",
    "# # Load data from Neo4j into DataFrames\n",
    "# # 1. Nodes: Retrieve all Paper nodes\n",
    "# nodes_df = spark.read.format(\"neo4j\") \\\n",
    "#     .option(\"url\", \"bolt://localhost:7689\") \\\n",
    "#     .option(\"query\", \"MATCH (p:Paper) RETURN p.id as id\") \\\n",
    "#     .load()\n",
    "\n",
    "# # 2. Edges: Retrieve all 'CITES' relationships\n",
    "# edges_df = spark.read.format(\"neo4j\") \\\n",
    "#     .option(\"url\", \"bolt://localhost:7689\") \\\n",
    "#     .option(\"query\", \"MATCH (a:Paper)-[:CITES]->(b:Paper) RETURN a.id as src, b.id as dst\") \\\n",
    "#     .load()\n",
    "\n",
    "# # Construct the GraphFrame\n",
    "# graph = GraphFrame(nodes_df, edges_df)\n",
    "\n",
    "# # Show the graph structure (nodes and edges)\n",
    "# print(\"Vertices (Papers):\")\n",
    "# graph.vertices.show()  # Show Paper nodes\n",
    "# print(\"Edges (Cites relationships):\")\n",
    "# graph.edges.show()     # Show CITES relationships\n",
    "\n",
    "# # Example: Calculate the number of neighbors for each Paper (degree)\n",
    "# degree_df = graph.degrees\n",
    "# degree_df.show()\n",
    "\n",
    "# # Example: Find the shortest path between two nodes (papers) - if applicable\n",
    "# # This will require you to define start and end papers (e.g., \"Paper1\", \"Paper2\")\n",
    "# # Replace 'Paper1' and 'Paper2' with actual node IDs or paper IDs.\n",
    "# shortest_paths_df = graph.shortestPaths(landmarks=[\"Paper1\", \"Paper2\"])\n",
    "# shortest_paths_df.show()\n",
    "\n",
    "# # Example: SimRank computation (optional, assuming you have a similarity measure)\n",
    "# # Note: You may need additional steps depending on the specific algorithm used for SimRank or other graph algorithms.\n",
    "# # Placeholder for SimRank or other graph analysis functions.\n",
    "# # GraphFrames does not have direct support for SimRank, but you can implement it based on node similarity, for example.\n",
    "\n",
    "# # Example of filtering nodes with degree greater than a threshold (e.g., nodes with at least 3 neighbors)\n",
    "# filtered_nodes = degree_df.filter(degree_df[\"degree\"] >= 3)\n",
    "# filtered_nodes.show()\n",
    "\n",
    "# # Optionally, you can perform more graph-related tasks here such as finding connected components,\n",
    "# # graph traversal, or clustering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/13 10:29:16 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o373.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: org.neo4j.spark.DataSource. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat jdk.internal.reflect.GeneratedMethodAccessor19.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: java.lang.ClassNotFoundException: org.neo4j.spark.DataSource.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:587)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:520)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 14 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 25\u001b[0m\n\u001b[1;32m      8\u001b[0m spark \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m      9\u001b[0m     SparkSession\u001b[38;5;241m.\u001b[39mbuilder\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.jars.packages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg.neo4j:neo4j-connector-apache-spark_2.12:4.0.0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;241m.\u001b[39mgetOrCreate()\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Example query to retrieve data from Neo4j\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# This query will match all nodes and return them; modify as needed\u001b[39;00m\n\u001b[1;32m     20\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43morg.neo4j.spark.DataSource\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43murl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauthentication.basic.username\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musername\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauthentication.basic.password\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquery\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMATCH (n) RETURN n LIMIT 10\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m---> 25\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Show the data\u001b[39;00m\n\u001b[1;32m     28\u001b[0m df\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pyspark/sql/readwriter.py:314\u001b[0m, in \u001b[0;36mDataFrameReader.load\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jreader\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoSeq(path)))\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 314\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o373.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: org.neo4j.spark.DataSource. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat jdk.internal.reflect.GeneratedMethodAccessor19.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: java.lang.ClassNotFoundException: org.neo4j.spark.DataSource.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:587)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:520)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 14 more\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "url = \"bolt://localhost:7689\"\n",
    "username = \"neo4j\"\n",
    "password = \"paras2003\"\n",
    "dbname = \"neo4j\"\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .config(\"spark.jars.packages\", \"org.neo4j:neo4j-connector-apache-spark_2.12:4.0.0\")\n",
    "    .config(\"neo4j.url\", url)\n",
    "    .config(\"neo4j.authentication.basic.username\", username)\n",
    "    .config(\"neo4j.authentication.basic.password\", password)\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "\n",
    "# Example query to retrieve data from Neo4j\n",
    "# This query will match all nodes and return them; modify as needed\n",
    "df = spark.read.format(\"org.neo4j.spark.DataSource\") \\\n",
    "    .option(\"url\", url) \\\n",
    "    .option(\"authentication.basic.username\", username) \\\n",
    "    .option(\"authentication.basic.password\", password) \\\n",
    "    .option(\"query\", \"MATCH (n) RETURN n LIMIT 10\") \\\n",
    "    .load()\n",
    "\n",
    "# Show the data\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
