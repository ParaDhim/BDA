{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/15 17:46:18 WARN Utils: Your hostname, Parass-MacBook-Air-2.local resolves to a loopback address: 127.0.0.1; using 192.168.49.38 instead (on interface en0)\n",
      "24/11/15 17:46:18 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Ivy Default Cache set to: /Users/parasdhiman/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/parasdhiman/.ivy2/jars\n",
      "graphframes#graphframes added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-bb400dc4-8c59-4786-8ffa-5cc7ff716bf6;1.0\n",
      "\tconfs: [default]\n",
      "\tfound graphframes#graphframes;0.8.2-spark3.0-s_2.12 in spark-packages\n",
      "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
      ":: resolution report :: resolve 69ms :: artifacts dl 2ms\n",
      "\t:: modules in use:\n",
      "\tgraphframes#graphframes;0.8.2-spark3.0-s_2.12 from spark-packages in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-bb400dc4-8c59-4786-8ffa-5cc7ff716bf6\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/2ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/parasdhiman/opt/anaconda3/lib/python3.9/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/15 17:46:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "/Users/parasdhiman/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/dataframe.py:168: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/wd/p8rd_3q56c3gyt1n_mf2kh8h0000gn/T/ipykernel_19444/331193242.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0mquery_nodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"2982615777\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"1556418098\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mC\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.9\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m     \u001b[0msimrank_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msimrank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_nodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"SimRank results for C={C}:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimilar_nodes\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msimrank_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/wd/p8rd_3q56c3gyt1n_mf2kh8h0000gn/T/ipykernel_19444/331193242.py\u001b[0m in \u001b[0;36msimrank\u001b[0;34m(graph, query_nodes, C, max_iterations, tolerance)\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;31m# Build adjacency lists for faster neighbor lookup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvertices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         in_neighbors_cache[v.id] = [\n\u001b[0m\u001b[1;32m    100\u001b[0m             \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0medges\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdst\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         ]\n",
      "\u001b[0;32m/var/folders/wd/p8rd_3q56c3gyt1n_mf2kh8h0000gn/T/ipykernel_19444/331193242.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvertices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         in_neighbors_cache[v.id] = [\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0medges\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdst\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         ]\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/types.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   2389\u001b[0m             \u001b[0;31m# it will be slow when it has many fields,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2390\u001b[0m             \u001b[0;31m# but this will not be used in normal cases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2391\u001b[0;31m             \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__fields__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2392\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2393\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36msignal_handler\u001b[0;34m(signal, frame)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0msignal_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mNoReturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcancelAllJobs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0;31m# see http://stackoverflow.com/questions/23206787/\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "from py2neo import Graph, Node, Relationship\n",
    "from pyspark.sql import SparkSession\n",
    "from graphframes import GraphFrame\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Connect to Neo4j\n",
    "neo4j_url = \"bolt://localhost:7689\"\n",
    "neo4j_username = \"neo4j\"\n",
    "neo4j_password = \"paras2003\"  # replace with your password\n",
    "graph = Graph(neo4j_url, auth=(neo4j_username, neo4j_password))\n",
    "\n",
    "# Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SimRank\") \\\n",
    "    .config(\"spark.jars.packages\", \"graphframes:graphframes:0.8.2-spark3.0-s_2.12\") \\\n",
    "    .getOrCreate()\n",
    "    # .config(\"spark.jars.packages\", \"neo4j-connector-apache-spark_2.12-5.3.2_for_spark_3.jar\") \\\n",
    "    \n",
    "    \n",
    "    \n",
    "# # Spark Session\n",
    "# spark = SparkSession.builder \\\n",
    "#     .appName(\"SimRank\") \\\n",
    "#     .config(\"spark.jars.packages\", \"graphframes:graphframes:0.8.2-spark3.0-s_2.12\") \\\n",
    "#     .config(\"spark.jars.packages\", \"neo4j-connector-apache-spark_2.12-5.3.2_for_spark_3.jar\") \\\n",
    "#     .getOrCreate()\n",
    "    \n",
    "\n",
    "class Neo4jHandler:\n",
    "    def __init__(self, graph, batch_size=1000):\n",
    "        self.graph = graph\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def clear_database(self):\n",
    "        \"\"\"Clear database in batches to avoid memory issues\"\"\"\n",
    "        # Get the total number of nodes in the database\n",
    "        total_nodes = self.graph.run(\"MATCH (n) RETURN count(n) as count\").evaluate()\n",
    "        deleted = 0\n",
    "        with tqdm(total=total_nodes, desc=\"Clearing database\") as pbar:\n",
    "            while True:\n",
    "                result = self.graph.run(\n",
    "                    f\"MATCH (n) WITH n LIMIT {self.batch_size} \"\n",
    "                    \"DETACH DELETE n RETURN count(n)\"\n",
    "                ).evaluate()\n",
    "                \n",
    "                if result == 0:\n",
    "                    break\n",
    "                deleted += result\n",
    "                pbar.update(result)\n",
    "        print(f\"Cleared {deleted} nodes.\")\n",
    "\n",
    "# Parse JSON data and construct citation graph in Neo4j\n",
    "def load_data_to_neo4j(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = [json.loads(line) for line in file]\n",
    "\n",
    "    # Clear existing data in Neo4j using the new method\n",
    "    handler = Neo4jHandler(graph)\n",
    "    handler.clear_database()\n",
    "    \n",
    "    # Adding tqdm for outer loop to track the progress of processing each paper\n",
    "    with tqdm(total=len(data), desc=\"Processing papers\") as pbar:\n",
    "        for entry in data:\n",
    "            paper_id = entry[\"paper\"]\n",
    "            references = entry[\"reference\"]\n",
    "            \n",
    "            # Create the citing paper node\n",
    "            paper_node = Node(\"Paper\", id=paper_id)\n",
    "            graph.merge(paper_node, \"Paper\", \"id\")\n",
    "            \n",
    "            # Create cited paper nodes and citation relationships\n",
    "            for ref in references:\n",
    "                ref_node = Node(\"Paper\", id=ref)\n",
    "                graph.merge(ref_node, \"Paper\", \"id\")\n",
    "                citation = Relationship(paper_node, \"CITES\", ref_node)\n",
    "                graph.merge(citation)\n",
    "            pbar.update(1)\n",
    "\n",
    "# Export Neo4j graph to CSV for Spark\n",
    "def export_graph_to_csv():\n",
    "    # Query nodes and edges from Neo4j\n",
    "    nodes = graph.run(\"MATCH (p:Paper) RETURN p.id AS id\").to_data_frame()\n",
    "    edges = graph.run(\"MATCH (a:Paper)-[:CITES]->(b:Paper) RETURN a.id AS src, b.id AS dst\").to_data_frame()\n",
    "\n",
    "    # Save nodes and edges to CSV\n",
    "    nodes.to_csv(\"/tmp/nodes.csv\", index=False)\n",
    "    edges.to_csv(\"/tmp/edges.csv\", index=False)\n",
    "\n",
    "# Load the graph into Spark\n",
    "def load_graph_in_spark():\n",
    "    # Load nodes and edges\n",
    "    nodes_df = spark.read.csv(\"/tmp/nodes.csv\", header=True)\n",
    "    edges_df = spark.read.csv(\"/tmp/edges.csv\", header=True)\n",
    "\n",
    "    # Create GraphFrame\n",
    "    graph = GraphFrame(nodes_df, edges_df)\n",
    "    return graph\n",
    "\n",
    "# Compute SimRank\n",
    "def simrank(graph, query_nodes, C=0.8, max_iterations=10, tolerance=1e-4):\n",
    "    # Pre-compute all in-neighbors to avoid repeated queries\n",
    "    in_neighbors_cache = {}\n",
    "    vertices = graph.vertices.collect()\n",
    "    edges = graph.edges.collect()\n",
    "    \n",
    "    # Build adjacency lists for faster neighbor lookup\n",
    "    for v in vertices:\n",
    "        in_neighbors_cache[v.id] = [\n",
    "            e.src for e in edges if e.dst == v.id\n",
    "        ]\n",
    "    \n",
    "    # Initialize similarity matrix using dictionary for sparse storage\n",
    "    similarities = defaultdict(float)\n",
    "    for node in query_nodes:\n",
    "        similarities[(node[\"id\"], node[\"id\"])] = 1.0\n",
    "    \n",
    "    # Convert to numpy array for faster computation of query node pairs\n",
    "    query_ids = [node[\"id\"] for node in query_nodes]\n",
    "    \n",
    "    with tqdm(total=max_iterations, desc=\"SimRank Iterations\") as pbar:\n",
    "        for _ in range(max_iterations):\n",
    "            new_similarities = defaultdict(float)\n",
    "            max_change = 0.0\n",
    "            \n",
    "            # Process only necessary node pairs\n",
    "            node_pairs = [\n",
    "                (u.id, v.id) for u in vertices \n",
    "                for v in vertices if u.id <= v.id  # Process unique pairs only\n",
    "            ]\n",
    "            \n",
    "            for u_id, v_id in tqdm(node_pairs, desc=\"Processing node pairs\", leave=False):\n",
    "                if u_id == v_id:\n",
    "                    new_similarities[(u_id, v_id)] = 1.0\n",
    "                    continue\n",
    "                \n",
    "                in_neighbors_u = in_neighbors_cache[u_id]\n",
    "                in_neighbors_v = in_neighbors_cache[v_id]\n",
    "                \n",
    "                if in_neighbors_u and in_neighbors_v:\n",
    "                    # Vectorized similarity computation\n",
    "                    sim_sum = sum(\n",
    "                        similarities[(n1, n2)] \n",
    "                        for n1 in in_neighbors_u \n",
    "                        for n2 in in_neighbors_v\n",
    "                    )\n",
    "                    scale = C / (len(in_neighbors_u) * len(in_neighbors_v))\n",
    "                    new_sim = scale * sim_sum\n",
    "                    new_similarities[(u_id, v_id)] = new_sim\n",
    "                    new_similarities[(v_id, u_id)] = new_sim  # Symmetry\n",
    "                    \n",
    "                    # Update max change\n",
    "                    old_sim = similarities[(u_id, v_id)]\n",
    "                    max_change = max(max_change, abs(new_sim - old_sim))\n",
    "            \n",
    "            similarities = new_similarities\n",
    "            pbar.update(1)\n",
    "            \n",
    "            if max_change < tolerance:\n",
    "                break\n",
    "    \n",
    "    # Compute results for query nodes\n",
    "    results = {}\n",
    "    for q_id in query_ids:\n",
    "        # Use numpy for faster sorting\n",
    "        sims = [(v.id, similarities[(q_id, v.id)]) \n",
    "                for v in vertices if v.id != q_id]\n",
    "        sorted_sims = sorted(sims, key=lambda x: -x[1])\n",
    "        results[q_id] = sorted_sims[:5]\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run the whole process\n",
    "# def main():\n",
    "# Step 1: Load data into Neo4j\n",
    "# load_data_to_neo4j('train.json')\n",
    "\n",
    "# Step 2: Export the graph to CSV for Spark\n",
    "export_graph_to_csv()\n",
    "\n",
    "# Step 3: Load graph in Spark\n",
    "spark_graph = load_graph_in_spark()\n",
    "\n",
    "# Step 4: Compute SimRank with different C values\n",
    "query_nodes = [{\"id\": \"2982615777\"}, {\"id\": \"1556418098\"}]\n",
    "for C in [0.7, 0.8, 0.9]:\n",
    "    simrank_results = simrank(spark_graph, query_nodes, C=C)\n",
    "    print(f\"SimRank results for C={C}:\")\n",
    "    for query, similar_nodes in simrank_results.items():\n",
    "        print(f\"Query Node {query}: {similar_nodes}\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully exported nodes to graph_nodes.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parasdhiman/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/dataframe.py:168: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully exported edges to graph_edges.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Simrank:   0%|          | 0/3 [03:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/wd/p8rd_3q56c3gyt1n_mf2kh8h0000gn/T/ipykernel_22104/2248360023.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0mquery_nodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"2982615777\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"1556418098\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mC\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.9\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Computing Simrank\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m     \u001b[0msimrank_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msimrank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_nodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"SimRank results for C={C}:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimilar_nodes\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msimrank_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/wd/p8rd_3q56c3gyt1n_mf2kh8h0000gn/T/ipykernel_22104/2248360023.py\u001b[0m in \u001b[0;36msimrank\u001b[0;34m(graph, query_nodes, C, max_iterations, tolerance)\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0medges\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medges\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvertices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0min_neighbors_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0medges\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdst\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0msimilarities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/wd/p8rd_3q56c3gyt1n_mf2kh8h0000gn/T/ipykernel_22104/2248360023.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0medges\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medges\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvertices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0min_neighbors_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0medges\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdst\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0msimilarities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/types.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   2390\u001b[0m             \u001b[0;31m# but this will not be used in normal cases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2391\u001b[0m             \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__fields__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2392\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2393\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2394\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/types.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   2372\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2373\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2374\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2375\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2376\u001b[0m             \u001b[0;31m# it will be slow when it has many fields,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from py2neo import Graph, Node, Relationship\n",
    "from pyspark.sql import SparkSession\n",
    "from graphframes import GraphFrame\n",
    "\n",
    "# Connect to Neo4j\n",
    "neo4j_url = \"bolt://localhost:7689\"\n",
    "neo4j_username = \"neo4j\"\n",
    "neo4j_password = \"paras2003\"\n",
    "graph = Graph(neo4j_url, auth=(neo4j_username, neo4j_password))\n",
    "\n",
    "# Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SimRank\") \\\n",
    "    .config(\"spark.jars.packages\", \"graphframes:graphframes:0.8.2-spark3.0-s_2.12\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "class Neo4jHandler:\n",
    "    def __init__(self, graph, batch_size=1000):\n",
    "        self.graph = graph\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def clear_database(self):\n",
    "        \"\"\"Clear database in batches to avoid memory issues\"\"\"\n",
    "        total_nodes = self.graph.run(\"MATCH (n) RETURN count(n) as count\").evaluate()\n",
    "        deleted = 0\n",
    "        with tqdm(total=total_nodes, desc=\"Clearing database\") as pbar:\n",
    "            while True:\n",
    "                result = self.graph.run(\n",
    "                    f\"MATCH (n) WITH n LIMIT {self.batch_size} \"\n",
    "                    \"DETACH DELETE n RETURN count(n)\"\n",
    "                ).evaluate()\n",
    "                if result == 0:\n",
    "                    break\n",
    "                deleted += result\n",
    "                pbar.update(result)\n",
    "        print(f\"Cleared {deleted} nodes.\")\n",
    "\n",
    "def load_data_to_neo4j(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = [json.loads(line) for line in file]\n",
    "\n",
    "    handler = Neo4jHandler(graph)\n",
    "    handler.clear_database()\n",
    "    \n",
    "    with tqdm(total=len(data), desc=\"Processing papers\") as pbar:\n",
    "        for entry in data:\n",
    "            paper_id = entry[\"paper\"]\n",
    "            references = entry[\"reference\"]\n",
    "            \n",
    "            paper_node = Node(\"Paper\", id=paper_id)\n",
    "            graph.merge(paper_node, \"Paper\", \"id\")\n",
    "            \n",
    "            for ref in references:\n",
    "                ref_node = Node(\"Paper\", id=ref)\n",
    "                graph.merge(ref_node, \"Paper\", \"id\")\n",
    "                citation = Relationship(paper_node, \"CITES\", ref_node)\n",
    "                graph.merge(citation)\n",
    "            pbar.update(1)\n",
    "\n",
    "def export_graph_to_csv():\n",
    "    nodes = graph.run(\"MATCH (p:Paper) RETURN p.id AS id\").to_data_frame()\n",
    "    edges = graph.run(\"MATCH (a:Paper)-[:CITES]->(b:Paper) RETURN a.id AS src, b.id AS dst\").to_data_frame()\n",
    "    nodes.to_csv(\"nodes.csv\", index=False)\n",
    "    edges.to_csv(\"edges.csv\", index=False)\n",
    "\n",
    "def load_graph_in_spark():\n",
    "    nodes_df = spark.read.csv(\"graph_nodes.csv\", header=True)\n",
    "    edges_df = spark.read.csv(\"graph_edges.csv\", header=True)\n",
    "    return GraphFrame(nodes_df, edges_df)\n",
    "\n",
    "import csv\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "def export_graph_from_neo4j(uri=\"neo4j://localhost:7687\", \n",
    "                            username=\"neo4j\", \n",
    "                            password=\"paras2003\",\n",
    "                            nodes_output_file=\"graph_nodes.csv\",\n",
    "                            edges_output_file=\"graph_edges.csv\"):\n",
    "    \"\"\"\n",
    "    Export Neo4j citation graph data into separate CSV files for nodes and edges without requiring APOC.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    uri : str\n",
    "        Neo4j connection URI\n",
    "    username : str\n",
    "        Neo4j username\n",
    "    password : str\n",
    "        Neo4j password\n",
    "    nodes_output_file : str\n",
    "        Path to output CSV file for nodes\n",
    "    edges_output_file : str\n",
    "        Path to output CSV file for edges\n",
    "    \"\"\"\n",
    "    driver = GraphDatabase.driver(uri, auth=(username, password))\n",
    "    \n",
    "    try:\n",
    "        with driver.session() as session:\n",
    "            # Export nodes\n",
    "            nodes_query = \"\"\"\n",
    "            MATCH (p:Paper)\n",
    "            RETURN p.id AS id\n",
    "            \"\"\"\n",
    "            nodes_result = session.run(nodes_query)\n",
    "            \n",
    "            with open(nodes_output_file, 'w', newline='') as f:\n",
    "                writer = csv.writer(f)\n",
    "                # Write header\n",
    "                writer.writerow(['id'])\n",
    "                # Write data rows\n",
    "                for record in nodes_result:\n",
    "                    writer.writerow([record['id']])\n",
    "            print(f\"Successfully exported nodes to {nodes_output_file}\")\n",
    "            \n",
    "            # Export edges\n",
    "            edges_query = \"\"\"\n",
    "            MATCH (p1:Paper)-[:CITES]->(p2:Paper)\n",
    "            RETURN p1.id AS source, p2.id AS target\n",
    "            \"\"\"\n",
    "            edges_result = session.run(edges_query)\n",
    "            \n",
    "            with open(edges_output_file, 'w', newline='') as f:\n",
    "                writer = csv.writer(f)\n",
    "                # Write header\n",
    "                writer.writerow(['src', 'dst'])\n",
    "                # Write data rows\n",
    "                for record in edges_result:\n",
    "                    writer.writerow([record['source'], record['target']])\n",
    "            print(f\"Successfully exported edges to {edges_output_file}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error exporting graph: {str(e)}\")\n",
    "    finally:\n",
    "        driver.close()\n",
    "\n",
    "# Call the function to export nodes and edges\n",
    "\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "def simrank(graph, query_nodes, C=0.8, max_iterations=10, tolerance=1e-4):\n",
    "    in_neighbors_cache = {}\n",
    "    vertices = graph.vertices.collect()\n",
    "    edges = graph.edges.collect()\n",
    "    for v in vertices:\n",
    "        in_neighbors_cache[v.id] = [e.src for e in edges if e.dst == v.id]\n",
    "    \n",
    "    similarities = defaultdict(float)\n",
    "    for node in query_nodes:\n",
    "        similarities[(node[\"id\"], node[\"id\"])] = 1.0\n",
    "    query_ids = [node[\"id\"] for node in query_nodes]\n",
    "    print(\"st\")\n",
    "    with tqdm(total=max_iterations, desc=\"SimRank Iterations\") as pbar:\n",
    "        for _ in range(max_iterations):\n",
    "            new_similarities = defaultdict(float)\n",
    "            max_change = 0.0\n",
    "            node_pairs = [\n",
    "                (u.id, v.id) for u in vertices\n",
    "                for v in vertices if u.id <= v.id\n",
    "            ]\n",
    "            for u_id, v_id in tqdm(node_pairs, desc=\"Processing node pairs\", leave=False):\n",
    "                if u_id == v_id:\n",
    "                    new_similarities[(u_id, v_id)] = 1.0\n",
    "                    continue\n",
    "                in_neighbors_u = in_neighbors_cache[u_id]\n",
    "                in_neighbors_v = in_neighbors_cache[v_id]\n",
    "                if in_neighbors_u and in_neighbors_v:\n",
    "                    sim_sum = sum(\n",
    "                        similarities[(n1, n2)]\n",
    "                        for n1 in in_neighbors_u\n",
    "                        for n2 in in_neighbors_v\n",
    "                    )\n",
    "                    scale = C / (len(in_neighbors_u) * len(in_neighbors_v))\n",
    "                    new_sim = scale * sim_sum\n",
    "                    new_similarities[(u_id, v_id)] = new_sim\n",
    "                    new_similarities[(v_id, u_id)] = new_sim\n",
    "                    old_sim = similarities[(u_id, v_id)]\n",
    "                    max_change = max(max_change, abs(new_sim - old_sim))\n",
    "            similarities = new_similarities\n",
    "            pbar.update(1)\n",
    "            if max_change < tolerance:\n",
    "                break\n",
    "    \n",
    "    results = {}\n",
    "    for q_id in query_ids:\n",
    "        sims = [(v.id, similarities[(q_id, v.id)]) for v in vertices if v.id != q_id]\n",
    "        sorted_sims = sorted(sims, key=lambda x: -x[1])\n",
    "        results[q_id] = sorted_sims[:5]\n",
    "    return results\n",
    "\n",
    "# Step 1: Load data into Neo4j\n",
    "# load_data_to_neo4j('train.json')\n",
    "\n",
    "# Step 2: Export the graph to CSV for Spark\n",
    "# export_graph_to_csv()\n",
    "export_graph_from_neo4j()\n",
    "\n",
    "# Step 3: Load graph in Spark\n",
    "spark_graph = load_graph_in_spark()\n",
    "\n",
    "# Step 4: Compute SimRank\n",
    "query_nodes = [{\"id\": \"2982615777\"}, {\"id\": \"1556418098\"}]\n",
    "for C in tqdm([0.7, 0.8, 0.9],\"Computing Simrank\"):\n",
    "    simrank_results = simrank(spark_graph, query_nodes, C=C)\n",
    "    print(f\"SimRank results for C={C}:\")\n",
    "    for query, similar_nodes in simrank_results.items():\n",
    "        print(f\"Query Node {query}: {similar_nodes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08ef5ad33cd24630bee614e0abf5f0e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing decay factors:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing SimRank with decay factor C = 0.7\n",
      "Caching in-neighbors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `source` cannot be resolved. Did you mean one of the following? [`src`, `dst`].;\n'Project ['source]\n+- Relation [src#395,dst#396] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/wd/p8rd_3q56c3gyt1n_mf2kh8h0000gn/T/ipykernel_22104/755742003.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0medges_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0medges_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumnRenamed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"source\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"src\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumnRenamed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"target\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dst\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m final_results, top_results = run_simrank_analysis(\n\u001b[0m\u001b[1;32m    168\u001b[0m     \u001b[0medges_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0mquery_nodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery_nodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/wd/p8rd_3q56c3gyt1n_mf2kh8h0000gn/T/ipykernel_22104/755742003.py\u001b[0m in \u001b[0;36mrun_simrank_analysis\u001b[0;34m(edges_df, query_nodes, decay_factors, output_dir)\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nComputing SimRank with decay factor C = {C}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mresults_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_simrank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medges_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_nodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0mresults_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'decay_factor'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mall_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/wd/p8rd_3q56c3gyt1n_mf2kh8h0000gn/T/ipykernel_22104/755742003.py\u001b[0m in \u001b[0;36mcompute_simrank\u001b[0;34m(df, query_nodes, C, max_iterations, tolerance)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;31m# Get all unique nodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m     all_nodes = set([row['node'] for row in df.select(\"source\").union(\n\u001b[0m\u001b[1;32m     81\u001b[0m         df.select(\"target\")).distinct().withColumnRenamed(\"source\", \"node\").collect()])\n\u001b[1;32m     82\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Total unique nodes: {len(all_nodes)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   3227\u001b[0m         \u001b[0;34m+\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3228\u001b[0m         \"\"\"\n\u001b[0;32m-> 3229\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3230\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `source` cannot be resolved. Did you mean one of the following? [`src`, `dst`].;\n'Project ['source]\n+- Relation [src#395,dst#396] csv\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def cache_in_neighbors(df):\n",
    "    \"\"\"Cache in-neighbors for all nodes to avoid repeated queries.\"\"\"\n",
    "    in_neighbors_df = df.groupBy('target').agg(\n",
    "        F.collect_list('source').alias('in_neighbors')\n",
    "    ).cache()\n",
    "    return {row['target']: row['in_neighbors'] for row in in_neighbors_df.collect()}\n",
    "\n",
    "def compute_simrank_similarity(a, b, in_neighbors_dict, C, max_iterations=10, tolerance=1e-4):\n",
    "    \"\"\"Compute SimRank similarity between two nodes.\"\"\"\n",
    "    if a == b:\n",
    "        return 1.0\n",
    "        \n",
    "    in_neighbors_a = in_neighbors_dict.get(a, [])\n",
    "    in_neighbors_b = in_neighbors_dict.get(b, [])\n",
    "    \n",
    "    if not in_neighbors_a or not in_neighbors_b:\n",
    "        return 0.0\n",
    "    \n",
    "    # Initialize similarity matrix for in-neighbors\n",
    "    sim_matrix = {}\n",
    "    for na in in_neighbors_a:\n",
    "        for nb in in_neighbors_b:\n",
    "            if na == nb:\n",
    "                sim_matrix[(na, nb)] = 1.0\n",
    "            else:\n",
    "                sim_matrix[(na, nb)] = 0.0\n",
    "    \n",
    "    # Iterate until convergence\n",
    "    for _ in range(max_iterations):\n",
    "        new_sim_matrix = {}\n",
    "        max_diff = 0.0\n",
    "        \n",
    "        for na in in_neighbors_a:\n",
    "            for nb in in_neighbors_b:\n",
    "                if na == nb:\n",
    "                    new_sim_matrix[(na, nb)] = 1.0\n",
    "                    continue\n",
    "                    \n",
    "                in_na = in_neighbors_dict.get(na, [])\n",
    "                in_nb = in_neighbors_dict.get(nb, [])\n",
    "                \n",
    "                if not in_na or not in_nb:\n",
    "                    new_sim_matrix[(na, nb)] = 0.0\n",
    "                    continue\n",
    "                \n",
    "                sum_sim = 0.0\n",
    "                for i in in_na:\n",
    "                    for j in in_nb:\n",
    "                        sum_sim += sim_matrix.get((i, j), 0.0)\n",
    "                \n",
    "                new_sim = (C / (len(in_na) * len(in_nb))) * sum_sim\n",
    "                new_sim_matrix[(na, nb)] = new_sim\n",
    "                max_diff = max(max_diff, abs(new_sim - sim_matrix.get((na, nb), 0.0)))\n",
    "        \n",
    "        sim_matrix = new_sim_matrix\n",
    "        if max_diff < tolerance:\n",
    "            break\n",
    "    \n",
    "    # Calculate final similarity\n",
    "    sum_sim = 0.0\n",
    "    for na in in_neighbors_a:\n",
    "        for nb in in_neighbors_b:\n",
    "            sum_sim += sim_matrix.get((na, nb), 0.0)\n",
    "    \n",
    "    return (C / (len(in_neighbors_a) * len(in_neighbors_b))) * sum_sim\n",
    "\n",
    "def compute_simrank(df, query_nodes, C=0.8, max_iterations=10, tolerance=1e-4):\n",
    "    \"\"\"Compute SimRank similarities for given query nodes.\"\"\"\n",
    "    print(\"Caching in-neighbors...\")\n",
    "    in_neighbors_dict = cache_in_neighbors(df)\n",
    "    \n",
    "    # Get all unique nodes\n",
    "    all_nodes = set([row['node'] for row in df.select(\"source\").union(\n",
    "        df.select(\"target\")).distinct().withColumnRenamed(\"source\", \"node\").collect()])\n",
    "    print(f\"Total unique nodes: {len(all_nodes)}\")\n",
    "    \n",
    "    results = []\n",
    "    for query_node in tqdm(query_nodes, desc=\"Processing query nodes\"):\n",
    "        node_results = []\n",
    "        for target_node in tqdm(all_nodes, desc=f\"Computing similarities for node {query_node}\", leave=False):\n",
    "            sim = compute_simrank_similarity(\n",
    "                query_node, \n",
    "                target_node, \n",
    "                in_neighbors_dict, \n",
    "                C,\n",
    "                max_iterations,\n",
    "                tolerance\n",
    "            )\n",
    "            node_results.append((query_node, target_node, sim))\n",
    "        results.extend(node_results)\n",
    "    \n",
    "    return pd.DataFrame(results, columns=['query_node', 'target_node', 'similarity'])\n",
    "\n",
    "def run_simrank_analysis(edges_df, query_nodes, decay_factors, output_dir=\"simrank_results\"):\n",
    "    \"\"\"Run SimRank analysis for multiple decay factors and save results.\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    for C in tqdm(decay_factors, desc=\"Processing decay factors\"):\n",
    "        print(f\"\\nComputing SimRank with decay factor C = {C}\")\n",
    "        \n",
    "        results_df = compute_simrank(edges_df, query_nodes, C=C)\n",
    "        results_df['decay_factor'] = C\n",
    "        all_results.append(results_df)\n",
    "        \n",
    "        # Save intermediate results\n",
    "        results_df.to_csv(\n",
    "            f\"{output_dir}/simrank_results_C{C}_{timestamp}.csv\",\n",
    "            index=False\n",
    "        )\n",
    "    \n",
    "    final_results = pd.concat(all_results, ignore_index=True)\n",
    "    \n",
    "    # Save complete results\n",
    "    final_results.to_csv(\n",
    "        f\"{output_dir}/simrank_all_results_{timestamp}.csv\",\n",
    "        index=False\n",
    "    )\n",
    "    \n",
    "    # Generate and save top 10 results\n",
    "    top_results = []\n",
    "    for C in decay_factors:\n",
    "        for query in query_nodes:\n",
    "            top_10 = final_results[\n",
    "                (final_results['decay_factor'] == C) & \n",
    "                (final_results['query_node'] == query)\n",
    "            ].nlargest(10, 'similarity')\n",
    "            top_10['rank'] = range(1, 11)\n",
    "            top_results.append(top_10)\n",
    "    \n",
    "    top_results_df = pd.concat(top_results, ignore_index=True)\n",
    "    top_results_df.to_csv(\n",
    "        f\"{output_dir}/simrank_top_results_{timestamp}.csv\",\n",
    "        index=False\n",
    "    )\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\nTop 5 most similar nodes for each query node and decay factor:\")\n",
    "    for C in decay_factors:\n",
    "        print(f\"\\nDecay factor C = {C}\")\n",
    "        for query in query_nodes:\n",
    "            print(f\"\\nQuery node: {query}\")\n",
    "            top_5 = top_results_df[\n",
    "                (top_results_df['decay_factor'] == C) & \n",
    "                (top_results_df['query_node'] == query)\n",
    "            ].head()\n",
    "            print(top_5[['target_node', 'similarity', 'rank']].to_string())\n",
    "    \n",
    "    return final_results, top_results_df\n",
    "\n",
    "# Run the analysis\n",
    "query_nodes = [2982615777, 1556418098]\n",
    "decay_factors = [0.7, 0.8, 0.9]\n",
    "\n",
    "# Correct column names in the edges DataFrame\n",
    "edges_df = spark.read.csv(\"graph_edges.csv\", header=True)\n",
    "edges_df = edges_df.withColumnRenamed(\"source\", \"src\").withColumnRenamed(\"target\", \"dst\")\n",
    "\n",
    "final_results, top_results = run_simrank_analysis(\n",
    "    edges_df,\n",
    "    query_nodes=query_nodes,\n",
    "    decay_factors=decay_factors,\n",
    "    output_dir=\"simrank_results\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from neo4j import GraphDatabase\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "class CitationGraphAnalyzer:\n",
    "    def __init__(self, neo4j_uri=\"bolt://localhost:7687\", \n",
    "                 neo4j_user=\"neo4j\", neo4j_password=\"paras2003\"):\n",
    "        \"\"\"Initialize with Neo4j and Spark connections\"\"\"\n",
    "        self.driver = GraphDatabase.driver(neo4j_uri, auth=(neo4j_user, neo4j_password))\n",
    "        self.spark = SparkSession.builder \\\n",
    "            .appName(\"Citation Graph Analysis\") \\\n",
    "            .config(\"spark.driver.memory\", \"4g\") \\\n",
    "            .config(\"spark.executor.memory\", \"4g\") \\\n",
    "            .config(\"spark.task.maxFailures\", \"4\") \\\n",
    "            .getOrCreate()\n",
    "\n",
    "    def get_graph_data(self):\n",
    "        \"\"\"Extract graph data from Neo4j for Spark processing\"\"\"\n",
    "        with self.driver.session() as session:\n",
    "            result = session.run(\"\"\"\n",
    "                MATCH (p1:Paper)-[:CITES]->(p2:Paper)\n",
    "                RETURN p1.id as source, p2.id as target\n",
    "            \"\"\")\n",
    "            edges = [(record[\"source\"], record[\"target\"]) for record in result]\n",
    "            return edges\n",
    "\n",
    "    def compute_simrank(self, edges_df, query_nodes, decay_factors, output_dir=\"simrank_results\", max_iterations=10, tolerance=1e-4):\n",
    "        \"\"\"Run SimRank analysis on citation graph\"\"\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        # Ensure column names are standardized\n",
    "        edges_df = edges_df.withColumnRenamed(\"src\", \"source\").withColumnRenamed(\"dst\", \"target\")\n",
    "        \n",
    "        # Cache in-neighbors\n",
    "        in_neighbors_dict = self._cache_in_neighbors(edges_df)\n",
    "        \n",
    "        all_results = []\n",
    "        \n",
    "        for C in decay_factors:\n",
    "            print(f\"\\nComputing SimRank with decay factor C = {C}\")\n",
    "            \n",
    "            results = []\n",
    "            all_nodes = set(row['source'] for row in edges_df.select(\"source\").distinct().collect())\n",
    "            all_nodes.update(row['target'] for row in edges_df.select(\"target\").distinct().collect())\n",
    "            \n",
    "            for query_node in tqdm(query_nodes, desc=\"Processing query nodes\"):\n",
    "                node_results = []\n",
    "                for target_node in tqdm(all_nodes, desc=f\"Computing similarities for node {query_node}\", leave=False):\n",
    "                    sim = self._compute_simrank_similarity(\n",
    "                        query_node,\n",
    "                        target_node,\n",
    "                        in_neighbors_dict,\n",
    "                        C,\n",
    "                        max_iterations,\n",
    "                        tolerance\n",
    "                    )\n",
    "                    node_results.append((query_node, target_node, sim))\n",
    "                results.extend(node_results)\n",
    "            \n",
    "            results_df = pd.DataFrame(results, columns=['query_node', 'target_node', 'similarity'])\n",
    "            results_df['decay_factor'] = C\n",
    "            all_results.append(results_df)\n",
    "            \n",
    "            # Save intermediate results\n",
    "            output_path = f\"{output_dir}/simrank_results_C{C}_{timestamp}.csv\"\n",
    "            results_df.to_csv(output_path, index=False)\n",
    "        \n",
    "        return self._save_and_summarize_results(all_results, query_nodes, decay_factors, timestamp, output_dir)\n",
    "\n",
    "    def _cache_in_neighbors(self, edges_df):\n",
    "        \"\"\"Cache in-neighbors for all nodes\"\"\"\n",
    "        in_neighbors = edges_df.groupBy('target').agg(F.collect_list('source').alias('in_neighbors'))\n",
    "        return {row['target']: row['in_neighbors'] for row in in_neighbors.collect()}\n",
    "\n",
    "    def _compute_simrank_similarity(self, a, b, in_neighbors_dict, C, max_iterations, tolerance):\n",
    "        \"\"\"Compute SimRank similarity between two nodes.\"\"\"\n",
    "        if a == b:\n",
    "            return 1.0\n",
    "            \n",
    "        in_neighbors_a = in_neighbors_dict.get(a, [])\n",
    "        in_neighbors_b = in_neighbors_dict.get(b, [])\n",
    "        \n",
    "        if not in_neighbors_a or not in_neighbors_b:\n",
    "            return 0.0\n",
    "        \n",
    "        # Initialize similarity matrix for in-neighbors\n",
    "        sim_matrix = {}\n",
    "        for na in in_neighbors_a:\n",
    "            for nb in in_neighbors_b:\n",
    "                sim_matrix[(na, nb)] = 0.0\n",
    "        \n",
    "        # Iterate until convergence\n",
    "        for _ in range(max_iterations):\n",
    "            new_sim_matrix = {}\n",
    "            max_diff = 0.0\n",
    "            \n",
    "            for na in in_neighbors_a:\n",
    "                for nb in in_neighbors_b:\n",
    "                    in_na = in_neighbors_dict.get(na, [])\n",
    "                    in_nb = in_neighbors_dict.get(nb, [])\n",
    "                    \n",
    "                    if not in_na or not in_nb:\n",
    "                        continue\n",
    "                    \n",
    "                    sum_sim = sum(sim_matrix.get((i, j), 0.0) for i in in_na for j in in_nb)\n",
    "                    new_sim = (C / (len(in_na) * len(in_nb))) * sum_sim\n",
    "                    new_sim_matrix[(na, nb)] = new_sim\n",
    "                    max_diff = max(max_diff, abs(new_sim - sim_matrix.get((na, nb), 0.0)))\n",
    "            \n",
    "            sim_matrix = new_sim_matrix\n",
    "            if max_diff < tolerance:\n",
    "                break\n",
    "        \n",
    "        sum_sim = sum(sim_matrix.get((na, nb), 0.0) for na in in_neighbors_a for nb in in_neighbors_b)\n",
    "        return (C / (len(in_neighbors_a) * len(in_neighbors_b))) * sum_sim\n",
    "\n",
    "    def _save_and_summarize_results(self, all_results, query_nodes, decay_factors, timestamp, output_dir):\n",
    "        \"\"\"Save and summarize final results\"\"\"\n",
    "        final_results = pd.concat(all_results, ignore_index=True)\n",
    "        final_results_path = f\"{output_dir}/final_simrank_results_{timestamp}.csv\"\n",
    "        final_results.to_csv(final_results_path, index=False)\n",
    "        \n",
    "        top_results = final_results.groupby(['query_node', 'decay_factor']).apply(\n",
    "            lambda group: group.nlargest(10, 'similarity')).reset_index(drop=True)\n",
    "        top_results_path = f\"{output_dir}/top_simrank_results_{timestamp}.csv\"\n",
    "        top_results.to_csv(top_results_path, index=False)\n",
    "        \n",
    "        return final_results, top_results\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"Close Neo4j and Spark connections\"\"\"\n",
    "        self.driver.close()\n",
    "        self.spark.stop()\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    edges_csv_path = \"graph_edges.csv\"\n",
    "    edges_df = pd.read_csv(edges_csv_path)\n",
    "    \n",
    "    spark = SparkSession.builder.appName(\"SimRank Analysis\").getOrCreate()\n",
    "    edges_sdf = spark.createDataFrame(edges_df)\n",
    "\n",
    "    analyzer = CitationGraphAnalyzer()\n",
    "\n",
    "    try:\n",
    "        query_nodes = [2982615777, 1556418098]\n",
    "        decay_factors = [0.7, 0.8, 0.9]\n",
    "        print(\"Running SimRank analysis...\")\n",
    "        final_results, top_results = analyzer.compute_simrank(\n",
    "            edges_sdf,\n",
    "            query_nodes=query_nodes,\n",
    "            decay_factors=decay_factors\n",
    "        )\n",
    "    finally:\n",
    "        analyzer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from neo4j import GraphDatabase\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "class CitationGraphAnalyzer:\n",
    "    def __init__(self, neo4j_uri=\"bolt://localhost:7687\", \n",
    "                 neo4j_user=\"neo4j\", neo4j_password=\"paras2003\"):\n",
    "        \"\"\"Initialize with Neo4j and Spark connections\"\"\"\n",
    "        self.driver = GraphDatabase.driver(neo4j_uri, auth=(neo4j_user, neo4j_password))\n",
    "        self.spark = SparkSession.builder \\\n",
    "            .appName(\"Citation Graph Analysis\") \\\n",
    "            .config(\"spark.driver.memory\", \"4g\") \\\n",
    "            .config(\"spark.executor.memory\", \"4g\") \\\n",
    "            .config(\"spark.task.maxFailures\", \"4\") \\\n",
    "            .getOrCreate()\n",
    "\n",
    "    def get_graph_data(self):\n",
    "        \"\"\"Extract graph data from Neo4j for Spark processing\"\"\"\n",
    "        with self.driver.session() as session:\n",
    "            result = session.run(\"\"\"\n",
    "                MATCH (p1:Paper)-[:CITES]->(p2:Paper)\n",
    "                RETURN p1.id as source, p2.id as target\n",
    "            \"\"\")\n",
    "            edges = [(record[\"source\"], record[\"target\"]) for record in result]\n",
    "            return edges\n",
    "\n",
    "    def compute_simrank(self, edges_df, query_nodes, decay_factors, output_dir=\"simrank_results\", max_iterations=10, tolerance=1e-4):\n",
    "        \"\"\"Run SimRank analysis on citation graph\"\"\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        # Ensure column names are standardized\n",
    "        edges_df = edges_df.withColumnRenamed(\"src\", \"source\").withColumnRenamed(\"dst\", \"target\")\n",
    "        \n",
    "        # Cache in-neighbors\n",
    "        in_neighbors_dict = self._cache_in_neighbors(edges_df)\n",
    "        \n",
    "        all_results = []\n",
    "        \n",
    "        for C in decay_factors:\n",
    "            print(f\"\\nComputing SimRank with decay factor C = {C}\")\n",
    "            \n",
    "            results = []\n",
    "            all_nodes = set(row['source'] for row in edges_df.select(\"source\").distinct().collect())\n",
    "            all_nodes.update(row['target'] for row in edges_df.select(\"target\").distinct().collect())\n",
    "            \n",
    "            for query_node in tqdm(query_nodes, desc=\"Processing query nodes\"):\n",
    "                node_results = []\n",
    "                for target_node in tqdm(all_nodes, desc=f\"Computing similarities for node {query_node}\", leave=False):\n",
    "                    sim = self._compute_simrank_similarity(\n",
    "                        query_node,\n",
    "                        target_node,\n",
    "                        in_neighbors_dict,\n",
    "                        C,\n",
    "                        max_iterations,\n",
    "                        tolerance\n",
    "                    )\n",
    "                    node_results.append((query_node, target_node, sim))\n",
    "                results.extend(node_results)\n",
    "            \n",
    "            results_df = pd.DataFrame(results, columns=['query_node', 'target_node', 'similarity'])\n",
    "            results_df['decay_factor'] = C\n",
    "            all_results.append(results_df)\n",
    "            \n",
    "            # Save intermediate results\n",
    "            output_path = f\"{output_dir}/simrank_results_C{C}_{timestamp}.csv\"\n",
    "            results_df.to_csv(output_path, index=False)\n",
    "        \n",
    "        return self._save_and_summarize_results(all_results, query_nodes, decay_factors, timestamp, output_dir)\n",
    "\n",
    "    def _cache_in_neighbors(self, edges_df):\n",
    "        \"\"\"Cache in-neighbors for all nodes\"\"\"\n",
    "        in_neighbors = edges_df.groupBy('target').agg(F.collect_list('source').alias('in_neighbors'))\n",
    "        return {row['target']: row['in_neighbors'] for row in in_neighbors.collect()}\n",
    "\n",
    "    def _compute_simrank_similarity(self, a, b, in_neighbors_dict, C, max_iterations, tolerance):\n",
    "        \"\"\"Compute SimRank similarity between two nodes.\"\"\"\n",
    "        if a == b:\n",
    "            return 1.0\n",
    "        \n",
    "        in_neighbors_a = in_neighbors_dict.get(a, [])\n",
    "        in_neighbors_b = in_neighbors_dict.get(b, [])\n",
    "        \n",
    "        if not in_neighbors_a or not in_neighbors_b:\n",
    "            return 0.0\n",
    "        \n",
    "        # Initialize similarity matrix for in-neighbors\n",
    "        sim_matrix = {}\n",
    "        for na in in_neighbors_a:\n",
    "            for nb in in_neighbors_b:\n",
    "                sim_matrix[(na, nb)] = 0.0\n",
    "        \n",
    "        # Iterate until convergence\n",
    "        for _ in range(max_iterations):\n",
    "            new_sim_matrix = {}\n",
    "            max_diff = 0.0\n",
    "            \n",
    "            for na in in_neighbors_a:\n",
    "                for nb in in_neighbors_b:\n",
    "                    in_na = in_neighbors_dict.get(na, [])\n",
    "                    in_nb = in_neighbors_dict.get(nb, [])\n",
    "                    \n",
    "                    if not in_na or not in_nb:\n",
    "                        continue\n",
    "                    \n",
    "                    sum_sim = sum(sim_matrix.get((i, j), 0.0) for i in in_na for j in in_nb)\n",
    "                    new_sim = (C / (len(in_na) * len(in_nb))) * sum_sim\n",
    "                    new_sim_matrix[(na, nb)] = new_sim\n",
    "                    max_diff = max(max_diff, abs(new_sim - sim_matrix.get((na, nb), 0.0)))\n",
    "            \n",
    "            sim_matrix = new_sim_matrix\n",
    "            if max_diff < tolerance:\n",
    "                break\n",
    "        \n",
    "        sum_sim = sum(sim_matrix.get((na, nb), 0.0) for na in in_neighbors_a for nb in in_neighbors_b)\n",
    "        normalization_factor = (len(in_neighbors_a) * len(in_neighbors_b)) or 1  # Prevent division by 0\n",
    "        return (C / normalization_factor) * sum_sim\n",
    "\n",
    "    def _save_and_summarize_results(self, all_results, query_nodes, decay_factors, timestamp, output_dir):\n",
    "        \"\"\"Save and summarize final results\"\"\"\n",
    "        final_results = pd.concat(all_results, ignore_index=True)\n",
    "        final_results_path = f\"{output_dir}/final_simrank_results_{timestamp}.csv\"\n",
    "        final_results.to_csv(final_results_path, index=False)\n",
    "        \n",
    "        top_results = final_results.groupby(['query_node', 'decay_factor']).apply(\n",
    "            lambda group: group.nlargest(10, 'similarity')).reset_index(drop=True)\n",
    "        top_results_path = f\"{output_dir}/top_simrank_results_{timestamp}.csv\"\n",
    "        top_results.to_csv(top_results_path, index=False)\n",
    "        \n",
    "        return final_results, top_results\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"Close Neo4j and Spark connections\"\"\"\n",
    "        self.driver.close()\n",
    "        self.spark.stop()\n",
    "\n",
    "    def bfs_traversal(self, start_node, depth=2):\n",
    "        \"\"\"Perform BFS to get all nodes within a given depth\"\"\"\n",
    "        with self.driver.session() as session:\n",
    "            result = session.run(\"\"\"\n",
    "                MATCH (start:Paper)-[:CITES*1..{depth}]->(p:Paper)\n",
    "                WHERE start.id = $start_node\n",
    "                RETURN p.id as node\n",
    "            \"\"\", start_node=start_node, depth=depth)\n",
    "            return [record[\"node\"] for record in result]\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    edges_csv_path = \"graph_edges.csv\"\n",
    "    edges_df = pd.read_csv(edges_csv_path)\n",
    "    \n",
    "    spark = SparkSession.builder.appName(\"SimRank Analysis\").getOrCreate()\n",
    "    edges_sdf = spark.createDataFrame(edges_df)\n",
    "\n",
    "    analyzer = CitationGraphAnalyzer()\n",
    "\n",
    "    try:\n",
    "        query_nodes = [2982615777, 1556418098]\n",
    "        decay_factors = [0.7, 0.8, 0.9]\n",
    "        print(\"Running SimRank analysis...\")\n",
    "        final_results, top_results = analyzer.compute_simrank(\n",
    "            edges_sdf,\n",
    "            query_nodes=query_nodes,\n",
    "            decay_factors=decay_factors\n",
    "        )\n",
    "    finally:\n",
    "        analyzer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
