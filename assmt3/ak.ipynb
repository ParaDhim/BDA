{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/wd/p8rd_3q56c3gyt1n_mf2kh8h0000gn/T/ipykernel_57854/1621090385.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m                             paper_id=paper_id, ref=ref)\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mcreate_graph_in_neo4j\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/wd/p8rd_3q56c3gyt1n_mf2kh8h0000gn/T/ipykernel_57854/1621090385.py\u001b[0m in \u001b[0;36mcreate_graph_in_neo4j\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_graph_in_neo4j\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train.json\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/wd/p8rd_3q56c3gyt1n_mf2kh8h0000gn/T/ipykernel_57854/1621090385.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_graph_in_neo4j\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train.json\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'json' is not defined"
     ]
    }
   ],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "# Initialize Neo4j Driver\n",
    "driver = GraphDatabase.driver(\"neo4j://localhost:7687\", auth=(\"neo4j\", \"password\"))\n",
    "\n",
    "# Parse the JSON data and create graph in Neo4j\n",
    "def create_graph_in_neo4j():\n",
    "    with open(\"train.json\") as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "    \n",
    "    with driver.session() as session:\n",
    "        for record in data:\n",
    "            paper_id = record[\"paper\"]\n",
    "            references = record[\"reference\"]\n",
    "            # Create paper nodes\n",
    "            session.run(\"MERGE (p:Paper {id: $paper_id})\", paper_id=paper_id)\n",
    "            # Create reference nodes and directed citation relationships\n",
    "            for ref in references:\n",
    "                session.run(\"MERGE (r:Paper {id: $ref})\", ref=ref)\n",
    "                session.run(\"MATCH (p:Paper {id: $paper_id}), (r:Paper {id: $ref}) \"\n",
    "                            \"MERGE (p)-[:CITES]->(r)\",\n",
    "                            paper_id=paper_id, ref=ref)\n",
    "\n",
    "create_graph_in_neo4j()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully exported graph data to graph_edges.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "# Initialize Neo4j Driver\n",
    "driver = GraphDatabase.driver(\"neo4j://localhost:7687\", auth=(\"neo4j\", \"paras2003\"))\n",
    "\n",
    "def create_graph_in_neo4j():\n",
    "    \"\"\"\n",
    "    Parses the JSON data and creates the citation graph in Neo4j.\n",
    "    Each paper is a node, and each citation is a directed edge.\n",
    "    \"\"\"\n",
    "    with open(\"train.json\") as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "    \n",
    "    with driver.session() as session:\n",
    "        for record in data:\n",
    "            paper_id = record[\"paper\"]\n",
    "            references = record[\"reference\"]\n",
    "            # Create paper nodes\n",
    "            session.run(\"MERGE (p:Paper {id: $paper_id})\", paper_id=paper_id)\n",
    "            # Create reference nodes and directed citation relationships\n",
    "            for ref in references:\n",
    "                session.run(\"MERGE (r:Paper {id: $ref})\", ref=ref)\n",
    "                session.run(\"MATCH (p:Paper {id: $paper_id}), (r:Paper {id: $ref}) \"\n",
    "                            \"MERGE (p)-[:CITES]->(r)\",\n",
    "                            paper_id=paper_id, ref=ref)\n",
    "\n",
    "# create_graph_in_neo4j()\n",
    "\n",
    "from neo4j import GraphDatabase\n",
    "import csv\n",
    "\n",
    "def export_graph_from_neo4j(uri=\"neo4j://localhost:7687\", \n",
    "                           username=\"neo4j\", \n",
    "                           password=\"paras2003\",\n",
    "                           output_file=\"graph_edges.csv\"):\n",
    "    \"\"\"\n",
    "    Export Neo4j citation graph data into a CSV file without requiring APOC.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    uri : str\n",
    "        Neo4j connection URI\n",
    "    username : str\n",
    "        Neo4j username\n",
    "    password : str\n",
    "        Neo4j password\n",
    "    output_file : str\n",
    "        Path to output CSV file\n",
    "    \"\"\"\n",
    "    driver = GraphDatabase.driver(uri, auth=(username, password))\n",
    "    \n",
    "    try:\n",
    "        with driver.session() as session:\n",
    "            # Query to get all citations\n",
    "            query = \"\"\"\n",
    "            MATCH (p1:Paper)-[:CITES]->(p2:Paper)\n",
    "            RETURN p1.id AS source, p2.id AS target\n",
    "            \"\"\"\n",
    "            \n",
    "            # Execute query and fetch results\n",
    "            result = session.run(query)\n",
    "            \n",
    "            # Write results to CSV\n",
    "            with open(output_file, 'w', newline='') as f:\n",
    "                writer = csv.writer(f)\n",
    "                # Write header\n",
    "                writer.writerow(['source', 'target'])\n",
    "                # Write data rows\n",
    "                for record in result:\n",
    "                    writer.writerow([record['source'], record['target']])\n",
    "                    \n",
    "            print(f\"Successfully exported graph data to {output_file}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error exporting graph: {str(e)}\")\n",
    "    finally:\n",
    "        driver.close()\n",
    "\n",
    "\n",
    "\n",
    "export_graph_from_neo4j()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/15 01:50:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/11/15 01:50:25 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from itertools import product\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"SimRank\").getOrCreate()\n",
    "\n",
    "# Load the graph data from CSV\n",
    "edges_df = spark.read.csv(\"graph_edges.csv\", header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decay Factor: 0.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.                             \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/parasdhiman/opt/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/Users/parasdhiman/opt/anaconda3/lib/python3.9/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/Users/parasdhiman/opt/anaconda3/lib/python3.9/socket.py\", line 704, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/wd/p8rd_3q56c3gyt1n_mf2kh8h0000gn/T/ipykernel_57854/3600932055.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mall_nodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0medges_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"source\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medges_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"target\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistinct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_nodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0msimilarity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msimrank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medges_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'source'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m             \u001b[0msimilarities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'source'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimilarity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/wd/p8rd_3q56c3gyt1n_mf2kh8h0000gn/T/ipykernel_57854/3600932055.py\u001b[0m in \u001b[0;36msimrank\u001b[0;34m(df, u, v, C, max_iterations, tolerance)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mnew_sim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# Get in-neighbors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0min_neighbors_u\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_in_neighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0min_neighbors_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_in_neighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/wd/p8rd_3q56c3gyt1n_mf2kh8h0000gn/T/ipykernel_57854/3600932055.py\u001b[0m in \u001b[0;36mget_in_neighbors\u001b[0;34m(df, node)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_in_neighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"\"\" Returns a list of in-neighbors for a given node. \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'source'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'source'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msimrank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1261\u001b[0m         \"\"\"\n\u001b[1;32m   1262\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1263\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1264\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1322\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1039\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/clientserver.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m                 \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m                 \u001b[0;31m# Happens when a the other end is dead. There might be an empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread \"serve-DataFrame\" java.net.SocketTimeoutException: Accept timed out\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:708)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:752)\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:675)\n",
      "\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:641)\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:617)\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:574)\n",
      "\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:532)\n",
      "\tat org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:65)\n"
     ]
    }
   ],
   "source": [
    "def get_in_neighbors(df, node):\n",
    "    \"\"\" Returns a list of in-neighbors for a given node. \"\"\"\n",
    "    return [row['source'] for row in df.filter(df['target'] == node).select('source').collect()]\n",
    "\n",
    "def simrank(df, u, v, C=0.8, max_iterations=10, tolerance=1e-4):\n",
    "    \"\"\"\n",
    "    Calculate the SimRank similarity between two nodes using Spark DataFrames.\n",
    "    \"\"\"\n",
    "    if u == v:\n",
    "        return 1.0\n",
    "    \n",
    "    sim = {(u, v): 0.0}\n",
    "    for iteration in range(max_iterations):\n",
    "        new_sim = sim.copy()\n",
    "        # Get in-neighbors\n",
    "        in_neighbors_u = get_in_neighbors(df, u)\n",
    "        in_neighbors_v = get_in_neighbors(df, v)\n",
    "\n",
    "        if not in_neighbors_u or not in_neighbors_v:\n",
    "            return 0.0\n",
    "\n",
    "        scale = C / (len(in_neighbors_u) * len(in_neighbors_v))\n",
    "        sim_value = scale * sum(sim.get((w, x), 0) for w, x in product(in_neighbors_u, in_neighbors_v))\n",
    "        \n",
    "        new_sim[(u, v)] = sim_value\n",
    "        \n",
    "        # Check for convergence\n",
    "        if abs(new_sim[(u, v)] - sim[(u, v)]) < tolerance:\n",
    "            break\n",
    "        \n",
    "        sim = new_sim\n",
    "    \n",
    "    return sim[(u, v)]\n",
    "\n",
    "# Example query nodes\n",
    "query_nodes = [2982615777, 1556418098]\n",
    "decay_factors = [0.7, 0.8, 0.9]\n",
    "\n",
    "# Run SimRank for each decay factor and query nodes\n",
    "for C in decay_factors:\n",
    "    print(f\"Decay Factor: {C}\")\n",
    "    for query in query_nodes:\n",
    "        # Compute similarity for query node against all other nodes in the graph\n",
    "        similarities = []\n",
    "        all_nodes = edges_df.select(\"source\").union(edges_df.select(\"target\")).distinct().collect()\n",
    "        for node in all_nodes:\n",
    "            similarity = simrank(edges_df, query, node['source'], C=C)\n",
    "            similarities.append((node['source'], similarity))\n",
    "        \n",
    "        # Sort by similarity and print top 10 most similar nodes\n",
    "        similarities = sorted(similarities, key=lambda x: x[1], reverse=True)\n",
    "        print(f\"Top similarities for node {query}:\")\n",
    "        for node, sim_value in similarities[:10]:\n",
    "            print(f\"Node: {node}, Similarity: {sim_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unique nodes...\n",
      "Total unique nodes: 359143\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7d1fc14b510420ca3c613004c9640dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Decay Factors:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3924555bfa2342f39db59703c54d4b08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/360 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/15 02:00:26 WARN CacheManager: Asked to cache already cached data.        \n",
      "24/11/15 02:00:29 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving results to CSV files...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a354bf66ab714578b3a00feaf0fc303b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results saved in directory: simrank_results\n",
      "- All results: simrank_results/simrank_all_results_20241115_020021.csv\n",
      "- Top results: simrank_results/simrank_top_results_20241115_020021.csv\n",
      "- Summary statistics: simrank_results/simrank_summary_20241115_020021.csv\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from itertools import product\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def cache_in_neighbors(df):\n",
    "    \"\"\"Cache in-neighbors for all nodes to avoid repeated queries.\"\"\"\n",
    "    in_neighbors_df = df.groupBy('target').agg(\n",
    "        F.collect_list('source').alias('in_neighbors')\n",
    "    ).cache()\n",
    "    return {row['target']: row['in_neighbors'] for row in in_neighbors_df.collect()}\n",
    "\n",
    "def batch_simrank(df, query_nodes, target_nodes, C=0.8, max_iterations=10, tolerance=1e-4, \n",
    "                  batch_size=1000):\n",
    "    \"\"\"\n",
    "    Calculate SimRank similarities for multiple node pairs in batches.\n",
    "    \"\"\"\n",
    "    # Cache in-neighbors for all nodes\n",
    "    in_neighbors_dict = cache_in_neighbors(df)\n",
    "    \n",
    "    similarities = []\n",
    "    \n",
    "    # Process in batches\n",
    "    for i in range(0, len(target_nodes), batch_size):\n",
    "        batch_targets = target_nodes[i:i + batch_size]\n",
    "        batch_results = []\n",
    "        \n",
    "        for u in query_nodes:\n",
    "            for v in batch_targets:\n",
    "                if u == v:\n",
    "                    batch_results.append((u, v, 1.0))\n",
    "                    continue\n",
    "                \n",
    "                in_neighbors_u = in_neighbors_dict.get(u, [])\n",
    "                in_neighbors_v = in_neighbors_dict.get(v, [])\n",
    "                \n",
    "                if not in_neighbors_u or not in_neighbors_v:\n",
    "                    batch_results.append((u, v, 0.0))\n",
    "                    continue\n",
    "                \n",
    "                sim = 0.0\n",
    "                for _ in range(max_iterations):\n",
    "                    scale = C / (len(in_neighbors_u) * len(in_neighbors_v))\n",
    "                    new_sim = scale * sum(\n",
    "                        sim if w == x else 0.0 \n",
    "                        for w, x in product(in_neighbors_u, in_neighbors_v)\n",
    "                    )\n",
    "                    \n",
    "                    if abs(new_sim - sim) < tolerance:\n",
    "                        break\n",
    "                    sim = new_sim\n",
    "                \n",
    "                batch_results.append((u, v, sim))\n",
    "        \n",
    "        similarities.extend(batch_results)\n",
    "    \n",
    "    return similarities\n",
    "\n",
    "def compute_and_save_similarities(edges_df, query_nodes, decay_factors, output_dir=\"simrank_results\", \n",
    "                                batch_size=1000):\n",
    "    \"\"\"\n",
    "    Compute SimRank similarities in batches and save results.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Get all unique nodes once and convert to list\n",
    "    print(\"Collecting unique nodes...\")\n",
    "    all_nodes = [row['node'] for row in edges_df.select(\"source\").union(\n",
    "        edges_df.select(\"target\")).distinct().withColumnRenamed(\n",
    "        \"source\", \"node\").collect()]\n",
    "    total_nodes = len(all_nodes)\n",
    "    print(f\"Total unique nodes: {total_nodes}\")\n",
    "    \n",
    "    all_results = []\n",
    "    top_results = []\n",
    "    \n",
    "    # Create progress bars\n",
    "    decay_pbar = tqdm(decay_factors, desc=\"Decay Factors\", position=0)\n",
    "    batch_pbar = tqdm(total=total_nodes // batch_size + 1, desc=\"Batches\", position=1)\n",
    "    \n",
    "    try:\n",
    "        for C in decay_pbar:\n",
    "            decay_pbar.set_description(f\"Decay Factor: {C}\")\n",
    "            batch_pbar.reset()\n",
    "            \n",
    "            # Process all nodes in batches\n",
    "            similarities = batch_simrank(\n",
    "                edges_df, \n",
    "                query_nodes, \n",
    "                all_nodes, \n",
    "                C=C, \n",
    "                batch_size=batch_size\n",
    "            )\n",
    "            \n",
    "            # Convert results to DataFrame\n",
    "            batch_df = pd.DataFrame(similarities, columns=['query_node', 'target_node', 'similarity'])\n",
    "            batch_df['decay_factor'] = C\n",
    "            \n",
    "            # Store all results\n",
    "            all_results.append(batch_df)\n",
    "            \n",
    "            # Get top 10 for each query node\n",
    "            for query in query_nodes:\n",
    "                query_results = batch_df[batch_df['query_node'] == query].nlargest(\n",
    "                    10, 'similarity').copy()\n",
    "                query_results['rank'] = range(1, 11)\n",
    "                top_results.append(query_results)\n",
    "            \n",
    "            batch_pbar.update(total_nodes // batch_size + 1)\n",
    "    \n",
    "    finally:\n",
    "        batch_pbar.close()\n",
    "        decay_pbar.close()\n",
    "    \n",
    "    # Combine results\n",
    "    all_results_df = pd.concat(all_results, ignore_index=True)\n",
    "    top_results_df = pd.concat(top_results, ignore_index=True)\n",
    "    \n",
    "    print(\"\\nSaving results to CSV files...\")\n",
    "    \n",
    "    # Save results with progress bars\n",
    "    with tqdm(total=3, desc=\"Saving files\") as pbar:\n",
    "        # Save all results\n",
    "        all_results_path = os.path.join(output_dir, f'simrank_all_results_{timestamp}.csv')\n",
    "        all_results_df.to_csv(all_results_path, index=False)\n",
    "        pbar.update(1)\n",
    "        \n",
    "        # Save top results\n",
    "        top_results_path = os.path.join(output_dir, f'simrank_top_results_{timestamp}.csv')\n",
    "        top_results_df.to_csv(top_results_path, index=False)\n",
    "        pbar.update(1)\n",
    "        \n",
    "        # Calculate and save summary statistics\n",
    "        summary_stats = pd.DataFrame({\n",
    "            'decay_factor': decay_factors,\n",
    "            'avg_similarity': [all_results_df[all_results_df['decay_factor'] == C]['similarity'].mean() \n",
    "                             for C in decay_factors],\n",
    "            'max_similarity': [all_results_df[all_results_df['decay_factor'] == C]['similarity'].max() \n",
    "                             for C in decay_factors],\n",
    "            'min_similarity': [all_results_df[all_results_df['decay_factor'] == C]['similarity'].min() \n",
    "                             for C in decay_factors],\n",
    "            'std_similarity': [all_results_df[all_results_df['decay_factor'] == C]['similarity'].std() \n",
    "                             for C in decay_factors]\n",
    "        })\n",
    "        \n",
    "        summary_path = os.path.join(output_dir, f'simrank_summary_{timestamp}.csv')\n",
    "        summary_stats.to_csv(summary_path, index=False)\n",
    "        pbar.update(1)\n",
    "    \n",
    "    print(f\"\\nResults saved in directory: {output_dir}\")\n",
    "    print(f\"- All results: {all_results_path}\")\n",
    "    print(f\"- Top results: {top_results_path}\")\n",
    "    print(f\"- Summary statistics: {summary_path}\")\n",
    "    \n",
    "    return all_results_df, top_results_df, summary_stats\n",
    "\n",
    "# Example usage\n",
    "# if __name__ == \"__main__\":\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"SimRank\").getOrCreate()\n",
    "\n",
    "# Load the graph data from CSV\n",
    "edges_df = spark.read.csv(\"graph_edges.csv\", header=True)\n",
    "\n",
    "# Example query nodes and decay factors\n",
    "query_nodes = [2982615777, 1556418098]\n",
    "decay_factors = [0.7, 0.8, 0.9]\n",
    "\n",
    "# Compute similarities and save results\n",
    "all_results, top_results, summary = compute_and_save_similarities(\n",
    "    edges_df,\n",
    "    query_nodes=query_nodes,\n",
    "    decay_factors=decay_factors,\n",
    "    output_dir=\"simrank_results\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "334faf53819e4b92a785294cb97b5228",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing decay factors:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing SimRank with decay factor C = 0.7\n",
      "Caching in-neighbors...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "SparkContext or SparkSession should be created first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/wd/p8rd_3q56c3gyt1n_mf2kh8h0000gn/T/ipykernel_57854/3167486631.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0mdecay_factors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.9\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m final_results, top_results = run_simrank_analysis(\n\u001b[0m\u001b[1;32m    165\u001b[0m     \u001b[0medges_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0mquery_nodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery_nodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/wd/p8rd_3q56c3gyt1n_mf2kh8h0000gn/T/ipykernel_57854/3167486631.py\u001b[0m in \u001b[0;36mrun_simrank_analysis\u001b[0;34m(edges_df, query_nodes, decay_factors, output_dir)\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nComputing SimRank with decay factor C = {C}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mresults_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_simrank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medges_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_nodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0mresults_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'decay_factor'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mall_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/wd/p8rd_3q56c3gyt1n_mf2kh8h0000gn/T/ipykernel_57854/3167486631.py\u001b[0m in \u001b[0;36mcompute_simrank\u001b[0;34m(df, query_nodes, C, max_iterations, tolerance)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;34m\"\"\"Compute SimRank similarities for given query nodes.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Caching in-neighbors...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m     \u001b[0min_neighbors_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache_in_neighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;31m# Get all unique nodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/wd/p8rd_3q56c3gyt1n_mf2kh8h0000gn/T/ipykernel_57854/3167486631.py\u001b[0m in \u001b[0;36mcache_in_neighbors\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcache_in_neighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;34m\"\"\"Cache in-neighbors for all nodes to avoid repeated queries.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     in_neighbors_df = df.groupBy('target').agg(\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'source'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'in_neighbors'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     ).cache()\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mgroupBy\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   3414\u001b[0m         \u001b[0;34m+\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3415\u001b[0m         \"\"\"\n\u001b[0;32m-> 3416\u001b[0;31m         \u001b[0mjgd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3417\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGroupedData\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m_jcols\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   2764\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2765\u001b[0m             \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2766\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jseq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_to_java_column\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2767\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2768\u001b[0m     def _sort_cols(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m_jseq\u001b[0;34m(self, cols, converter)\u001b[0m\n\u001b[1;32m   2751\u001b[0m     ) -> JavaObject:\n\u001b[1;32m   2752\u001b[0m         \u001b[0;34m\"\"\"Return a JVM Seq of Columns from a list of Column or names\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2753\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_to_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2755\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_jmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjm\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mJavaObject\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/column.py\u001b[0m in \u001b[0;36m_to_seq\u001b[0;34m(sc, cols, converter)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \"\"\"\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/column.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \"\"\"\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/column.py\u001b[0m in \u001b[0;36m_to_java_column\u001b[0;34m(col)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mjcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mjcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_column_from_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         raise PySparkTypeError(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/column.py\u001b[0m in \u001b[0;36m_create_column_from_name\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_create_column_from_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"Column\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_active_spark_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mJVMView\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mget_active_spark_context\u001b[0;34m()\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SparkContext or SparkSession should be created first.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: SparkContext or SparkSession should be created first."
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def cache_in_neighbors(df):\n",
    "    \"\"\"Cache in-neighbors for all nodes to avoid repeated queries.\"\"\"\n",
    "    in_neighbors_df = df.groupBy('target').agg(\n",
    "        F.collect_list('source').alias('in_neighbors')\n",
    "    ).cache()\n",
    "    return {row['target']: row['in_neighbors'] for row in in_neighbors_df.collect()}\n",
    "\n",
    "def compute_simrank_similarity(a, b, in_neighbors_dict, C, max_iterations=10, tolerance=1e-4):\n",
    "    \"\"\"Compute SimRank similarity between two nodes.\"\"\"\n",
    "    if a == b:\n",
    "        return 1.0\n",
    "        \n",
    "    in_neighbors_a = in_neighbors_dict.get(a, [])\n",
    "    in_neighbors_b = in_neighbors_dict.get(b, [])\n",
    "    \n",
    "    if not in_neighbors_a or not in_neighbors_b:\n",
    "        return 0.0\n",
    "    \n",
    "    # Initialize similarity matrix for in-neighbors\n",
    "    sim_matrix = {}\n",
    "    for na in in_neighbors_a:\n",
    "        for nb in in_neighbors_b:\n",
    "            if na == nb:\n",
    "                sim_matrix[(na, nb)] = 1.0\n",
    "            else:\n",
    "                sim_matrix[(na, nb)] = 0.0\n",
    "    \n",
    "    # Iterate until convergence\n",
    "    for _ in range(max_iterations):\n",
    "        new_sim_matrix = {}\n",
    "        max_diff = 0.0\n",
    "        \n",
    "        for na in in_neighbors_a:\n",
    "            for nb in in_neighbors_b:\n",
    "                if na == nb:\n",
    "                    new_sim_matrix[(na, nb)] = 1.0\n",
    "                    continue\n",
    "                    \n",
    "                in_na = in_neighbors_dict.get(na, [])\n",
    "                in_nb = in_neighbors_dict.get(nb, [])\n",
    "                \n",
    "                if not in_na or not in_nb:\n",
    "                    new_sim_matrix[(na, nb)] = 0.0\n",
    "                    continue\n",
    "                \n",
    "                sum_sim = 0.0\n",
    "                for i in in_na:\n",
    "                    for j in in_nb:\n",
    "                        sum_sim += sim_matrix.get((i, j), 0.0)\n",
    "                \n",
    "                new_sim = (C / (len(in_na) * len(in_nb))) * sum_sim\n",
    "                new_sim_matrix[(na, nb)] = new_sim\n",
    "                max_diff = max(max_diff, abs(new_sim - sim_matrix.get((na, nb), 0.0)))\n",
    "        \n",
    "        sim_matrix = new_sim_matrix\n",
    "        if max_diff < tolerance:\n",
    "            break\n",
    "    \n",
    "    # Calculate final similarity\n",
    "    sum_sim = 0.0\n",
    "    for na in in_neighbors_a:\n",
    "        for nb in in_neighbors_b:\n",
    "            sum_sim += sim_matrix.get((na, nb), 0.0)\n",
    "    \n",
    "    return (C / (len(in_neighbors_a) * len(in_neighbors_b))) * sum_sim\n",
    "\n",
    "def compute_simrank(df, query_nodes, C=0.8, max_iterations=10, tolerance=1e-4):\n",
    "    \"\"\"Compute SimRank similarities for given query nodes.\"\"\"\n",
    "    print(\"Caching in-neighbors...\")\n",
    "    in_neighbors_dict = cache_in_neighbors(df)\n",
    "    \n",
    "    # Get all unique nodes\n",
    "    all_nodes = set([row['node'] for row in df.select(\"source\").union(\n",
    "        df.select(\"target\")).distinct().withColumnRenamed(\"source\", \"node\").collect()])\n",
    "    print(f\"Total unique nodes: {len(all_nodes)}\")\n",
    "    \n",
    "    results = []\n",
    "    for query_node in tqdm(query_nodes, desc=\"Processing query nodes\"):\n",
    "        node_results = []\n",
    "        for target_node in tqdm(all_nodes, desc=f\"Computing similarities for node {query_node}\", leave=False):\n",
    "            sim = compute_simrank_similarity(\n",
    "                query_node, \n",
    "                target_node, \n",
    "                in_neighbors_dict, \n",
    "                C,\n",
    "                max_iterations,\n",
    "                tolerance\n",
    "            )\n",
    "            node_results.append((query_node, target_node, sim))\n",
    "        results.extend(node_results)\n",
    "    \n",
    "    return pd.DataFrame(results, columns=['query_node', 'target_node', 'similarity'])\n",
    "\n",
    "def run_simrank_analysis(edges_df, query_nodes, decay_factors, output_dir=\"simrank_results\"):\n",
    "    \"\"\"Run SimRank analysis for multiple decay factors and save results.\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    for C in tqdm(decay_factors, desc=\"Processing decay factors\"):\n",
    "        print(f\"\\nComputing SimRank with decay factor C = {C}\")\n",
    "        \n",
    "        results_df = compute_simrank(edges_df, query_nodes, C=C)\n",
    "        results_df['decay_factor'] = C\n",
    "        all_results.append(results_df)\n",
    "        \n",
    "        # Save intermediate results\n",
    "        results_df.to_csv(\n",
    "            f\"{output_dir}/simrank_results_C{C}_{timestamp}.csv\",\n",
    "            index=False\n",
    "        )\n",
    "    \n",
    "    final_results = pd.concat(all_results, ignore_index=True)\n",
    "    \n",
    "    # Save complete results\n",
    "    final_results.to_csv(\n",
    "        f\"{output_dir}/simrank_all_results_{timestamp}.csv\",\n",
    "        index=False\n",
    "    )\n",
    "    \n",
    "    # Generate and save top 10 results\n",
    "    top_results = []\n",
    "    for C in decay_factors:\n",
    "        for query in query_nodes:\n",
    "            top_10 = final_results[\n",
    "                (final_results['decay_factor'] == C) & \n",
    "                (final_results['query_node'] == query)\n",
    "            ].nlargest(10, 'similarity')\n",
    "            top_10['rank'] = range(1, 11)\n",
    "            top_results.append(top_10)\n",
    "    \n",
    "    top_results_df = pd.concat(top_results, ignore_index=True)\n",
    "    top_results_df.to_csv(\n",
    "        f\"{output_dir}/simrank_top_results_{timestamp}.csv\",\n",
    "        index=False\n",
    "    )\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\nTop 5 most similar nodes for each query node and decay factor:\")\n",
    "    for C in decay_factors:\n",
    "        print(f\"\\nDecay factor C = {C}\")\n",
    "        for query in query_nodes:\n",
    "            print(f\"\\nQuery node: {query}\")\n",
    "            top_5 = top_results_df[\n",
    "                (top_results_df['decay_factor'] == C) & \n",
    "                (top_results_df['query_node'] == query)\n",
    "            ].head()\n",
    "            print(top_5[['target_node', 'similarity', 'rank']].to_string())\n",
    "    \n",
    "    return final_results, top_results_df\n",
    "\n",
    "# Run the analysis\n",
    "query_nodes = [2982615777, 1556418098]\n",
    "decay_factors = [0.7, 0.8, 0.9]\n",
    "\n",
    "final_results, top_results = run_simrank_analysis(\n",
    "    edges_df,\n",
    "    query_nodes=query_nodes,\n",
    "    decay_factors=decay_factors,\n",
    "    output_dir=\"simrank_results\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/15 02:07:28 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing SimRank with decay factor C = 0.7\n",
      "Caching in-neighbors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1809:>                                                       (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique nodes: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6def2f64c2c742859a9758afd2e69f1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing query nodes:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "570ba3ca3e21454db0f9d40338757f9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing similarities for node 2982615777:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eca87f274dd4122bff149c6db638ed4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing similarities for node 1556418098:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/15 02:07:29 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved intermediate results to simrank_results/simrank_results_C0.7_20241115_020728.csv\n",
      "\n",
      "Computing SimRank with decay factor C = 0.8\n",
      "Caching in-neighbors...\n",
      "Total unique nodes: 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9795c9a419d4074a731c8d434ca2d45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing query nodes:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ed6755f10c14bcc8ef638eaf9cd3644",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing similarities for node 2982615777:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee1d9616d8c54e7f9ed0115e1ae39f8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing similarities for node 1556418098:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved intermediate results to simrank_results/simrank_results_C0.8_20241115_020728.csv\n",
      "\n",
      "Computing SimRank with decay factor C = 0.9\n",
      "Caching in-neighbors...\n",
      "Total unique nodes: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/15 02:07:29 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99ec15bded3b4bf8bd936219d6b6555b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing query nodes:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "138c7b2b1a824d4f9fe26cf97e0e87be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing similarities for node 2982615777:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2c31df160964aa888f579d3cac77188",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing similarities for node 1556418098:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved intermediate results to simrank_results/simrank_results_C0.9_20241115_020728.csv\n",
      "Saved complete results to simrank_results/simrank_all_results_20241115_020728.csv\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length of values (10) does not match length of index (3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/wd/p8rd_3q56c3gyt1n_mf2kh8h0000gn/T/ipykernel_57854/3641801083.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/wd/p8rd_3q56c3gyt1n_mf2kh8h0000gn/T/ipykernel_57854/3641801083.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# Run analysis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m     final_results, top_results = analyzer.analyze_and_save_results(\n\u001b[0m\u001b[1;32m    188\u001b[0m         \u001b[0medges_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0mquery_nodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery_nodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/wd/p8rd_3q56c3gyt1n_mf2kh8h0000gn/T/ipykernel_57854/3641801083.py\u001b[0m in \u001b[0;36manalyze_and_save_results\u001b[0;34m(self, edges_df, query_nodes, decay_factors, output_dir)\u001b[0m\n\u001b[1;32m    143\u001b[0m                 \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfinal_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'decay_factor'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfinal_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'query_node'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                 \u001b[0mtop_10\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinal_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlargest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'similarity'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m                 \u001b[0mtop_10\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rank'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m                 \u001b[0mtop_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3978\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3979\u001b[0m             \u001b[0;31m# set column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3980\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3981\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3982\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4172\u001b[0m         \u001b[0mensure\u001b[0m \u001b[0mhomogeneity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4173\u001b[0m         \"\"\"\n\u001b[0;32m-> 4174\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4176\u001b[0m         if (\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   4913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4914\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_list_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4915\u001b[0;31m             \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequire_length_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4916\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msanitize_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4917\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/common.py\u001b[0m in \u001b[0;36mrequire_length_match\u001b[0;34m(data, index)\u001b[0m\n\u001b[1;32m    569\u001b[0m     \"\"\"\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    572\u001b[0m             \u001b[0;34m\"Length of values \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m             \u001b[0;34mf\"({len(data)}) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Length of values (10) does not match length of index (3)"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "class SimRankAnalyzer:\n",
    "    def __init__(self, spark_session=None):\n",
    "        \"\"\"Initialize SimRank analyzer with optional Spark session.\"\"\"\n",
    "        self.spark = spark_session or SparkSession.builder \\\n",
    "            .appName(\"SimRank Analysis\") \\\n",
    "            .getOrCreate()\n",
    "        \n",
    "    def create_graph(self, edges_data):\n",
    "        \"\"\"Create graph from citation data.\"\"\"\n",
    "        # Convert edges data to Spark DataFrame\n",
    "        edges_df = self.spark.createDataFrame(edges_data, [\"source\", \"target\"])\n",
    "        \n",
    "        # Cache the DataFrame as we'll be using it repeatedly\n",
    "        return edges_df.cache()\n",
    "    \n",
    "    def cache_in_neighbors(self, df):\n",
    "        \"\"\"Cache in-neighbors for all nodes to avoid repeated queries.\"\"\"\n",
    "        in_neighbors_df = df.groupBy('target').agg(\n",
    "            F.collect_list('source').alias('in_neighbors')\n",
    "        ).cache()\n",
    "        return {row['target']: row['in_neighbors'] for row in in_neighbors_df.collect()}\n",
    "    \n",
    "    def compute_simrank_similarity(self, a, b, in_neighbors_dict, C, max_iterations=10, tolerance=1e-4):\n",
    "        \"\"\"Compute SimRank similarity between two nodes.\"\"\"\n",
    "        if a == b:\n",
    "            return 1.0\n",
    "            \n",
    "        in_neighbors_a = in_neighbors_dict.get(a, [])\n",
    "        in_neighbors_b = in_neighbors_dict.get(b, [])\n",
    "        \n",
    "        if not in_neighbors_a or not in_neighbors_b:\n",
    "            return 0.0\n",
    "        \n",
    "        # Initialize similarity matrix for in-neighbors\n",
    "        sim_matrix = {(na, nb): 1.0 if na == nb else 0.0 \n",
    "                     for na in in_neighbors_a \n",
    "                     for nb in in_neighbors_b}\n",
    "        \n",
    "        # Iterate until convergence\n",
    "        for _ in range(max_iterations):\n",
    "            new_sim_matrix = {}\n",
    "            max_diff = 0.0\n",
    "            \n",
    "            for na in in_neighbors_a:\n",
    "                for nb in in_neighbors_b:\n",
    "                    if na == nb:\n",
    "                        new_sim_matrix[(na, nb)] = 1.0\n",
    "                        continue\n",
    "                        \n",
    "                    in_na = in_neighbors_dict.get(na, [])\n",
    "                    in_nb = in_neighbors_dict.get(nb, [])\n",
    "                    \n",
    "                    if not in_na or not in_nb:\n",
    "                        new_sim_matrix[(na, nb)] = 0.0\n",
    "                        continue\n",
    "                    \n",
    "                    sum_sim = sum(sim_matrix.get((i, j), 0.0) \n",
    "                                for i in in_na \n",
    "                                for j in in_nb)\n",
    "                    \n",
    "                    new_sim = (C / (len(in_na) * len(in_nb))) * sum_sim\n",
    "                    new_sim_matrix[(na, nb)] = new_sim\n",
    "                    max_diff = max(max_diff, abs(new_sim - sim_matrix.get((na, nb), 0.0)))\n",
    "            \n",
    "            sim_matrix = new_sim_matrix\n",
    "            if max_diff < tolerance:\n",
    "                break\n",
    "        \n",
    "        # Calculate final similarity\n",
    "        sum_sim = sum(sim_matrix.get((na, nb), 0.0) \n",
    "                     for na in in_neighbors_a \n",
    "                     for nb in in_neighbors_b)\n",
    "        \n",
    "        return (C / (len(in_neighbors_a) * len(in_neighbors_b))) * sum_sim\n",
    "    \n",
    "    def compute_simrank(self, df, query_nodes, C=0.8, max_iterations=10, tolerance=1e-4):\n",
    "        \"\"\"Compute SimRank similarities for given query nodes.\"\"\"\n",
    "        print(\"Caching in-neighbors...\")\n",
    "        in_neighbors_dict = self.cache_in_neighbors(df)\n",
    "        \n",
    "        # Get all unique nodes\n",
    "        all_nodes = set(row['node'] for row in df.select(\"source\")\n",
    "                       .union(df.select(\"target\"))\n",
    "                       .distinct()\n",
    "                       .withColumnRenamed(\"source\", \"node\")\n",
    "                       .collect())\n",
    "        print(f\"Total unique nodes: {len(all_nodes)}\")\n",
    "        \n",
    "        results = []\n",
    "        for query_node in tqdm(query_nodes, desc=\"Processing query nodes\"):\n",
    "            node_results = []\n",
    "            for target_node in tqdm(all_nodes, desc=f\"Computing similarities for node {query_node}\", leave=False):\n",
    "                sim = self.compute_simrank_similarity(\n",
    "                    query_node, \n",
    "                    target_node, \n",
    "                    in_neighbors_dict, \n",
    "                    C,\n",
    "                    max_iterations,\n",
    "                    tolerance\n",
    "                )\n",
    "                node_results.append((query_node, target_node, sim))\n",
    "            results.extend(node_results)\n",
    "        \n",
    "        return pd.DataFrame(results, columns=['query_node', 'target_node', 'similarity'])\n",
    "    \n",
    "    def analyze_and_save_results(self, edges_df, query_nodes, decay_factors, output_dir=\"simrank_results\"):\n",
    "        \"\"\"Run SimRank analysis with multiple decay factors and save results.\"\"\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        all_results = []\n",
    "        \n",
    "        for C in decay_factors:\n",
    "            print(f\"\\nComputing SimRank with decay factor C = {C}\")\n",
    "            results_df = self.compute_simrank(edges_df, query_nodes, C=C)\n",
    "            results_df['decay_factor'] = C\n",
    "            all_results.append(results_df)\n",
    "            \n",
    "            # Save intermediate results\n",
    "            output_path = f\"{output_dir}/simrank_results_C{C}_{timestamp}.csv\"\n",
    "            results_df.to_csv(output_path, index=False)\n",
    "            print(f\"Saved intermediate results to {output_path}\")\n",
    "        \n",
    "        # Combine all results\n",
    "        final_results = pd.concat(all_results, ignore_index=True)\n",
    "        \n",
    "        # Save complete results\n",
    "        final_path = f\"{output_dir}/simrank_all_results_{timestamp}.csv\"\n",
    "        final_results.to_csv(final_path, index=False)\n",
    "        print(f\"Saved complete results to {final_path}\")\n",
    "        \n",
    "        # Generate and save top 10 results\n",
    "        top_results = []\n",
    "        for C in decay_factors:\n",
    "            for query in query_nodes:\n",
    "                mask = (final_results['decay_factor'] == C) & (final_results['query_node'] == query)\n",
    "                top_10 = final_results[mask].nlargest(10, 'similarity')\n",
    "                top_10['rank'] = range(1, 11)\n",
    "                top_results.append(top_10)\n",
    "        \n",
    "        top_results_df = pd.concat(top_results, ignore_index=True)\n",
    "        top_path = f\"{output_dir}/simrank_top_results_{timestamp}.csv\"\n",
    "        top_results_df.to_csv(top_path, index=False)\n",
    "        print(f\"Saved top results to {top_path}\")\n",
    "        \n",
    "        self._print_summary(top_results_df, decay_factors, query_nodes)\n",
    "        \n",
    "        return final_results, top_results_df\n",
    "    \n",
    "    def _print_summary(self, top_results_df, decay_factors, query_nodes):\n",
    "        \"\"\"Print summary of top results.\"\"\"\n",
    "        print(\"\\nTop 5 most similar nodes for each query node and decay factor:\")\n",
    "        for C in decay_factors:\n",
    "            print(f\"\\nDecay factor C = {C}\")\n",
    "            for query in query_nodes:\n",
    "                print(f\"\\nQuery node: {query}\")\n",
    "                mask = (top_results_df['decay_factor'] == C) & (top_results_df['query_node'] == query)\n",
    "                top_5 = top_results_df[mask].head()\n",
    "                print(top_5[['target_node', 'similarity', 'rank']].to_string())\n",
    "\n",
    "# Example usage\n",
    "def main():\n",
    "    # Initialize analyzer\n",
    "    analyzer = SimRankAnalyzer()\n",
    "    \n",
    "    # Create example edges DataFrame (replace with your actual data)\n",
    "    \n",
    "    edges_df = analyzer.create_graph(edges_data)\n",
    "    \n",
    "    # Set parameters\n",
    "    query_nodes = [2982615777, 1556418098]\n",
    "    decay_factors = [0.7, 0.8, 0.9]\n",
    "    \n",
    "    # Run analysis\n",
    "    final_results, top_results = analyzer.analyze_and_save_results(\n",
    "        edges_df,\n",
    "        query_nodes=query_nodes,\n",
    "        decay_factors=decay_factors,\n",
    "        output_dir=\"simrank_results\"\n",
    "    )\n",
    "    \n",
    "    return final_results, top_results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing SimRank with decay factor C = 0.7\n",
      "Caching in-neighbors...\n",
      "Total unique nodes: 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f106fda89da43a586382f152033731b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing query nodes:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b22d0ce6d1b4b39ad7b617099df99ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing similarities for node 2982615777:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01a83eb74f524f89949dd67d06e1d858",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing similarities for node 1556418098:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved intermediate results to simrank_results/simrank_results_C0.7_20241115_021205.csv\n",
      "\n",
      "Computing SimRank with decay factor C = 0.8\n",
      "Caching in-neighbors...\n",
      "Total unique nodes: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/15 02:12:05 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de6a969a08d245c792f4a19b3e840ee8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing query nodes:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b91976ed60814d00a1d4c0db2b5b42d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing similarities for node 2982615777:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9e7879e87424d17833a213cb5feb0d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing similarities for node 1556418098:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved intermediate results to simrank_results/simrank_results_C0.8_20241115_021205.csv\n",
      "\n",
      "Computing SimRank with decay factor C = 0.9\n",
      "Caching in-neighbors...\n",
      "Total unique nodes: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/15 02:12:06 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c3c6cef0955402b821b1ce594327025",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing query nodes:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eaf48b766a64fdc890ff2be967fa6a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing similarities for node 2982615777:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2c2d05b7834494c89600c2ab247c2fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing similarities for node 1556418098:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved intermediate results to simrank_results/simrank_results_C0.9_20241115_021205.csv\n",
      "Saved complete results to simrank_results/simrank_all_results_20241115_021205.csv\n",
      "Saved top results to simrank_results/simrank_top_results_20241115_021205.csv\n",
      "\n",
      "Top 5 most similar nodes for each query node and decay factor:\n",
      "\n",
      "Decay factor C = 0.7\n",
      "\n",
      "Query node: 2982615777\n",
      "   target_node  similarity  rank\n",
      "0   2982615777         1.0     1\n",
      "1   2044328306         0.0     2\n",
      "2   2087551257         0.0     3\n",
      "\n",
      "Query node: 1556418098\n",
      "   target_node  similarity  rank\n",
      "3   2982615777         0.0     1\n",
      "4   2044328306         0.0     2\n",
      "5   2087551257         0.0     3\n",
      "\n",
      "Decay factor C = 0.8\n",
      "\n",
      "Query node: 2982615777\n",
      "   target_node  similarity  rank\n",
      "6   2982615777         1.0     1\n",
      "7   2044328306         0.0     2\n",
      "8   2087551257         0.0     3\n",
      "\n",
      "Query node: 1556418098\n",
      "    target_node  similarity  rank\n",
      "9    2982615777         0.0     1\n",
      "10   2044328306         0.0     2\n",
      "11   2087551257         0.0     3\n",
      "\n",
      "Decay factor C = 0.9\n",
      "\n",
      "Query node: 2982615777\n",
      "    target_node  similarity  rank\n",
      "12   2982615777         1.0     1\n",
      "13   2044328306         0.0     2\n",
      "14   2087551257         0.0     3\n",
      "\n",
      "Query node: 1556418098\n",
      "    target_node  similarity  rank\n",
      "15   2982615777         0.0     1\n",
      "16   2044328306         0.0     2\n",
      "17   2087551257         0.0     3\n",
      "    query_node  target_node  similarity  decay_factor\n",
      "0   2982615777   2982615777         1.0           0.7\n",
      "1   2982615777   2044328306         0.0           0.7\n",
      "2   2982615777   2087551257         0.0           0.7\n",
      "3   1556418098   2982615777         0.0           0.7\n",
      "4   1556418098   2044328306         0.0           0.7\n",
      "5   1556418098   2087551257         0.0           0.7\n",
      "6   2982615777   2982615777         1.0           0.8\n",
      "7   2982615777   2044328306         0.0           0.8\n",
      "8   2982615777   2087551257         0.0           0.8\n",
      "9   1556418098   2982615777         0.0           0.8\n",
      "10  1556418098   2044328306         0.0           0.8\n",
      "11  1556418098   2087551257         0.0           0.8\n",
      "12  2982615777   2982615777         1.0           0.9\n",
      "13  2982615777   2044328306         0.0           0.9\n",
      "14  2982615777   2087551257         0.0           0.9\n",
      "15  1556418098   2982615777         0.0           0.9\n",
      "16  1556418098   2044328306         0.0           0.9\n",
      "17  1556418098   2087551257         0.0           0.9     query_node  target_node  similarity  decay_factor  rank\n",
      "0   2982615777   2982615777         1.0           0.7     1\n",
      "1   2982615777   2044328306         0.0           0.7     2\n",
      "2   2982615777   2087551257         0.0           0.7     3\n",
      "3   1556418098   2982615777         0.0           0.7     1\n",
      "4   1556418098   2044328306         0.0           0.7     2\n",
      "5   1556418098   2087551257         0.0           0.7     3\n",
      "6   2982615777   2982615777         1.0           0.8     1\n",
      "7   2982615777   2044328306         0.0           0.8     2\n",
      "8   2982615777   2087551257         0.0           0.8     3\n",
      "9   1556418098   2982615777         0.0           0.8     1\n",
      "10  1556418098   2044328306         0.0           0.8     2\n",
      "11  1556418098   2087551257         0.0           0.8     3\n",
      "12  2982615777   2982615777         1.0           0.9     1\n",
      "13  2982615777   2044328306         0.0           0.9     2\n",
      "14  2982615777   2087551257         0.0           0.9     3\n",
      "15  1556418098   2982615777         0.0           0.9     1\n",
      "16  1556418098   2044328306         0.0           0.9     2\n",
      "17  1556418098   2087551257         0.0           0.9     3\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "from tqdm.auto import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "class SimRankAnalyzer:\n",
    "    def __init__(self, spark_session=None):\n",
    "        \"\"\"Initialize SimRank analyzer with optional Spark session.\"\"\"\n",
    "        self.spark = spark_session or SparkSession.builder \\\n",
    "            .appName(\"SimRank Analysis\") \\\n",
    "            .getOrCreate()\n",
    "        \n",
    "    def create_graph(self, edges_data):\n",
    "        \"\"\"Create graph from citation data.\"\"\"\n",
    "        edges_df = self.spark.createDataFrame(edges_data, [\"source\", \"target\"])\n",
    "        return edges_df.cache()\n",
    "    \n",
    "    def cache_in_neighbors(self, df):\n",
    "        \"\"\"Cache in-neighbors for all nodes to avoid repeated queries.\"\"\"\n",
    "        in_neighbors_df = df.groupBy('target').agg(\n",
    "            F.collect_list('source').alias('in_neighbors')\n",
    "        ).cache()\n",
    "        return {row['target']: row['in_neighbors'] for row in in_neighbors_df.collect()}\n",
    "    \n",
    "    def compute_simrank_similarity(self, a, b, in_neighbors_dict, C, max_iterations=10, tolerance=1e-4):\n",
    "        \"\"\"Compute SimRank similarity between two nodes.\"\"\"\n",
    "        if a == b:\n",
    "            return 1.0\n",
    "            \n",
    "        in_neighbors_a = in_neighbors_dict.get(a, [])\n",
    "        in_neighbors_b = in_neighbors_dict.get(b, [])\n",
    "        \n",
    "        if not in_neighbors_a or not in_neighbors_b:\n",
    "            return 0.0\n",
    "        \n",
    "        # Initialize similarity scores for all node pairs that might be needed\n",
    "        all_nodes = set(in_neighbors_a + in_neighbors_b)\n",
    "        sim_scores = defaultdict(lambda: defaultdict(float))\n",
    "        for node in all_nodes:\n",
    "            sim_scores[node][node] = 1.0\n",
    "        \n",
    "        # SimRank iterations\n",
    "        for _ in range(max_iterations):\n",
    "            new_scores = defaultdict(lambda: defaultdict(float))\n",
    "            max_diff = 0.0\n",
    "            \n",
    "            # Update similarity for each pair of nodes\n",
    "            for na in in_neighbors_a:\n",
    "                for nb in in_neighbors_b:\n",
    "                    if na == nb:\n",
    "                        new_scores[na][nb] = 1.0\n",
    "                        continue\n",
    "                    \n",
    "                    # Get in-neighbors of the current pair\n",
    "                    in_na = in_neighbors_dict.get(na, [])\n",
    "                    in_nb = in_neighbors_dict.get(nb, [])\n",
    "                    \n",
    "                    if not in_na or not in_nb:\n",
    "                        continue\n",
    "                    \n",
    "                    # Compute similarity using the SimRank formula\n",
    "                    similarity_sum = sum(sim_scores[i][j] \n",
    "                                      for i in in_na \n",
    "                                      for j in in_nb)\n",
    "                    \n",
    "                    new_sim = (C / (len(in_na) * len(in_nb))) * similarity_sum\n",
    "                    new_scores[na][nb] = new_sim\n",
    "                    new_scores[nb][na] = new_sim  # SimRank is symmetric\n",
    "                    \n",
    "                    # Track maximum change for convergence check\n",
    "                    old_sim = sim_scores[na][nb]\n",
    "                    max_diff = max(max_diff, abs(new_sim - old_sim))\n",
    "            \n",
    "            # Update similarity scores\n",
    "            sim_scores = new_scores\n",
    "            \n",
    "            # Check for convergence\n",
    "            if max_diff < tolerance:\n",
    "                break\n",
    "        \n",
    "        # Calculate final similarity between input nodes\n",
    "        similarity_sum = sum(sim_scores[i][j] \n",
    "                           for i in in_neighbors_a \n",
    "                           for j in in_neighbors_b)\n",
    "        \n",
    "        if not in_neighbors_a or not in_neighbors_b:\n",
    "            return 0.0\n",
    "            \n",
    "        return (C / (len(in_neighbors_a) * len(in_neighbors_b))) * similarity_sum\n",
    "    \n",
    "    def compute_simrank(self, df, query_nodes, C=0.8, max_iterations=10, tolerance=1e-4):\n",
    "        \"\"\"Compute SimRank similarities for given query nodes.\"\"\"\n",
    "        print(\"Caching in-neighbors...\")\n",
    "        in_neighbors_dict = self.cache_in_neighbors(df)\n",
    "        \n",
    "        # Get all unique nodes from the graph\n",
    "        all_nodes = set(row['node'] for row in df.select(\"source\")\n",
    "                       .union(df.select(\"target\"))\n",
    "                       .distinct()\n",
    "                       .withColumnRenamed(\"source\", \"node\")\n",
    "                       .collect())\n",
    "        print(f\"Total unique nodes: {len(all_nodes)}\")\n",
    "        \n",
    "        results = []\n",
    "        for query_node in tqdm(query_nodes, desc=\"Processing query nodes\"):\n",
    "            node_results = []\n",
    "            for target_node in tqdm(all_nodes, desc=f\"Computing similarities for node {query_node}\", leave=False):\n",
    "                sim = self.compute_simrank_similarity(\n",
    "                    query_node, \n",
    "                    target_node, \n",
    "                    in_neighbors_dict, \n",
    "                    C,\n",
    "                    max_iterations,\n",
    "                    tolerance\n",
    "                )\n",
    "                node_results.append((query_node, target_node, sim))\n",
    "            results.extend(node_results)\n",
    "        \n",
    "        return pd.DataFrame(results, columns=['query_node', 'target_node', 'similarity'])\n",
    "    \n",
    "    def analyze_and_save_results(self, edges_df, query_nodes, decay_factors, output_dir=\"simrank_results\"):\n",
    "        \"\"\"Run SimRank analysis with multiple decay factors and save results.\"\"\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        all_results = []\n",
    "        \n",
    "        for C in decay_factors:\n",
    "            print(f\"\\nComputing SimRank with decay factor C = {C}\")\n",
    "            results_df = self.compute_simrank(edges_df, query_nodes, C=C)\n",
    "            results_df['decay_factor'] = C\n",
    "            all_results.append(results_df)\n",
    "            \n",
    "            output_path = f\"{output_dir}/simrank_results_C{C}_{timestamp}.csv\"\n",
    "            results_df.to_csv(output_path, index=False)\n",
    "            print(f\"Saved intermediate results to {output_path}\")\n",
    "        \n",
    "        final_results = pd.concat(all_results, ignore_index=True)\n",
    "        \n",
    "        final_path = f\"{output_dir}/simrank_all_results_{timestamp}.csv\"\n",
    "        final_results.to_csv(final_path, index=False)\n",
    "        print(f\"Saved complete results to {final_path}\")\n",
    "        \n",
    "        # Generate top results\n",
    "        top_results = []\n",
    "        for C in decay_factors:\n",
    "            for query in query_nodes:\n",
    "                mask = (final_results['decay_factor'] == C) & (final_results['query_node'] == query)\n",
    "                subset = final_results[mask].nlargest(10, 'similarity')\n",
    "                subset = subset.copy()\n",
    "                subset['rank'] = range(1, len(subset) + 1)\n",
    "                top_results.append(subset)\n",
    "        \n",
    "        top_results_df = pd.concat(top_results, ignore_index=True)\n",
    "        top_path = f\"{output_dir}/simrank_top_results_{timestamp}.csv\"\n",
    "        top_results_df.to_csv(top_path, index=False)\n",
    "        print(f\"Saved top results to {top_path}\")\n",
    "        \n",
    "        self._print_summary(top_results_df, decay_factors, query_nodes)\n",
    "        \n",
    "        return final_results, top_results_df\n",
    "    \n",
    "    def _print_summary(self, top_results_df, decay_factors, query_nodes):\n",
    "        \"\"\"Print summary of top results.\"\"\"\n",
    "        print(\"\\nTop 5 most similar nodes for each query node and decay factor:\")\n",
    "        for C in decay_factors:\n",
    "            print(f\"\\nDecay factor C = {C}\")\n",
    "            for query in query_nodes:\n",
    "                print(f\"\\nQuery node: {query}\")\n",
    "                mask = (top_results_df['decay_factor'] == C) & (top_results_df['query_node'] == query)\n",
    "                top_5 = top_results_df[mask].head()\n",
    "                if not top_5.empty:\n",
    "                    print(top_5[['target_node', 'similarity', 'rank']].to_string())\n",
    "                else:\n",
    "                    print(\"No results found\")\n",
    "\n",
    "\n",
    "analyzer = SimRankAnalyzer()\n",
    "\n",
    "# Create example edges DataFrame\n",
    "edges_data = [\n",
    "    (2982615777, 2087551257),\n",
    "    (2982615777, 2044328306),\n",
    "    (2044328306, 2087551257),  # Added more edges to show similarity\n",
    "    (2087551257, 2044328306),  # Added circular reference\n",
    "]\n",
    "\n",
    "edges_df = analyzer.create_graph(edges_data)\n",
    "\n",
    "# Set parameters\n",
    "query_nodes = [2982615777, 1556418098]\n",
    "decay_factors = [0.7, 0.8, 0.9]\n",
    "\n",
    "# Run analysis\n",
    "final_results, top_results = analyzer.analyze_and_save_results(\n",
    "    edges_df,\n",
    "    query_nodes=query_nodes,\n",
    "    decay_factors=decay_factors,\n",
    "    output_dir=\"simrank_results\"\n",
    ")\n",
    "\n",
    "print(final_results, top_results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/15 02:16:00 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Neo4j graph...\n",
      "Running SimRank analysis...\n",
      "\n",
      "Computing SimRank with decay factor C = 0.7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e050051601be4e02bf876fad44f2ab5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing query nodes:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5df5eb20c3c244fca500280658200775",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing similarities for node 2982615777:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "094ec49a6cac44368527cc7bec34b865",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing similarities for node 1556418098:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing SimRank with decay factor C = 0.8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07fbcfba096048ffbeb8b64649b4612f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing query nodes:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e64dcf3b44d4dfa97d83ba223ca868b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing similarities for node 2982615777:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac9d34616c4e4798809f4045eb5b1eff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing similarities for node 1556418098:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing SimRank with decay factor C = 0.9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3893ab7bd106450fab82b2aba7f040c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing query nodes:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "032e110c995b431ba68ae7e469d28e8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing similarities for node 2982615777:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d2421bbc4144c3aa0de91a60cc3be04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing similarities for node 1556418098:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 most similar nodes for each query node and decay factor:\n",
      "\n",
      "Decay factor C = 0.7\n",
      "\n",
      "Query node: 2982615777\n",
      "   target_node  similarity  rank\n",
      "0   2982615777         1.0     1\n",
      "1   2044328306         0.0     2\n",
      "2   2087551257         0.0     3\n",
      "\n",
      "Query node: 1556418098\n",
      "   target_node  similarity  rank\n",
      "3   2982615777         0.0     1\n",
      "4   2044328306         0.0     2\n",
      "5   2087551257         0.0     3\n",
      "\n",
      "Decay factor C = 0.8\n",
      "\n",
      "Query node: 2982615777\n",
      "   target_node  similarity  rank\n",
      "6   2982615777         1.0     1\n",
      "7   2044328306         0.0     2\n",
      "8   2087551257         0.0     3\n",
      "\n",
      "Query node: 1556418098\n",
      "    target_node  similarity  rank\n",
      "9    2982615777         0.0     1\n",
      "10   2044328306         0.0     2\n",
      "11   2087551257         0.0     3\n",
      "\n",
      "Decay factor C = 0.9\n",
      "\n",
      "Query node: 2982615777\n",
      "    target_node  similarity  rank\n",
      "12   2982615777         1.0     1\n",
      "13   2044328306         0.0     2\n",
      "14   2087551257         0.0     3\n",
      "\n",
      "Query node: 1556418098\n",
      "    target_node  similarity  rank\n",
      "15   2982615777         0.0     1\n",
      "16   2044328306         0.0     2\n",
      "17   2087551257         0.0     3\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from neo4j import GraphDatabase\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "from tqdm.auto import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "class CitationGraphAnalyzer:\n",
    "    def __init__(self, neo4j_uri=\"bolt://localhost:7687\", \n",
    "                 neo4j_user=\"neo4j\", neo4j_password=\"paras2003\"):\n",
    "        \"\"\"Initialize with Neo4j and Spark connections\"\"\"\n",
    "        # Initialize Neo4j connection\n",
    "        self.driver = GraphDatabase.driver(neo4j_uri, \n",
    "                                         auth=(neo4j_user, neo4j_password))\n",
    "        \n",
    "        # Initialize Spark session\n",
    "        self.spark = SparkSession.builder \\\n",
    "            .appName(\"Citation Graph Analysis\") \\\n",
    "            .config(\"spark.driver.memory\", \"4g\") \\\n",
    "            .config(\"spark.executor.memory\", \"4g\") \\\n",
    "            .getOrCreate()\n",
    "    \n",
    "    def create_neo4j_graph(self, papers_data):\n",
    "        \"\"\"Create graph in Neo4j from citation data\"\"\"\n",
    "        with self.driver.session() as session:\n",
    "            # Clear existing data\n",
    "            session.run(\"MATCH (n) DETACH DELETE n\")\n",
    "            \n",
    "            # Create paper nodes\n",
    "            for paper_id in papers_data.keys():\n",
    "                session.run(\"\"\"\n",
    "                    CREATE (p:Paper {id: $paper_id})\n",
    "                \"\"\", paper_id=paper_id)\n",
    "            \n",
    "            # Create citation relationships\n",
    "            for citing_paper, cited_papers in papers_data.items():\n",
    "                if cited_papers:  # Only create edges if there are references\n",
    "                    for cited_paper in cited_papers:\n",
    "                        session.run(\"\"\"\n",
    "                            MATCH (citing:Paper {id: $citing_id})\n",
    "                            MATCH (cited:Paper {id: $cited_id})\n",
    "                            CREATE (citing)-[:CITES]->(cited)\n",
    "                        \"\"\", citing_id=citing_paper, cited_id=cited_paper)\n",
    "                        \n",
    "    def get_graph_data(self):\n",
    "        \"\"\"Extract graph data from Neo4j for Spark processing\"\"\"\n",
    "        with self.driver.session() as session:\n",
    "            # Get all citation relationships\n",
    "            result = session.run(\"\"\"\n",
    "                MATCH (p1:Paper)-[:CITES]->(p2:Paper)\n",
    "                RETURN p1.id as source, p2.id as target\n",
    "            \"\"\")\n",
    "            edges = [(record[\"source\"], record[\"target\"]) for record in result]\n",
    "            \n",
    "            return edges\n",
    "    \n",
    "    def compute_simrank_similarity(self, a, b, in_neighbors_dict, C, max_iterations=10, tolerance=1e-4):\n",
    "        \"\"\"Compute SimRank similarity between two nodes\"\"\"\n",
    "        if a == b:\n",
    "            return 1.0\n",
    "            \n",
    "        in_neighbors_a = in_neighbors_dict.get(a, [])\n",
    "        in_neighbors_b = in_neighbors_dict.get(b, [])\n",
    "        \n",
    "        if not in_neighbors_a or not in_neighbors_b:\n",
    "            return 0.0\n",
    "        \n",
    "        # Initialize similarity scores\n",
    "        sim_scores = defaultdict(lambda: defaultdict(float))\n",
    "        for node in set(in_neighbors_a + in_neighbors_b):\n",
    "            sim_scores[node][node] = 1.0\n",
    "        \n",
    "        # SimRank iterations\n",
    "        for _ in range(max_iterations):\n",
    "            new_scores = defaultdict(lambda: defaultdict(float))\n",
    "            max_diff = 0.0\n",
    "            \n",
    "            for na in in_neighbors_a:\n",
    "                for nb in in_neighbors_b:\n",
    "                    if na == nb:\n",
    "                        new_scores[na][nb] = 1.0\n",
    "                        continue\n",
    "                    \n",
    "                    in_na = in_neighbors_dict.get(na, [])\n",
    "                    in_nb = in_neighbors_dict.get(nb, [])\n",
    "                    \n",
    "                    if not in_na or not in_nb:\n",
    "                        continue\n",
    "                    \n",
    "                    similarity_sum = sum(sim_scores[i][j] \n",
    "                                      for i in in_na \n",
    "                                      for j in in_nb)\n",
    "                    \n",
    "                    new_sim = (C / (len(in_na) * len(in_nb))) * similarity_sum\n",
    "                    new_scores[na][nb] = new_sim\n",
    "                    new_scores[nb][na] = new_sim\n",
    "                    \n",
    "                    max_diff = max(max_diff, abs(new_sim - sim_scores[na][nb]))\n",
    "            \n",
    "            sim_scores = new_scores\n",
    "            \n",
    "            if max_diff < tolerance:\n",
    "                break\n",
    "        \n",
    "        similarity_sum = sum(sim_scores[i][j] \n",
    "                           for i in in_neighbors_a \n",
    "                           for j in in_neighbors_b)\n",
    "        \n",
    "        if not in_neighbors_a or not in_neighbors_b:\n",
    "            return 0.0\n",
    "            \n",
    "        return (C / (len(in_neighbors_a) * len(in_neighbors_b))) * similarity_sum\n",
    "    \n",
    "    def analyze_citation_graph(self, query_nodes, decay_factors, output_dir=\"simrank_results\"):\n",
    "        \"\"\"Run SimRank analysis on citation graph\"\"\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        # Get graph data from Neo4j\n",
    "        edges = self.get_graph_data()\n",
    "        edges_df = self.spark.createDataFrame(edges, [\"source\", \"target\"])\n",
    "        \n",
    "        # Cache in-neighbors\n",
    "        in_neighbors_dict = self._cache_in_neighbors(edges_df)\n",
    "        \n",
    "        all_results = []\n",
    "        \n",
    "        for C in decay_factors:\n",
    "            print(f\"\\nComputing SimRank with decay factor C = {C}\")\n",
    "            \n",
    "            results = []\n",
    "            # Get all unique nodes\n",
    "            all_nodes = set([row['node'] for row in edges_df.select(\"source\")\n",
    "                           .union(edges_df.select(\"target\"))\n",
    "                           .distinct()\n",
    "                           .withColumnRenamed(\"source\", \"node\")\n",
    "                           .collect()])\n",
    "            \n",
    "            for query_node in tqdm(query_nodes, desc=\"Processing query nodes\"):\n",
    "                node_results = []\n",
    "                for target_node in tqdm(all_nodes, desc=f\"Computing similarities for node {query_node}\", leave=False):\n",
    "                    sim = self.compute_simrank_similarity(\n",
    "                        query_node,\n",
    "                        target_node,\n",
    "                        in_neighbors_dict,\n",
    "                        C\n",
    "                    )\n",
    "                    node_results.append((query_node, target_node, sim))\n",
    "                results.extend(node_results)\n",
    "            \n",
    "            results_df = pd.DataFrame(results, columns=['query_node', 'target_node', 'similarity'])\n",
    "            results_df['decay_factor'] = C\n",
    "            all_results.append(results_df)\n",
    "            \n",
    "            # Save intermediate results\n",
    "            output_path = f\"{output_dir}/simrank_results_C{C}_{timestamp}.csv\"\n",
    "            results_df.to_csv(output_path, index=False)\n",
    "            \n",
    "        return self._save_and_summarize_results(all_results, query_nodes, decay_factors, timestamp, output_dir)\n",
    "    \n",
    "    def _cache_in_neighbors(self, edges_df):\n",
    "        \"\"\"Cache in-neighbors for all nodes\"\"\"\n",
    "        in_neighbors = edges_df.groupBy('target').agg(\n",
    "            F.collect_list('source').alias('in_neighbors')\n",
    "        ).collect()\n",
    "        return {row['target']: row['in_neighbors'] for row in in_neighbors}\n",
    "    \n",
    "    def _save_and_summarize_results(self, all_results, query_nodes, decay_factors, timestamp, output_dir):\n",
    "        \"\"\"Save and summarize final results\"\"\"\n",
    "        final_results = pd.concat(all_results, ignore_index=True)\n",
    "        \n",
    "        # Save complete results\n",
    "        final_path = f\"{output_dir}/simrank_all_results_{timestamp}.csv\"\n",
    "        final_results.to_csv(final_path, index=False)\n",
    "        \n",
    "        # Generate top results\n",
    "        top_results = []\n",
    "        for C in decay_factors:\n",
    "            for query in query_nodes:\n",
    "                mask = (final_results['decay_factor'] == C) & (final_results['query_node'] == query)\n",
    "                subset = final_results[mask].nlargest(10, 'similarity')\n",
    "                subset = subset.copy()\n",
    "                subset['rank'] = range(1, len(subset) + 1)\n",
    "                top_results.append(subset)\n",
    "        \n",
    "        top_results_df = pd.concat(top_results, ignore_index=True)\n",
    "        top_path = f\"{output_dir}/simrank_top_results_{timestamp}.csv\"\n",
    "        top_results_df.to_csv(top_path, index=False)\n",
    "        \n",
    "        self._print_summary(top_results_df, decay_factors, query_nodes)\n",
    "        \n",
    "        return final_results, top_results_df\n",
    "    \n",
    "    def _print_summary(self, top_results_df, decay_factors, query_nodes):\n",
    "        \"\"\"Print summary of results\"\"\"\n",
    "        print(\"\\nTop 5 most similar nodes for each query node and decay factor:\")\n",
    "        for C in decay_factors:\n",
    "            print(f\"\\nDecay factor C = {C}\")\n",
    "            for query in query_nodes:\n",
    "                print(f\"\\nQuery node: {query}\")\n",
    "                mask = (top_results_df['decay_factor'] == C) & (top_results_df['query_node'] == query)\n",
    "                top_5 = top_results_df[mask].head()\n",
    "                if not top_5.empty:\n",
    "                    print(top_5[['target_node', 'similarity', 'rank']].to_string())\n",
    "                else:\n",
    "                    print(\"No results found\")\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Close Neo4j and Spark connections\"\"\"\n",
    "        self.driver.close()\n",
    "        self.spark.stop()\n",
    "\n",
    "# Example usage\n",
    "# if __name__ == \"__main__\":\n",
    "    # Sample citation data\n",
    "papers_data = {\n",
    "    2982615777: [2087551257, 2044328306],\n",
    "    2044328306: [2087551257],\n",
    "    2087551257: [2044328306],\n",
    "    1556418098: []  # Paper with no references\n",
    "}\n",
    "\n",
    "# Initialize analyzer\n",
    "analyzer = CitationGraphAnalyzer()\n",
    "\n",
    "try:\n",
    "    # Create Neo4j graph\n",
    "    print(\"Creating Neo4j graph...\")\n",
    "    analyzer.create_neo4j_graph(papers_data)\n",
    "    \n",
    "    # Run analysis\n",
    "    query_nodes = [2982615777, 1556418098]\n",
    "    decay_factors = [0.7, 0.8, 0.9]\n",
    "    \n",
    "    print(\"Running SimRank analysis...\")\n",
    "    final_results, top_results = analyzer.analyze_citation_graph(\n",
    "        query_nodes=query_nodes,\n",
    "        decay_factors=decay_factors\n",
    "    )\n",
    "    \n",
    "finally:\n",
    "    # Clean up connections\n",
    "    analyzer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/15 02:26:40 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Neo4j graph...\n",
      "Running SimRank analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing SimRank with decay factor C = 0.7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb0b77faa6d04d859d0b04894c473726",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing query nodes:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e9051a7969c4220be99b96215b1ec84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing similarities for node 2982615777:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "compute_simrank_similarity() takes 4 positional arguments but 5 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/wd/p8rd_3q56c3gyt1n_mf2kh8h0000gn/T/ipykernel_57854/1868342504.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Running SimRank analysis...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m     final_results, top_results = analyzer.analyze_citation_graph(\n\u001b[0m\u001b[1;32m    239\u001b[0m         \u001b[0mquery_nodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery_nodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0mdecay_factors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecay_factors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/wd/p8rd_3q56c3gyt1n_mf2kh8h0000gn/T/ipykernel_57854/1868342504.py\u001b[0m in \u001b[0;36manalyze_citation_graph\u001b[0;34m(self, query_nodes, decay_factors, output_dir)\u001b[0m\n\u001b[1;32m    142\u001b[0m                 \u001b[0mnode_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mtarget_node\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_nodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"Computing similarities for node {query_node}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m                     sim = self.compute_simrank_similarity(\n\u001b[0m\u001b[1;32m    145\u001b[0m                         \u001b[0mquery_node\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m                         \u001b[0mtarget_node\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: compute_simrank_similarity() takes 4 positional arguments but 5 were given"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/15 02:28:33 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Neo4j graph...\n",
      "Running SimRank analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing SimRank with decay factor C = 0.7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c982794ee304978a20758c51edb7298",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing query nodes:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7f7b32fdf824408a68570c4e7e108ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing similarities for node 2982615777:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8e699c563a54931b222b8c089384b28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing similarities for node 1556418098:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing SimRank with decay factor C = 0.8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "736892171dd74dc0a250ce1a8db87bd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing query nodes:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e167464cf1ad499789f09151a73e44e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing similarities for node 2982615777:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43ffcac8941748b893d91bd667f15545",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing similarities for node 1556418098:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing SimRank with decay factor C = 0.9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18976ae2e50f4fed99a4cf1f70bca2a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing query nodes:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef60e8022edb4e42a91c86e8ab14c168",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing similarities for node 2982615777:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a136870623e4566a65756d334f789a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing similarities for node 1556418098:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 most similar nodes for each query node and decay factor:\n",
      "\n",
      "Decay factor C = 0.7\n",
      "\n",
      "Query node: 2982615777\n",
      "   target_node  similarity  rank\n",
      "0   2982615777         1.0     1\n",
      "1   2044328306         0.0     2\n",
      "2   2087551257         0.0     3\n",
      "\n",
      "Query node: 1556418098\n",
      "   target_node  similarity  rank\n",
      "3   2982615777         0.0     1\n",
      "4   2044328306         0.0     2\n",
      "5   2087551257         0.0     3\n",
      "\n",
      "Decay factor C = 0.8\n",
      "\n",
      "Query node: 2982615777\n",
      "   target_node  similarity  rank\n",
      "6   2982615777         1.0     1\n",
      "7   2044328306         0.0     2\n",
      "8   2087551257         0.0     3\n",
      "\n",
      "Query node: 1556418098\n",
      "    target_node  similarity  rank\n",
      "9    2982615777         0.0     1\n",
      "10   2044328306         0.0     2\n",
      "11   2087551257         0.0     3\n",
      "\n",
      "Decay factor C = 0.9\n",
      "\n",
      "Query node: 2982615777\n",
      "    target_node  similarity  rank\n",
      "12   2982615777         1.0     1\n",
      "13   2044328306         0.0     2\n",
      "14   2087551257         0.0     3\n",
      "\n",
      "Query node: 1556418098\n",
      "    target_node  similarity  rank\n",
      "15   2982615777         0.0     1\n",
      "16   2044328306         0.0     2\n",
      "17   2087551257         0.0     3\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from neo4j import GraphDatabase\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "from tqdm.auto import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "class CitationGraphAnalyzer:\n",
    "    def __init__(self, neo4j_uri=\"bolt://localhost:7687\", \n",
    "                 neo4j_user=\"neo4j\", neo4j_password=\"paras2003\"):\n",
    "        \"\"\"Initialize with Neo4j and Spark connections\"\"\"\n",
    "        # Initialize Neo4j connection\n",
    "        self.driver = GraphDatabase.driver(neo4j_uri, \n",
    "                                         auth=(neo4j_user, neo4j_password))\n",
    "        \n",
    "        # Initialize Spark session\n",
    "        self.spark = SparkSession.builder \\\n",
    "            .appName(\"Citation Graph Analysis\") \\\n",
    "            .config(\"spark.driver.memory\", \"4g\") \\\n",
    "            .config(\"spark.executor.memory\", \"4g\") \\\n",
    "            .getOrCreate()\n",
    "    \n",
    "    def create_neo4j_graph(self, papers_data):\n",
    "        \"\"\"Create graph in Neo4j from citation data\"\"\"\n",
    "        with self.driver.session() as session:\n",
    "            # Clear existing data\n",
    "            session.run(\"MATCH (n) DETACH DELETE n\")\n",
    "            \n",
    "            # Create paper nodes\n",
    "            for paper_id in papers_data.keys():\n",
    "                session.run(\"\"\"\n",
    "                    CREATE (p:Paper {id: $paper_id})\n",
    "                \"\"\", paper_id=paper_id)\n",
    "            \n",
    "            # Create citation relationships\n",
    "            for citing_paper, cited_papers in papers_data.items():\n",
    "                if cited_papers:  # Only create edges if there are references\n",
    "                    for cited_paper in cited_papers:\n",
    "                        session.run(\"\"\"\n",
    "                            MATCH (citing:Paper {id: $citing_id})\n",
    "                            MATCH (cited:Paper {id: $cited_id})\n",
    "                            CREATE (citing)-[:CITES]->(cited)\n",
    "                        \"\"\", citing_id=citing_paper, cited_id=cited_paper)\n",
    "                        \n",
    "    def get_graph_data(self):\n",
    "        \"\"\"Extract graph data from Neo4j for Spark processing\"\"\"\n",
    "        with self.driver.session() as session:\n",
    "            # Get all citation relationships\n",
    "            result = session.run(\"\"\"\n",
    "                MATCH (p1:Paper)-[:CITES]->(p2:Paper)\n",
    "                RETURN p1.id as source, p2.id as target\n",
    "            \"\"\")\n",
    "            edges = [(record[\"source\"], record[\"target\"]) for record in result]\n",
    "            \n",
    "            return edges\n",
    "    \n",
    "    from pyspark.sql import functions as F\n",
    "\n",
    "    def compute_simrank_similarity(self, a, b, in_neighbors_dict, C):\n",
    "        \"\"\"Compute SimRank similarity between two nodes\"\"\"\n",
    "        if a == b:\n",
    "            return 1.0  # self-similarity is always 1.0\n",
    "        \n",
    "        in_neighbors_a = in_neighbors_dict.get(a, [])\n",
    "        in_neighbors_b = in_neighbors_dict.get(b, [])\n",
    "        \n",
    "        if not in_neighbors_a or not in_neighbors_b:\n",
    "            return 0.0\n",
    "        \n",
    "        # Initialize similarity scores\n",
    "        sim_scores = defaultdict(lambda: defaultdict(float))\n",
    "        for node in set(in_neighbors_a + in_neighbors_b):\n",
    "            sim_scores[node][node] = 1.0\n",
    "        \n",
    "        # SimRank iterations (you can adjust max_iterations/tolerance as needed)\n",
    "        max_iterations = 10\n",
    "        tolerance = 1e-4\n",
    "        for _ in range(max_iterations):\n",
    "            new_scores = defaultdict(lambda: defaultdict(float))\n",
    "            max_diff = 0.0\n",
    "            \n",
    "            for na in in_neighbors_a:\n",
    "                for nb in in_neighbors_b:\n",
    "                    if na == nb:\n",
    "                        new_scores[na][nb] = 1.0\n",
    "                        continue\n",
    "                    \n",
    "                    in_na = in_neighbors_dict.get(na, [])\n",
    "                    in_nb = in_neighbors_dict.get(nb, [])\n",
    "                    \n",
    "                    if not in_na or not in_nb:\n",
    "                        continue\n",
    "                    \n",
    "                    similarity_sum = sum(sim_scores[i][j] \n",
    "                                        for i in in_na \n",
    "                                        for j in in_nb)\n",
    "                    \n",
    "                    new_sim = (C / (len(in_na) * len(in_nb))) * similarity_sum\n",
    "                    new_scores[na][nb] = new_sim\n",
    "                    new_scores[nb][na] = new_sim\n",
    "                    \n",
    "                    max_diff = max(max_diff, abs(new_sim - sim_scores[na][nb]))\n",
    "            \n",
    "            sim_scores = new_scores\n",
    "            \n",
    "            if max_diff < tolerance:\n",
    "                break\n",
    "        \n",
    "        similarity_sum = sum(sim_scores[i][j] \n",
    "                            for i in in_neighbors_a \n",
    "                            for j in in_neighbors_b)\n",
    "        \n",
    "        return (C / (len(in_neighbors_a) * len(in_neighbors_b))) * similarity_sum\n",
    "\n",
    "\n",
    "    def analyze_citation_graph(self, query_nodes, decay_factors, output_dir=\"simrank_results\"):\n",
    "        \"\"\"Run SimRank analysis on citation graph\"\"\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        assert os.path.isdir(output_dir), f\"Could not create or access directory: {output_dir}\"\n",
    "\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        # Get graph data from Neo4j\n",
    "        edges = self.get_graph_data()\n",
    "        edges_df = self.spark.createDataFrame(edges, [\"source\", \"target\"])\n",
    "        \n",
    "        # Cache in-neighbors\n",
    "        in_neighbors_dict = self._cache_in_neighbors(edges_df)\n",
    "        \n",
    "        all_results = []\n",
    "        \n",
    "        for C in decay_factors:\n",
    "            print(f\"\\nComputing SimRank with decay factor C = {C}\")\n",
    "            \n",
    "            results = []\n",
    "            # Get all unique nodes\n",
    "            all_nodes = set(row['source'] for row in edges_df.select(\"source\").distinct().collect())\n",
    "            all_nodes.update(row['target'] for row in edges_df.select(\"target\").distinct().collect())\n",
    "            \n",
    "            for query_node in tqdm(query_nodes, desc=\"Processing query nodes\"):\n",
    "                node_results = []\n",
    "                for target_node in tqdm(all_nodes, desc=f\"Computing similarities for node {query_node}\", leave=False):\n",
    "                    sim = self.compute_simrank_similarity(\n",
    "                        query_node,\n",
    "                        target_node,\n",
    "                        in_neighbors_dict,\n",
    "                        C\n",
    "                    )\n",
    "                    node_results.append((query_node, target_node, sim))\n",
    "                results.extend(node_results)\n",
    "            \n",
    "            results_df = pd.DataFrame(results, columns=['query_node', 'target_node', 'similarity'])\n",
    "            results_df['decay_factor'] = C\n",
    "            all_results.append(results_df)\n",
    "            \n",
    "            # Save intermediate results\n",
    "            output_path = f\"{output_dir}/simrank_results_C{C}_{timestamp}.csv\"\n",
    "            results_df.to_csv(output_path, index=False)\n",
    "        \n",
    "        return self._save_and_summarize_results(all_results, query_nodes, decay_factors, timestamp, output_dir)\n",
    "\n",
    "    \n",
    "    def _cache_in_neighbors(self, edges_df):\n",
    "        \"\"\"Cache in-neighbors for all nodes\"\"\"\n",
    "        in_neighbors = edges_df.groupBy('target').agg(F.collect_list('source').alias('in_neighbors'))\n",
    "        return {row['target']: row['in_neighbors'] for row in in_neighbors.collect()}\n",
    "\n",
    "    \n",
    "    def _save_and_summarize_results(self, all_results, query_nodes, decay_factors, timestamp, output_dir):\n",
    "        \"\"\"Save and summarize final results\"\"\"\n",
    "        final_results = pd.concat(all_results, ignore_index=True)\n",
    "        \n",
    "        # Save complete results\n",
    "        final_path = f\"{output_dir}/simrank_all_results_{timestamp}.csv\"\n",
    "        final_results.to_csv(final_path, index=False)\n",
    "        \n",
    "        # Generate top results\n",
    "        top_results = []\n",
    "        for C in decay_factors:\n",
    "            for query in query_nodes:\n",
    "                mask = (final_results['decay_factor'] == C) & (final_results['query_node'] == query)\n",
    "                subset = final_results[mask].nlargest(10, 'similarity')\n",
    "                subset = subset.copy()\n",
    "                subset['rank'] = range(1, len(subset) + 1)\n",
    "                top_results.append(subset)\n",
    "        \n",
    "        top_results_df = pd.concat(top_results, ignore_index=True)\n",
    "        top_path = f\"{output_dir}/simrank_top_results_{timestamp}.csv\"\n",
    "        top_results_df.to_csv(top_path, index=False)\n",
    "        \n",
    "        self._print_summary(top_results_df, decay_factors, query_nodes)\n",
    "        \n",
    "        return final_results, top_results_df\n",
    "    \n",
    "    def _print_summary(self, top_results_df, decay_factors, query_nodes):\n",
    "        \"\"\"Print summary of results\"\"\"\n",
    "        print(\"\\nTop 5 most similar nodes for each query node and decay factor:\")\n",
    "        for C in decay_factors:\n",
    "            print(f\"\\nDecay factor C = {C}\")\n",
    "            for query in query_nodes:\n",
    "                print(f\"\\nQuery node: {query}\")\n",
    "                mask = (top_results_df['decay_factor'] == C) & (top_results_df['query_node'] == query)\n",
    "                top_5 = top_results_df[mask].head()\n",
    "                if not top_5.empty:\n",
    "                    print(top_5[['target_node', 'similarity', 'rank']].to_string())\n",
    "                else:\n",
    "                    print(\"No results found\")\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Close Neo4j and Spark connections\"\"\"\n",
    "        self.driver.close()\n",
    "        self.spark.stop()\n",
    "\n",
    "# Example usage\n",
    "# if __name__ == \"__main__\":\n",
    "    # Sample citation data\n",
    "papers_data = {\n",
    "    2982615777: [2087551257, 2044328306],\n",
    "    2044328306: [2087551257],\n",
    "    2087551257: [2044328306],\n",
    "    1556418098: []  # Paper with no references\n",
    "}\n",
    "\n",
    "# Initialize analyzer\n",
    "analyzer = CitationGraphAnalyzer()\n",
    "\n",
    "try:\n",
    "    \n",
    "    # Run analysis\n",
    "    query_nodes = [2982615777, 1556418098]\n",
    "    decay_factors = [0.7, 0.8, 0.9]\n",
    "    \n",
    "    print(\"Running SimRank analysis...\")\n",
    "    final_results, top_results = analyzer.analyze_citation_graph(\n",
    "        query_nodes=query_nodes,\n",
    "        decay_factors=decay_factors\n",
    "    )\n",
    "    \n",
    "finally:\n",
    "    # Clean up connections\n",
    "    analyzer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from neo4j import GraphDatabase\n",
    "from pyspark.sql import SparkSession\n",
    "import networkx as nx\n",
    "import json\n",
    "from itertools import product\n",
    "import numpy as np\n",
    "from typing import List, Dict, Set\n",
    "import logging\n",
    "from tqdm.auto import trange\n",
    "\n",
    "class BatchedCitationGraph:\n",
    "    def __init__(self, uri: str, user: str, password: str, batch_size: int = 1000):\n",
    "        self.driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "        self.batch_size = batch_size\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "    def close(self):\n",
    "        self.driver.close()\n",
    "\n",
    "    def clear_database(self):\n",
    "        \"\"\"Clear database in batches to avoid memory issues\"\"\"\n",
    "        total_nodes = self.driver.session().run(\"MATCH (n) RETURN count(n) as count\").single()[\"count\"]\n",
    "        deleted = 0\n",
    "        with tqdm(total=total_nodes, desc=\"Clearing database\") as pbar:\n",
    "            while True:\n",
    "                result = self.driver.session().run(\n",
    "                    f\"MATCH (n) WITH n LIMIT {self.batch_size} \"\n",
    "                    \"DETACH DELETE n RETURN count(n)\"\n",
    "                ).single()\n",
    "                if result[0] == 0:\n",
    "                    break\n",
    "                deleted += result[0]\n",
    "                pbar.update(result[0])\n",
    "\n",
    "    def create_paper_nodes_batch(self, paper_ids: Set[str]):\n",
    "        \"\"\"Create paper nodes in batches\"\"\"\n",
    "        with self.driver.session() as session:\n",
    "            for i in tqdm(range(0, len(paper_ids), self.batch_size), \n",
    "                         desc=\"Creating paper nodes\", \n",
    "                         total=(len(paper_ids) + self.batch_size - 1) // self.batch_size):\n",
    "                batch = list(paper_ids)[i:i + self.batch_size]\n",
    "                session.run(\"\"\"\n",
    "                    UNWIND $batch AS paper_id\n",
    "                    MERGE (p:Paper {id: paper_id})\n",
    "                \"\"\", batch=batch)\n",
    "\n",
    "    def create_citation_edges_batch(self, citations: List[tuple]):\n",
    "        \"\"\"Create citation edges in batches\"\"\"\n",
    "        with self.driver.session() as session:\n",
    "            for i in tqdm(range(0, len(citations), self.batch_size), \n",
    "                         desc=\"Creating citation edges\",\n",
    "                         total=(len(citations) + self.batch_size - 1) // self.batch_size):\n",
    "                batch = citations[i:i + self.batch_size]\n",
    "                session.run(\"\"\"\n",
    "                    UNWIND $batch AS citation\n",
    "                    MATCH (p1:Paper {id: citation[0]})\n",
    "                    MATCH (p2:Paper {id: citation[1]})\n",
    "                    MERGE (p1)-[:CITES]->(p2)\n",
    "                \"\"\", batch=batch)\n",
    "\n",
    "def create_neo4j_graph(data_file: str, batch_size: int = 1000):\n",
    "    \"\"\"Create Neo4j graph with batched operations\"\"\"\n",
    "    graph = BatchedCitationGraph(\n",
    "        \"bolt://localhost:7687\",\n",
    "        \"neo4j\",\n",
    "        \"paras2003\",\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    # Clear existing data\n",
    "    graph.logger.info(\"Starting database cleanup...\")\n",
    "    graph.clear_database()\n",
    "\n",
    "    # First pass: Collect all unique paper IDs\n",
    "    paper_ids = set()\n",
    "    citations = []\n",
    "    \n",
    "    # Count total lines in file first\n",
    "    total_lines = sum(1 for _ in open(data_file, 'r'))\n",
    "    \n",
    "    graph.logger.info(\"Processing data file...\")\n",
    "    with open(data_file, 'r') as f:\n",
    "        with tqdm(f, total=total_lines, desc=\"Reading papers\") as pbar:\n",
    "            for line in pbar:\n",
    "                record = json.loads(line)\n",
    "                citing_paper = record[\"paper\"]\n",
    "                references = record[\"reference\"]\n",
    "                \n",
    "                paper_ids.add(citing_paper)\n",
    "                paper_ids.update(references)\n",
    "                citations.extend((citing_paper, cited_paper) \n",
    "                               for cited_paper in references)\n",
    "                pbar.set_postfix({'papers': len(paper_ids), 'citations': len(citations)})\n",
    "\n",
    "    # Create nodes in batches\n",
    "    graph.logger.info(f\"Creating {len(paper_ids)} paper nodes...\")\n",
    "    graph.create_paper_nodes_batch(paper_ids)\n",
    "\n",
    "    # Create edges in batches\n",
    "    graph.logger.info(f\"Creating {len(citations)} citation edges...\")\n",
    "    graph.create_citation_edges_batch(citations)\n",
    "\n",
    "    graph.logger.info(\"Graph creation completed\")\n",
    "    graph.close()\n",
    "\n",
    "def convert_to_networkx(neo4j_driver, batch_size: int = 1000):\n",
    "    \"\"\"Convert Neo4j graph to NetworkX graph in batches\"\"\"\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    with neo4j_driver.session() as session:\n",
    "        # Get total counts for progress bars\n",
    "        total_nodes = session.run(\"MATCH (p:Paper) RETURN count(p) as count\").single()[\"count\"]\n",
    "        total_edges = session.run(\"MATCH ()-[:CITES]->() RETURN count(*) as count\").single()[\"count\"]\n",
    "        \n",
    "        # Process nodes in batches\n",
    "        with tqdm(total=total_nodes, desc=\"Adding nodes to NetworkX\") as pbar:\n",
    "            for offset in range(0, total_nodes, batch_size):\n",
    "                nodes = session.run(\n",
    "                    f\"MATCH (p:Paper) RETURN p.id AS id \"\n",
    "                    f\"SKIP {offset} LIMIT {batch_size}\"\n",
    "                )\n",
    "                batch_nodes = [node[\"id\"] for node in nodes]\n",
    "                G.add_nodes_from(batch_nodes)\n",
    "                pbar.update(len(batch_nodes))\n",
    "        \n",
    "        # Process edges in batches\n",
    "        with tqdm(total=total_edges, desc=\"Adding edges to NetworkX\") as pbar:\n",
    "            processed = 0\n",
    "            while processed < total_edges:\n",
    "                edges = session.run(f\"\"\"\n",
    "                    MATCH (p1:Paper)-[:CITES]->(p2:Paper) \n",
    "                    RETURN p1.id AS source, p2.id AS target \n",
    "                    SKIP {processed} LIMIT {batch_size}\n",
    "                \"\"\").data()\n",
    "                \n",
    "                if not edges:\n",
    "                    break\n",
    "                \n",
    "                G.add_edges_from((edge[\"source\"], edge[\"target\"]) for edge in edges)\n",
    "                processed += len(edges)\n",
    "                pbar.update(len(edges))\n",
    "            \n",
    "    return G\n",
    "\n",
    "def simrank_spark(G, query_nodes, C, max_iterations=100, tolerance=0.0001, partition_size=1000):\n",
    "    \"\"\"Memory-optimized SimRank implementation with progress tracking\"\"\"\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"SimRank\") \\\n",
    "        .config(\"spark.driver.memory\", \"4g\") \\\n",
    "        .config(\"spark.executor.memory\", \"4g\") \\\n",
    "        .config(\"spark.memory.offHeap.enabled\", \"true\") \\\n",
    "        .config(\"spark.memory.offHeap.size\", \"4g\") \\\n",
    "        .config(\"spark.driver.bindAddress\", \"127.0.0.1\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "\n",
    "    # Process predecessors in batches\n",
    "    predecessors = {}\n",
    "    nodes = list(G.nodes())\n",
    "    with tqdm(total=len(nodes), desc=\"Building predecessor map\") as pbar:\n",
    "        for i in range(0, len(nodes), partition_size):\n",
    "            batch_nodes = nodes[i:i + partition_size]\n",
    "            for node in batch_nodes:\n",
    "                predecessors[node] = list(G.predecessors(node))\n",
    "            pbar.update(len(batch_nodes))\n",
    "\n",
    "    node_to_idx = {node: i for i, node in enumerate(nodes)}\n",
    "    n = len(nodes)\n",
    "\n",
    "    # Initialize similarity matrix using sparse representation\n",
    "    from scipy.sparse import lil_matrix\n",
    "    sim_matrix = lil_matrix((n, n))\n",
    "    for i in tqdm(range(n), desc=\"Initializing similarity matrix\"):\n",
    "        sim_matrix[i, i] = 1.0\n",
    "\n",
    "    sim_rdd = spark.sparkContext.broadcast(sim_matrix.tocsr())\n",
    "\n",
    "    def compute_pair_similarity(u, v):\n",
    "        if u == v:\n",
    "            return 1.0\n",
    "        \n",
    "        in_u = predecessors[u]\n",
    "        in_v = predecessors[v]\n",
    "        \n",
    "        if not in_u or not in_v:\n",
    "            return 0.0\n",
    "        \n",
    "        scale = C / (len(in_u) * len(in_v))\n",
    "        curr_sim = sim_rdd.value\n",
    "        \n",
    "        sum_sim = 0\n",
    "        for w, x in product(in_u, in_v):\n",
    "            sum_sim += curr_sim[node_to_idx[w], node_to_idx[x]]\n",
    "        \n",
    "        return scale * sum_sim\n",
    "\n",
    "    # Process iterations\n",
    "    pbar = tqdm(total=max_iterations, desc=\"SimRank iterations\")\n",
    "    for iteration in range(max_iterations):\n",
    "        new_sim = lil_matrix((n, n))\n",
    "        \n",
    "        # Process node pairs in batches with nested progress bars\n",
    "        total_batches = ((n + partition_size - 1) // partition_size) ** 2\n",
    "        batch_pbar = tqdm(total=total_batches, desc=f\"Processing batch pairs\", leave=False)\n",
    "        \n",
    "        for i in range(0, n, partition_size):\n",
    "            for j in range(0, n, partition_size):\n",
    "                batch_pairs = [(nodes[ii], nodes[jj]) \n",
    "                             for ii in range(i, min(i + partition_size, n))\n",
    "                             for jj in range(j, min(j + partition_size, n))]\n",
    "                \n",
    "                node_pairs_rdd = spark.sparkContext.parallelize(batch_pairs)\n",
    "                similarities = node_pairs_rdd.map(lambda pair: \n",
    "                    (pair, compute_pair_similarity(pair[0], pair[1]))).collect()\n",
    "                \n",
    "                for (u, v), sim in similarities:\n",
    "                    new_sim[node_to_idx[u], node_to_idx[v]] = sim\n",
    "                \n",
    "                batch_pbar.update(1)\n",
    "        \n",
    "        batch_pbar.close()\n",
    "        \n",
    "        # Check convergence\n",
    "        diff = abs(new_sim - sim_rdd.value).max()\n",
    "        sim_rdd = spark.sparkContext.broadcast(new_sim.tocsr())\n",
    "        \n",
    "        pbar.set_postfix({'diff': f'{diff:.6f}'})\n",
    "        pbar.update(1)\n",
    "        \n",
    "        if diff < tolerance:\n",
    "            logging.info(f\"Converged after {iteration + 1} iterations\")\n",
    "            break\n",
    "    \n",
    "    pbar.close()\n",
    "    \n",
    "    # Get results for query nodes\n",
    "    results = {}\n",
    "    for query_node in tqdm(query_nodes, desc=\"Processing query nodes\"):\n",
    "        if query_node not in node_to_idx:\n",
    "            logging.warning(f\"Query node {query_node} not found in graph\")\n",
    "            continue\n",
    "            \n",
    "        query_idx = node_to_idx[query_node]\n",
    "        similarities = []\n",
    "        \n",
    "        # Process similarities in batches\n",
    "        for i in range(0, n, partition_size):\n",
    "            batch_nodes = nodes[i:i + partition_size]\n",
    "            batch_similarities = [(node, sim_rdd.value[query_idx, node_to_idx[node]])\n",
    "                                for node in batch_nodes if node != query_node]\n",
    "            similarities.extend(batch_similarities)\n",
    "        \n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        results[query_node] = similarities\n",
    "    \n",
    "    spark.stop()\n",
    "    return results\n",
    "def main():\n",
    "    # Create Neo4j graph with smaller batch size\n",
    "    create_neo4j_graph(\"train.json\", batch_size=500)\n",
    "    \n",
    "    neo4j_connection = GraphDatabase.driver(\n",
    "        \"bolt://localhost:7687\",\n",
    "        auth=(\"neo4j\", \"paras2003\")\n",
    "    )\n",
    "    \n",
    "    # Convert to NetworkX with batching\n",
    "    G = convert_to_networkx(neo4j_connection, batch_size=500)\n",
    "    neo4j_connection.close()\n",
    "    \n",
    "    query_nodes = [\"2982615777\", \"1556418098\"]\n",
    "    results = simrank_spark(G, query_nodes, C=0.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networkx.algorithms.similarity import simrank_similarity\n",
    "def create_neo4j_graph(data_file: str, batch_size: int = 1000):\n",
    "    \"\"\"Create Neo4j graph with batched operations\"\"\"\n",
    "    graph = BatchedCitationGraph(\n",
    "        \"bolt://localhost:7687\",\n",
    "        \"neo4j\",\n",
    "        \"paras2003\",\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    # Clear existing data\n",
    "    graph.logger.info(\"Starting database cleanup...\")\n",
    "    graph.clear_database()\n",
    "\n",
    "    # First pass: Collect all unique paper IDs\n",
    "    paper_ids = set()\n",
    "    citations = []\n",
    "    \n",
    "    # Count total lines in file first\n",
    "    total_lines = sum(1 for _ in open(data_file, 'r'))\n",
    "    \n",
    "    graph.logger.info(\"Processing data file...\")\n",
    "    with open(data_file, 'r') as f:\n",
    "        with tqdm(f, total=total_lines, desc=\"Reading papers\") as pbar:\n",
    "            for line in pbar:\n",
    "                record = json.loads(line)\n",
    "                citing_paper = record[\"paper\"]\n",
    "                references = record[\"reference\"]\n",
    "                \n",
    "                paper_ids.add(citing_paper)\n",
    "                paper_ids.update(references)\n",
    "                citations.extend((citing_paper, cited_paper) \n",
    "                               for cited_paper in references)\n",
    "                pbar.set_postfix({'papers': len(paper_ids), 'citations': len(citations)})\n",
    "\n",
    "    # Create nodes in batches\n",
    "    graph.logger.info(f\"Creating {len(paper_ids)} paper nodes...\")\n",
    "    graph.create_paper_nodes_batch(paper_ids)\n",
    "\n",
    "    # Create edges in batches\n",
    "    graph.logger.info(f\"Creating {len(citations)} citation edges...\")\n",
    "    graph.create_citation_edges_batch(citations)\n",
    "\n",
    "    graph.logger.info(\"Graph creation completed\")\n",
    "    graph.close()\n",
    "def compute_simrank(G, query_nodes, decay_factor=0.8):\n",
    "    \"\"\"\n",
    "    Compute SimRank using NetworkX's built-in function.\n",
    "    \n",
    "    Args:\n",
    "        G (networkx.DiGraph): The citation graph.\n",
    "        query_nodes (List[str]): Nodes for which to compute SimRank.\n",
    "        decay_factor (float): The decay factor for SimRank computation.\n",
    "    \n",
    "    Returns:\n",
    "        Dict[str, List[Tuple[str, float]]]: Similarity scores for query nodes.\n",
    "    \"\"\"\n",
    "    # Compute SimRank for the entire graph\n",
    "    simrank_scores = simrank_similarity(G, decay_factor=decay_factor)\n",
    "\n",
    "    # Extract and filter results for query nodes\n",
    "    results = {}\n",
    "    for query_node in query_nodes:\n",
    "        if query_node not in G.nodes:\n",
    "            logging.warning(f\"Query node {query_node} not found in graph\")\n",
    "            continue\n",
    "        \n",
    "        # Sort similarities for the query node, excluding self-similarity\n",
    "        similarities = sorted(\n",
    "            ((target_node, sim) for target_node, sim in simrank_scores[query_node].items() if target_node != query_node),\n",
    "            key=lambda x: x[1],\n",
    "            reverse=True\n",
    "        )\n",
    "        results[query_node] = similarities\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clearing database: 100%|| 384441/384441 [00:15<00:00, 24133.63it/s]\n",
      "Reading papers: 100%|| 564340/564340 [15:52<00:00, 592.56it/s, papers=614941, citations=1216784]\n",
      "Creating paper nodes: 100%|| 1230/1230 [00:46<00:00, 26.18it/s]\n",
      "Creating citation edges: 100%|| 2434/2434 [00:44<00:00, 54.62it/s]\n",
      "Adding nodes to NetworkX: 100%|| 614941/614941 [00:33<00:00, 18164.47it/s]\n",
      "Adding edges to NetworkX: 100%|| 1216784/1216784 [09:56<00:00, 2040.51it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "simrank_similarity() got an unexpected keyword argument 'decay_factor'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/wd/p8rd_3q56c3gyt1n_mf2kh8h0000gn/T/ipykernel_57854/784652329.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Compute SimRank using NetworkX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_simrank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_nodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecay_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecay_factor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Output results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/wd/p8rd_3q56c3gyt1n_mf2kh8h0000gn/T/ipykernel_57854/1870782951.py\u001b[0m in \u001b[0;36mcompute_simrank\u001b[0;34m(G, query_nodes, decay_factor)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \"\"\"\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# Compute SimRank for the entire graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0msimrank_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msimrank_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecay_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecay_factor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;31m# Extract and filter results for query nodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/networkx/utils/backends.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, backend, *args, **kwargs)\u001b[0m\n\u001b[1;32m    410\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbackends\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m             \u001b[0;31m# Fast path if no backends are installed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 412\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morig_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;31m# Use `backend_name` in this function instead of `backend`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: simrank_similarity() got an unexpected keyword argument 'decay_factor'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create Neo4j graph with smaller batch size\n",
    "create_neo4j_graph(\"train.json\", batch_size=500)\n",
    "\n",
    "neo4j_connection = GraphDatabase.driver(\n",
    "    \"bolt://localhost:7687\",\n",
    "    auth=(\"neo4j\", \"paras2003\")\n",
    ")\n",
    "\n",
    "# Convert to NetworkX with batching\n",
    "G = convert_to_networkx(neo4j_connection, batch_size=500)\n",
    "neo4j_connection.close()\n",
    "\n",
    "# Query nodes for SimRank\n",
    "query_nodes = [\"2982615777\", \"1556418098\"]\n",
    "decay_factor = 0.8\n",
    "\n",
    "# Compute SimRank using NetworkX\n",
    "results = compute_simrank(G, query_nodes, decay_factor=decay_factor)\n",
    "\n",
    "# Output results\n",
    "for query_node, similarities in results.items():\n",
    "    print(f\"SimRank results for query node {query_node}:\")\n",
    "    for target_node, similarity in similarities[:10]:  # Top 10 most similar nodes\n",
    "        print(f\"  {target_node}: {similarity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/15 19:14:59 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running SimRank analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/15 19:15:19 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "24/11/15 19:15:19 WARN TaskSetManager: Stage 0 contains a task of very large size (2001 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing SimRank with decay factor C = 0.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/15 19:15:24 WARN TaskSetManager: Stage 3 contains a task of very large size (2001 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/11/15 19:15:25 WARN TaskSetManager: Stage 6 contains a task of very large size (2001 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d3c03a42370414a823c7e611b6e8953",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing query nodes:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "920002dee41e4fd89f6fd62abf4ae77f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing similarities for node 2982615777:   0%|          | 0/359143 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eb7c18df17040bea6b3a6607c7dfd68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing similarities for node 1556418098:   0%|          | 0/359143 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing SimRank with decay factor C = 0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/15 19:16:16 WARN TaskSetManager: Stage 9 contains a task of very large size (2001 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/11/15 19:16:18 WARN TaskSetManager: Stage 12 contains a task of very large size (2001 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56d5be7e89824dccbfd0e09e3055b401",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing query nodes:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac33329ea2544583b3122ac48ace39c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing similarities for node 2982615777:   0%|          | 0/359143 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7f812ff193a409594440edf50ae10d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing similarities for node 1556418098:   0%|          | 0/359143 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing SimRank with decay factor C = 0.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/15 19:17:08 WARN TaskSetManager: Stage 15 contains a task of very large size (2001 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/11/15 19:17:10 WARN TaskSetManager: Stage 18 contains a task of very large size (2001 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79aba5717b444ba183d6fd666fadb454",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing query nodes:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ca3fe11a6b34d52aa11d24e2f758705",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing similarities for node 2982615777:   0%|          | 0/359143 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58c9984f575e4de68d0575cafd1a1d94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing similarities for node 1556418098:   0%|          | 0/359143 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from neo4j import GraphDatabase\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "class CitationGraphAnalyzer:\n",
    "    def __init__(self, neo4j_uri=\"bolt://localhost:7687\", \n",
    "                 neo4j_user=\"neo4j\", neo4j_password=\"paras2003\"):\n",
    "        \"\"\"Initialize with Neo4j and Spark connections\"\"\"\n",
    "        self.driver = GraphDatabase.driver(neo4j_uri, auth=(neo4j_user, neo4j_password))\n",
    "        self.spark = SparkSession.builder \\\n",
    "            .appName(\"Citation Graph Analysis\") \\\n",
    "            .config(\"spark.driver.memory\", \"4g\") \\\n",
    "            .config(\"spark.executor.memory\", \"4g\") \\\n",
    "            .config(\"spark.task.maxFailures\", \"4\") \\\n",
    "            .getOrCreate()\n",
    "\n",
    "    def get_graph_data(self):\n",
    "        \"\"\"Extract graph data from Neo4j for Spark processing\"\"\"\n",
    "        with self.driver.session() as session:\n",
    "            result = session.run(\"\"\"\n",
    "                MATCH (p1:Paper)-[:CITES]->(p2:Paper)\n",
    "                RETURN p1.id as source, p2.id as target\n",
    "            \"\"\")\n",
    "            edges = [(record[\"source\"], record[\"target\"]) for record in result]\n",
    "            return edges\n",
    "\n",
    "    def compute_simrank(self, edges_df, query_nodes, decay_factors, output_dir=\"simrank_results\", max_iterations=10, tolerance=1e-4):\n",
    "        \"\"\"Run SimRank analysis on citation graph\"\"\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        # Ensure column names are standardized\n",
    "        edges_df = edges_df.withColumnRenamed(\"src\", \"source\").withColumnRenamed(\"dst\", \"target\")\n",
    "        \n",
    "        # Cache in-neighbors\n",
    "        in_neighbors_dict = self._cache_in_neighbors(edges_df)\n",
    "        \n",
    "        all_results = []\n",
    "        \n",
    "        for C in decay_factors:\n",
    "            print(f\"\\nComputing SimRank with decay factor C = {C}\")\n",
    "            \n",
    "            results = []\n",
    "            all_nodes = set(row['source'] for row in edges_df.select(\"source\").distinct().collect())\n",
    "            all_nodes.update(row['target'] for row in edges_df.select(\"target\").distinct().collect())\n",
    "            \n",
    "            for query_node in tqdm(query_nodes, desc=\"Processing query nodes\"):\n",
    "                node_results = []\n",
    "                for target_node in tqdm(all_nodes, desc=f\"Computing similarities for node {query_node}\", leave=False):\n",
    "                    sim = self._compute_simrank_similarity(\n",
    "                        query_node,\n",
    "                        target_node,\n",
    "                        in_neighbors_dict,\n",
    "                        C,\n",
    "                        max_iterations,\n",
    "                        tolerance\n",
    "                    )\n",
    "                    node_results.append((query_node, target_node, sim))\n",
    "                results.extend(node_results)\n",
    "            \n",
    "            results_df = pd.DataFrame(results, columns=['query_node', 'target_node', 'similarity'])\n",
    "            results_df['decay_factor'] = C\n",
    "            all_results.append(results_df)\n",
    "            \n",
    "            # Save intermediate results\n",
    "            output_path = f\"{output_dir}/simrank_results_C{C}_{timestamp}.csv\"\n",
    "            results_df.to_csv(output_path, index=False)\n",
    "        \n",
    "        return self._save_and_summarize_results(all_results, query_nodes, decay_factors, timestamp, output_dir)\n",
    "\n",
    "    def _cache_in_neighbors(self, edges_df):\n",
    "        \"\"\"Cache in-neighbors for all nodes\"\"\"\n",
    "        in_neighbors = edges_df.groupBy('target').agg(F.collect_list('source').alias('in_neighbors'))\n",
    "        return {row['target']: row['in_neighbors'] for row in in_neighbors.collect()}\n",
    "\n",
    "    def _compute_simrank_similarity(self, a, b, in_neighbors_dict, C, max_iterations, tolerance):\n",
    "        \"\"\"Compute SimRank similarity between two nodes.\"\"\"\n",
    "        if a == b:\n",
    "            return 1.0\n",
    "            \n",
    "        in_neighbors_a = in_neighbors_dict.get(a, [])\n",
    "        in_neighbors_b = in_neighbors_dict.get(b, [])\n",
    "        \n",
    "        if not in_neighbors_a or not in_neighbors_b:\n",
    "            return 0.0\n",
    "        \n",
    "        # Initialize similarity matrix for in-neighbors\n",
    "        sim_matrix = {}\n",
    "        for na in in_neighbors_a:\n",
    "            for nb in in_neighbors_b:\n",
    "                sim_matrix[(na, nb)] = 0.0\n",
    "        \n",
    "        # Iterate until convergence\n",
    "        for _ in range(max_iterations):\n",
    "            new_sim_matrix = {}\n",
    "            max_diff = 0.0\n",
    "            \n",
    "            for na in in_neighbors_a:\n",
    "                for nb in in_neighbors_b:\n",
    "                    in_na = in_neighbors_dict.get(na, [])\n",
    "                    in_nb = in_neighbors_dict.get(nb, [])\n",
    "                    \n",
    "                    if not in_na or not in_nb:\n",
    "                        continue\n",
    "                    \n",
    "                    sum_sim = sum(sim_matrix.get((i, j), 0.0) for i in in_na for j in in_nb)\n",
    "                    new_sim = (C / (len(in_na) * len(in_nb))) * sum_sim\n",
    "                    new_sim_matrix[(na, nb)] = new_sim\n",
    "                    max_diff = max(max_diff, abs(new_sim - sim_matrix.get((na, nb), 0.0)))\n",
    "            \n",
    "            sim_matrix = new_sim_matrix\n",
    "            if max_diff < tolerance:\n",
    "                break\n",
    "        \n",
    "        sum_sim = sum(sim_matrix.get((na, nb), 0.0) for na in in_neighbors_a for nb in in_neighbors_b)\n",
    "        return (C / (len(in_neighbors_a) * len(in_neighbors_b))) * sum_sim\n",
    "\n",
    "    def _save_and_summarize_results(self, all_results, query_nodes, decay_factors, timestamp, output_dir):\n",
    "        \"\"\"Save and summarize final results\"\"\"\n",
    "        final_results = pd.concat(all_results, ignore_index=True)\n",
    "        final_results_path = f\"{output_dir}/final_simrank_results_{timestamp}.csv\"\n",
    "        final_results.to_csv(final_results_path, index=False)\n",
    "        \n",
    "        top_results = final_results.groupby(['query_node', 'decay_factor']).apply(\n",
    "            lambda group: group.nlargest(10, 'similarity')).reset_index(drop=True)\n",
    "        top_results_path = f\"{output_dir}/top_simrank_results_{timestamp}.csv\"\n",
    "        top_results.to_csv(top_results_path, index=False)\n",
    "        \n",
    "        return final_results, top_results\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"Close Neo4j and Spark connections\"\"\"\n",
    "        self.driver.close()\n",
    "        self.spark.stop()\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    edges_csv_path = \"edges.csv\"\n",
    "    edges_df = pd.read_csv(edges_csv_path)\n",
    "    \n",
    "    spark = SparkSession.builder.appName(\"SimRank Analysis\").getOrCreate()\n",
    "    edges_sdf = spark.createDataFrame(edges_df)\n",
    "\n",
    "    analyzer = CitationGraphAnalyzer()\n",
    "\n",
    "    try:\n",
    "        query_nodes = [2982615777, 1556418098]\n",
    "        decay_factors = [0.7, 0.8, 0.9]\n",
    "        print(\"Running SimRank analysis...\")\n",
    "        final_results, top_results = analyzer.compute_simrank(\n",
    "            edges_sdf,\n",
    "            query_nodes=query_nodes,\n",
    "            decay_factors=decay_factors\n",
    "        )\n",
    "    finally:\n",
    "        analyzer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/15 19:37:06 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/11/15 19:37:21 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "24/11/15 19:37:21 WARN TaskSetManager: Stage 0 contains a task of very large size (2001 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running SimRank analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing SimRank with decay factor C = 0.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/15 19:37:24 WARN TaskSetManager: Stage 3 contains a task of very large size (2001 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/11/15 19:37:26 WARN TaskSetManager: Stage 6 contains a task of very large size (2001 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "386f763d26fe4771ae6ccfeb1eeef801",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing query nodes:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b88d1201ab8549348e845af724331a66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing similarities for node 2982615777:   0%|          | 0/359143 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b3ce60fcfc641beb36a9707a2da5210",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing similarities for node 1556418098:   0%|          | 0/359143 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing SimRank with decay factor C = 0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/15 19:38:16 WARN TaskSetManager: Stage 9 contains a task of very large size (2001 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/11/15 19:38:18 WARN TaskSetManager: Stage 12 contains a task of very large size (2001 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad4a751ea7c64a478f0caf589f60f30e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing query nodes:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5416059b4d741d69af12fffd26e7aab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing similarities for node 2982615777:   0%|          | 0/359143 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de4d314f5a3148c0ac1de1d48b29f443",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing similarities for node 1556418098:   0%|          | 0/359143 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing SimRank with decay factor C = 0.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/15 19:39:08 WARN TaskSetManager: Stage 15 contains a task of very large size (2001 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/11/15 19:39:10 WARN TaskSetManager: Stage 18 contains a task of very large size (2001 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d919f0b6555e4f0b8e04183b57e4c412",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing query nodes:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4891a08264c74d40845308a325945da9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing similarities for node 2982615777:   0%|          | 0/359143 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd06dc2b001947b490911a0bf2426966",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing similarities for node 1556418098:   0%|          | 0/359143 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from neo4j import GraphDatabase\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "class CitationGraphAnalyzer:\n",
    "    def __init__(self, neo4j_uri=\"bolt://localhost:7687\", \n",
    "                 neo4j_user=\"neo4j\", neo4j_password=\"paras2003\"):\n",
    "        \"\"\"Initialize with Neo4j and Spark connections\"\"\"\n",
    "        self.driver = GraphDatabase.driver(neo4j_uri, auth=(neo4j_user, neo4j_password))\n",
    "        self.spark = SparkSession.builder \\\n",
    "            .appName(\"Citation Graph Analysis\") \\\n",
    "            .config(\"spark.driver.memory\", \"4g\") \\\n",
    "            .config(\"spark.executor.memory\", \"4g\") \\\n",
    "            .config(\"spark.task.maxFailures\", \"4\") \\\n",
    "            .getOrCreate()\n",
    "\n",
    "    def get_graph_data(self):\n",
    "        \"\"\"Extract graph data from Neo4j for Spark processing\"\"\"\n",
    "        with self.driver.session() as session:\n",
    "            result = session.run(\"\"\"\n",
    "                MATCH (p1:Paper)-[:CITES]->(p2:Paper)\n",
    "                RETURN p1.id as source, p2.id as target\n",
    "            \"\"\")\n",
    "            edges = [(record[\"source\"], record[\"target\"]) for record in result]\n",
    "            return edges\n",
    "\n",
    "    def compute_simrank(self, edges_df, query_nodes, decay_factors, output_dir=\"simrank_results\", max_iterations=10, tolerance=1e-4):\n",
    "        \"\"\"Run SimRank analysis on citation graph\"\"\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        # Ensure column names are standardized\n",
    "        edges_df = edges_df.withColumnRenamed(\"src\", \"source\").withColumnRenamed(\"dst\", \"target\")\n",
    "        \n",
    "        # Cache in-neighbors\n",
    "        in_neighbors_dict = self._cache_in_neighbors(edges_df)\n",
    "        \n",
    "        all_results = []\n",
    "        \n",
    "        for C in decay_factors:\n",
    "            print(f\"\\nComputing SimRank with decay factor C = {C}\")\n",
    "            \n",
    "            results = []\n",
    "            all_nodes = set(row['source'] for row in edges_df.select(\"source\").distinct().collect())\n",
    "            all_nodes.update(row['target'] for row in edges_df.select(\"target\").distinct().collect())\n",
    "            \n",
    "            for query_node in tqdm(query_nodes, desc=\"Processing query nodes\"):\n",
    "                node_results = []\n",
    "                for target_node in tqdm(all_nodes, desc=f\"Computing similarities for node {query_node}\", leave=False):\n",
    "                    sim = self._compute_simrank_similarity(\n",
    "                        query_node,\n",
    "                        target_node,\n",
    "                        in_neighbors_dict,\n",
    "                        C,\n",
    "                        max_iterations,\n",
    "                        tolerance\n",
    "                    )\n",
    "                    node_results.append((query_node, target_node, sim))\n",
    "                results.extend(node_results)\n",
    "            \n",
    "            results_df = pd.DataFrame(results, columns=['query_node', 'target_node', 'similarity'])\n",
    "            results_df['decay_factor'] = C\n",
    "            all_results.append(results_df)\n",
    "            \n",
    "            # Save intermediate results\n",
    "            output_path = f\"{output_dir}/simrank_results_C{C}_{timestamp}.csv\"\n",
    "            results_df.to_csv(output_path, index=False)\n",
    "        \n",
    "        return self._save_and_summarize_results(all_results, query_nodes, decay_factors, timestamp, output_dir)\n",
    "\n",
    "    def _cache_in_neighbors(self, edges_df):\n",
    "        \"\"\"Cache in-neighbors for all nodes\"\"\"\n",
    "        in_neighbors = edges_df.groupBy('target').agg(F.collect_list('source').alias('in_neighbors'))\n",
    "        return {row['target']: row['in_neighbors'] for row in in_neighbors.collect()}\n",
    "\n",
    "    def _compute_simrank_similarity(self, a, b, in_neighbors_dict, C, max_iterations, tolerance):\n",
    "        \"\"\"Compute SimRank similarity between two nodes.\"\"\"\n",
    "        if a == b:\n",
    "            return 1.0\n",
    "        \n",
    "        in_neighbors_a = in_neighbors_dict.get(a, [])\n",
    "        in_neighbors_b = in_neighbors_dict.get(b, [])\n",
    "        \n",
    "        if not in_neighbors_a or not in_neighbors_b:\n",
    "            return 0.0\n",
    "        \n",
    "        # Initialize similarity matrix for in-neighbors\n",
    "        sim_matrix = {}\n",
    "        for na in in_neighbors_a:\n",
    "            for nb in in_neighbors_b:\n",
    "                sim_matrix[(na, nb)] = 0.0\n",
    "        \n",
    "        # Iterate until convergence\n",
    "        for _ in range(max_iterations):\n",
    "            new_sim_matrix = {}\n",
    "            max_diff = 0.0\n",
    "            \n",
    "            for na in in_neighbors_a:\n",
    "                for nb in in_neighbors_b:\n",
    "                    in_na = in_neighbors_dict.get(na, [])\n",
    "                    in_nb = in_neighbors_dict.get(nb, [])\n",
    "                    \n",
    "                    if not in_na or not in_nb:\n",
    "                        continue\n",
    "                    \n",
    "                    sum_sim = sum(sim_matrix.get((i, j), 0.0) for i in in_na for j in in_nb)\n",
    "                    new_sim = (C / (len(in_na) * len(in_nb))) * sum_sim\n",
    "                    new_sim_matrix[(na, nb)] = new_sim\n",
    "                    max_diff = max(max_diff, abs(new_sim - sim_matrix.get((na, nb), 0.0)))\n",
    "            \n",
    "            sim_matrix = new_sim_matrix\n",
    "            if max_diff < tolerance:\n",
    "                break\n",
    "        \n",
    "        sum_sim = sum(sim_matrix.get((na, nb), 0.0) for na in in_neighbors_a for nb in in_neighbors_b)\n",
    "        normalization_factor = (len(in_neighbors_a) * len(in_neighbors_b)) or 1  # Prevent division by 0\n",
    "        return (C / normalization_factor) * sum_sim\n",
    "\n",
    "    def _save_and_summarize_results(self, all_results, query_nodes, decay_factors, timestamp, output_dir):\n",
    "        \"\"\"Save and summarize final results\"\"\"\n",
    "        final_results = pd.concat(all_results, ignore_index=True)\n",
    "        final_results_path = f\"{output_dir}/final_simrank_results_{timestamp}.csv\"\n",
    "        final_results.to_csv(final_results_path, index=False)\n",
    "        \n",
    "        top_results = final_results.groupby(['query_node', 'decay_factor']).apply(\n",
    "            lambda group: group.nlargest(10, 'similarity')).reset_index(drop=True)\n",
    "        top_results_path = f\"{output_dir}/top_simrank_results_{timestamp}.csv\"\n",
    "        top_results.to_csv(top_results_path, index=False)\n",
    "        \n",
    "        return final_results, top_results\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"Close Neo4j and Spark connections\"\"\"\n",
    "        self.driver.close()\n",
    "        self.spark.stop()\n",
    "\n",
    "    def bfs_traversal(self, start_node, depth=2):\n",
    "        \"\"\"Perform BFS to get all nodes within a given depth\"\"\"\n",
    "        with self.driver.session() as session:\n",
    "            result = session.run(\"\"\"\n",
    "                MATCH (start:Paper)-[:CITES*1..{depth}]->(p:Paper)\n",
    "                WHERE start.id = $start_node\n",
    "                RETURN p.id as node\n",
    "            \"\"\", start_node=start_node, depth=depth)\n",
    "            return [record[\"node\"] for record in result]\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    edges_csv_path = \"graph_edges.csv\"\n",
    "    edges_df = pd.read_csv(edges_csv_path)\n",
    "    \n",
    "    spark = SparkSession.builder.appName(\"SimRank Analysis\").getOrCreate()\n",
    "    edges_sdf = spark.createDataFrame(edges_df)\n",
    "\n",
    "    analyzer = CitationGraphAnalyzer()\n",
    "\n",
    "    try:\n",
    "        query_nodes = [2982615777, 1556418098]\n",
    "        decay_factors = [0.7, 0.8, 0.9]\n",
    "        print(\"Running SimRank analysis...\")\n",
    "        final_results, top_results = analyzer.compute_simrank(\n",
    "            edges_sdf,\n",
    "            query_nodes=query_nodes,\n",
    "            decay_factors=decay_factors\n",
    "        )\n",
    "    finally:\n",
    "        analyzer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
